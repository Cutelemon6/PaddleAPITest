2025-03-03 17:02:14.768678 test begin: paddle.add(Tensor([0, 1, 192],"float16"), Tensor([192],"float16"), )

W0303 17:02:17.577198 162241 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:02:17.578464 162241 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.add(Tensor([0, 1, 192],"float16"), Tensor([192],"float16"), )
2025-03-03 17:02:17.591449 test begin: paddle.add(Tensor([0, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([0, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:17.595682 test begin: paddle.add(Tensor([0, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([0, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:17.598930 test begin: paddle.add(Tensor([0, 1, 2, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([0, 1, 2, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1, 2, 2] and the shape of Y = [100, 1, 2, 2]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:17.604263 test begin: paddle.add(Tensor([0, 1, 2, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([0, 1, 2, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1, 2, 2] and the shape of Y = [100, 1, 2, 2]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:17.607250 test begin: paddle.add(Tensor([0, 1, 25, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([0, 1, 25, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )
2025-03-03 17:02:17.610673 test begin: paddle.add(Tensor([0, 1, 2],"float32"), Tensor([0, 1, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([0, 1, 2],"float32"), Tensor([0, 1, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:17.614063 test begin: paddle.add(Tensor([0, 1, 2],"float32"), Tensor([0, 1, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([0, 1, 2],"float32"), Tensor([0, 1, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:17.616241 test begin: paddle.add(Tensor([0, 1, 2],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([0, 1, 2],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1, 2] and the shape of Y = [100, 1, 2]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:17.618854 test begin: paddle.add(Tensor([0, 1, 2],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([0, 1, 2],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1, 2] and the shape of Y = [100, 1, 2]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:17.621121 test begin: paddle.add(Tensor([0, 1, 512, 1, 40],"float16"), Tensor([0, 26, 512, 1, 1],"float16"), )

[Pass] paddle.add(Tensor([0, 1, 512, 1, 40],"float16"), Tensor([0, 26, 512, 1, 1],"float16"), )
2025-03-03 17:02:17.623401 test begin: paddle.add(Tensor([0, 1, 512, 1, 40],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), )

[paddle error] paddle.add(Tensor([0, 1, 512, 1, 40],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1, 512, 1, 40] and the shape of Y = [64, 26, 512, 1, 1]. Received [0] in X is not equal to [64] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:17.642787 test begin: paddle.add(Tensor([0, 1, 512, 1, 40],"float32"), Tensor([0, 26, 512, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([0, 1, 512, 1, 40],"float32"), Tensor([0, 26, 512, 1, 1],"float32"), )
2025-03-03 17:02:17.645205 test begin: paddle.add(Tensor([0, 1, 512, 1, 40],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), )

[paddle error] paddle.add(Tensor([0, 1, 512, 1, 40],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1, 512, 1, 40] and the shape of Y = [64, 26, 512, 1, 1]. Received [0] in X is not equal to [64] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:17.658127 test begin: paddle.add(Tensor([0, 10, 15, 20],"float32"), Tensor([0, 10, 15, 20],"float32"), name=None, )

[Pass] paddle.add(Tensor([0, 10, 15, 20],"float32"), Tensor([0, 10, 15, 20],"float32"), name=None, )
2025-03-03 17:02:17.660985 test begin: paddle.add(Tensor([0, 10, 15, 20],"float32"), Tensor([5, 10, 15, 20],"float32"), name=None, )

[paddle error] paddle.add(Tensor([0, 10, 15, 20],"float32"), Tensor([5, 10, 15, 20],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 10, 15, 20] and the shape of Y = [5, 10, 15, 20]. Received [0] in X is not equal to [5] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:17.666421 test begin: paddle.add(Tensor([0, 1024, 62, 64],"float32"), Tensor([0, 1024, 62, 64],"float32"), None, )

[Pass] paddle.add(Tensor([0, 1024, 62, 64],"float32"), Tensor([0, 1024, 62, 64],"float32"), None, )
2025-03-03 17:02:17.669174 test begin: paddle.add(Tensor([0, 1024, 62, 64],"float32"), Tensor([1, 1024, 62, 64],"float32"), None, )

[Pass] paddle.add(Tensor([0, 1024, 62, 64],"float32"), Tensor([1, 1024, 62, 64],"float32"), None, )
2025-03-03 17:02:17.760313 test begin: paddle.add(Tensor([0, 1024, 64, 128],"float32"), Tensor([0, 1024, 64, 128],"float32"), None, )

[Pass] paddle.add(Tensor([0, 1024, 64, 128],"float32"), Tensor([0, 1024, 64, 128],"float32"), None, )
2025-03-03 17:02:17.763942 test begin: paddle.add(Tensor([0, 1024, 64, 128],"float32"), Tensor([1, 1024, 64, 128],"float32"), None, )

[Pass] paddle.add(Tensor([0, 1024, 64, 128],"float32"), Tensor([1, 1024, 64, 128],"float32"), None, )
2025-03-03 17:02:17.897262 test begin: paddle.add(Tensor([0, 1024],"float16"), Tensor([1024],"float16"), )

[Pass] paddle.add(Tensor([0, 1024],"float16"), Tensor([1024],"float16"), )
2025-03-03 17:02:17.901101 test begin: paddle.add(Tensor([0, 112, 32, 64],"float32"), Tensor([0, 112, 32, 64],"float32"), )

[Pass] paddle.add(Tensor([0, 112, 32, 64],"float32"), Tensor([0, 112, 32, 64],"float32"), )
2025-03-03 17:02:17.904199 test begin: paddle.add(Tensor([0, 112, 32, 64],"float32"), Tensor([1, 112, 32, 64],"float32"), )

[Pass] paddle.add(Tensor([0, 112, 32, 64],"float32"), Tensor([1, 112, 32, 64],"float32"), )
2025-03-03 17:02:17.909636 test begin: paddle.add(Tensor([0, 128, 200, 272],"float32"), Tensor([0, 128, 200, 272],"float32"), )

[Pass] paddle.add(Tensor([0, 128, 200, 272],"float32"), Tensor([0, 128, 200, 272],"float32"), )
2025-03-03 17:02:17.911829 test begin: paddle.add(Tensor([0, 128, 200, 272],"float32"), Tensor([1, 128, 200, 272],"float32"), )

[Pass] paddle.add(Tensor([0, 128, 200, 272],"float32"), Tensor([1, 128, 200, 272],"float32"), )
2025-03-03 17:02:18.025176 test begin: paddle.add(Tensor([0, 12],"float32"), Tensor([0, 12],"float32"), )

[Pass] paddle.add(Tensor([0, 12],"float32"), Tensor([0, 12],"float32"), )
2025-03-03 17:02:18.035555 test begin: paddle.add(Tensor([0, 12],"float32"), Tensor([10, 12],"float32"), )

[paddle error] paddle.add(Tensor([0, 12],"float32"), Tensor([10, 12],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 12] and the shape of Y = [10, 12]. Received [0] in X is not equal to [10] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.038100 test begin: paddle.add(Tensor([0, 16, 24],"float16"), Tensor([0, 16, 24],"float16"), )

[Pass] paddle.add(Tensor([0, 16, 24],"float16"), Tensor([0, 16, 24],"float16"), )
2025-03-03 17:02:18.040203 test begin: paddle.add(Tensor([0, 16, 24],"float16"), Tensor([12544, 16, 24],"float16"), )

[paddle error] paddle.add(Tensor([0, 16, 24],"float16"), Tensor([12544, 16, 24],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 16, 24] and the shape of Y = [12544, 16, 24]. Received [0] in X is not equal to [12544] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.125136 test begin: paddle.add(Tensor([0, 16, 24],"float32"), Tensor([0, 16, 24],"float32"), )

[Pass] paddle.add(Tensor([0, 16, 24],"float32"), Tensor([0, 16, 24],"float32"), )
2025-03-03 17:02:18.128260 test begin: paddle.add(Tensor([0, 16, 24],"float32"), Tensor([12544, 16, 24],"float32"), )

[paddle error] paddle.add(Tensor([0, 16, 24],"float32"), Tensor([12544, 16, 24],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 16, 24] and the shape of Y = [12544, 16, 24]. Received [0] in X is not equal to [12544] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.221529 test begin: paddle.add(Tensor([0, 192],"float16"), Tensor([192],"float16"), )

[Pass] paddle.add(Tensor([0, 192],"float16"), Tensor([192],"float16"), )
2025-03-03 17:02:18.225355 test begin: paddle.add(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:18.228096 test begin: paddle.add(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:18.230613 test begin: paddle.add(Tensor([0, 1],"float32"), Tensor([100, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([0, 1],"float32"), Tensor([100, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1] and the shape of Y = [100, 1]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.232493 test begin: paddle.add(Tensor([0, 1],"float32"), Tensor([100, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([0, 1],"float32"), Tensor([100, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1] and the shape of Y = [100, 1]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.234744 test begin: paddle.add(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), name="Cauchy_rsample", )
2025-03-03 17:02:18.237399 test begin: paddle.add(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), name="Cauchy_sample", )
2025-03-03 17:02:18.240001 test begin: paddle.add(Tensor([0, 1],"float64"), Tensor([100, 1],"float64"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([0, 1],"float64"), Tensor([100, 1],"float64"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1] and the shape of Y = [100, 1]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.242272 test begin: paddle.add(Tensor([0, 1],"float64"), Tensor([100, 1],"float64"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([0, 1],"float64"), Tensor([100, 1],"float64"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1] and the shape of Y = [100, 1]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.244350 test begin: paddle.add(Tensor([0, 2, 1],"float32"), Tensor([0, 2, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([0, 2, 1],"float32"), Tensor([0, 2, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:18.246551 test begin: paddle.add(Tensor([0, 2, 1],"float32"), Tensor([0, 2, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([0, 2, 1],"float32"), Tensor([0, 2, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:18.248556 test begin: paddle.add(Tensor([0, 2, 1],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([0, 2, 1],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2, 1] and the shape of Y = [100, 2, 1]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.250562 test begin: paddle.add(Tensor([0, 2, 1],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([0, 2, 1],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2, 1] and the shape of Y = [100, 2, 1]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.252589 test begin: paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([0, 2, 3, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([0, 2, 3, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:18.254171 test begin: paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([0, 2, 3, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([0, 2, 3, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:18.255665 test begin: paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2, 3, 1] and the shape of Y = [100, 2, 3, 1]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.257098 test begin: paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2, 3, 1] and the shape of Y = [100, 2, 3, 1]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.258640 test begin: paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([0, 2, 3, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )
2025-03-03 17:02:18.260976 test begin: paddle.add(Tensor([0, 2, 3],"complex128"), Tensor([2, 3],"complex128"), name="Normal_sample", )

[Pass] paddle.add(Tensor([0, 2, 3],"complex128"), Tensor([2, 3],"complex128"), name="Normal_sample", )
2025-03-03 17:02:18.263505 test begin: paddle.add(Tensor([0, 2, 3],"complex64"), Tensor([2, 3],"complex64"), name="Normal_sample", )

[Pass] paddle.add(Tensor([0, 2, 3],"complex64"), Tensor([2, 3],"complex64"), name="Normal_sample", )
2025-03-03 17:02:18.265352 test begin: paddle.add(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), name="Normal_kl_divergence", )

[Pass] paddle.add(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), name="Normal_kl_divergence", )
2025-03-03 17:02:18.266822 test begin: paddle.add(Tensor([0, 2],"float64"), Tensor([2, 2],"float64"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([0, 2],"float64"), Tensor([2, 2],"float64"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2] and the shape of Y = [2, 2]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.268273 test begin: paddle.add(Tensor([0, 3, 1, 1000],"float32"), Tensor([1000],"float32"), )

[Pass] paddle.add(Tensor([0, 3, 1, 1000],"float32"), Tensor([1000],"float32"), )
2025-03-03 17:02:18.270022 test begin: paddle.add(Tensor([0, 3, 1, 10],"float32"), Tensor([10],"float32"), )

[Pass] paddle.add(Tensor([0, 3, 1, 10],"float32"), Tensor([10],"float32"), )
2025-03-03 17:02:18.271755 test begin: paddle.add(Tensor([0, 3, 3, 4],"float32"), Tensor([3, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([0, 3, 3, 4],"float32"), Tensor([3, 1, 1],"float32"), )
2025-03-03 17:02:18.273539 test begin: paddle.add(Tensor([0, 3, 3, 4],"float32"), Tensor([3, 1, 4],"float32"), )

[Pass] paddle.add(Tensor([0, 3, 3, 4],"float32"), Tensor([3, 1, 4],"float32"), )
2025-03-03 17:02:18.275916 test begin: paddle.add(Tensor([0, 3, 4, 5],"complex128"), Tensor([4, 5],"float64"), )

W0303 17:02:18.277144 162395 dygraph_functions.cc:86196] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.add(Tensor([0, 3, 4, 5],"complex128"), Tensor([4, 5],"float64"), )
2025-03-03 17:02:18.278021 test begin: paddle.add(Tensor([0, 3, 4, 5],"complex64"), Tensor([4, 5],"float32"), )

[Pass] paddle.add(Tensor([0, 3, 4, 5],"complex64"), Tensor([4, 5],"float32"), )
2025-03-03 17:02:18.280992 test begin: paddle.add(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), name="Normal_entropy", )

[Pass] paddle.add(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), name="Normal_entropy", )
2025-03-03 17:02:18.283618 test begin: paddle.add(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), name="Normal_kl_divergence", )

[Pass] paddle.add(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), name="Normal_kl_divergence", )
2025-03-03 17:02:18.285301 test begin: paddle.add(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), name=None, )

[Pass] paddle.add(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), name=None, )
2025-03-03 17:02:18.286825 test begin: paddle.add(Tensor([0, 3],"float32"), Tensor([2, 3],"float32"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([0, 3],"float32"), Tensor([2, 3],"float32"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.288391 test begin: paddle.add(Tensor([0, 3],"float32"), Tensor([2, 3],"float32"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([0, 3],"float32"), Tensor([2, 3],"float32"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.289889 test begin: paddle.add(Tensor([0, 3],"float32"), Tensor([2, 3],"float32"), name=None, )

[paddle error] paddle.add(Tensor([0, 3],"float32"), Tensor([2, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.291321 test begin: paddle.add(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), name="Normal_entropy", )

[Pass] paddle.add(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), name="Normal_entropy", )
2025-03-03 17:02:18.292890 test begin: paddle.add(Tensor([0, 3],"float64"), Tensor([2, 3],"float64"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([0, 3],"float64"), Tensor([2, 3],"float64"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.294394 test begin: paddle.add(Tensor([0, 3],"int64"), Tensor([0, 3],"int64"), )

[Pass] paddle.add(Tensor([0, 3],"int64"), Tensor([0, 3],"int64"), )
2025-03-03 17:02:18.298865 test begin: paddle.add(Tensor([0, 3],"int64"), Tensor([3, 3],"int64"), )

[paddle error] paddle.add(Tensor([0, 3],"int64"), Tensor([3, 3],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [3, 3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.301272 test begin: paddle.add(Tensor([0, 40],"bfloat16"), Tensor([0, 40],"bfloat16"), name=None, )

[Pass] paddle.add(Tensor([0, 40],"bfloat16"), Tensor([0, 40],"bfloat16"), name=None, )
2025-03-03 17:02:18.303611 test begin: paddle.add(Tensor([0, 40],"bfloat16"), Tensor([40, 40],"bfloat16"), name=None, )

[paddle error] paddle.add(Tensor([0, 40],"bfloat16"), Tensor([40, 40],"bfloat16"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 40] and the shape of Y = [40, 40]. Received [0] in X is not equal to [40] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.307023 test begin: paddle.add(Tensor([0, 4],"complex128"), Tensor([4],"complex64"), name="Normal_sample", )

[Pass] paddle.add(Tensor([0, 4],"complex128"), Tensor([4],"complex64"), name="Normal_sample", )
2025-03-03 17:02:18.310290 test begin: paddle.add(Tensor([0, 4],"float64"), Tensor([4],"float64"), name="Normal_sample", )

[Pass] paddle.add(Tensor([0, 4],"float64"), Tensor([4],"float64"), name="Normal_sample", )
2025-03-03 17:02:18.313058 test begin: paddle.add(Tensor([0, 4],"float64"), Tensor([4],"float64"), name="Uniform_sample", )

[Pass] paddle.add(Tensor([0, 4],"float64"), Tensor([4],"float64"), name="Uniform_sample", )
2025-03-03 17:02:18.315612 test begin: paddle.add(Tensor([0, 5, 3],"float64"), Tensor([5, 3],"float64"), name="Uniform_sample", )

[Pass] paddle.add(Tensor([0, 5, 3],"float64"), Tensor([5, 3],"float64"), name="Uniform_sample", )
2025-03-03 17:02:18.317974 test begin: paddle.add(Tensor([0, 5, 6],"float32"), Tensor([5, 6],"float32"), name="Uniform_sample", )

[Pass] paddle.add(Tensor([0, 5, 6],"float32"), Tensor([5, 6],"float32"), name="Uniform_sample", )
2025-03-03 17:02:18.320258 test begin: paddle.add(Tensor([0, 7, 99],"float32"), Tensor([99],"float32"), )

[Pass] paddle.add(Tensor([0, 7, 99],"float32"), Tensor([99],"float32"), )
2025-03-03 17:02:18.322571 test begin: paddle.add(Tensor([0, 8, 4],"float32"), Tensor([8, 4],"float32"), )

[Pass] paddle.add(Tensor([0, 8, 4],"float32"), Tensor([8, 4],"float32"), )
2025-03-03 17:02:18.325460 test begin: paddle.add(Tensor([0],"float32"), Tensor([0],"float32"), )

[Pass] paddle.add(Tensor([0],"float32"), Tensor([0],"float32"), )
2025-03-03 17:02:18.327749 test begin: paddle.add(Tensor([0],"float32"), Tensor([0],"float32"), name="Cauchy_entropy", )

[Pass] paddle.add(Tensor([0],"float32"), Tensor([0],"float32"), name="Cauchy_entropy", )
2025-03-03 17:02:18.329907 test begin: paddle.add(Tensor([0],"float32"), Tensor([0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([0],"float32"), Tensor([0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:18.332022 test begin: paddle.add(Tensor([0],"float32"), Tensor([0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([0],"float32"), Tensor([0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:18.333684 test begin: paddle.add(Tensor([0],"float32"), Tensor([100],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([0],"float32"), Tensor([100],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [100]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.336012 test begin: paddle.add(Tensor([0],"float32"), Tensor([100],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([0],"float32"), Tensor([100],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [100]. Received [0] in X is not equal to [100] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.338228 test begin: paddle.add(Tensor([0],"float32"), Tensor([1],"float32"), )

[Pass] paddle.add(Tensor([0],"float32"), Tensor([1],"float32"), )
2025-03-03 17:02:18.340847 test begin: paddle.add(Tensor([0],"float32"), Tensor([1],"float32"), name="Cauchy_entropy", )

[Pass] paddle.add(Tensor([0],"float32"), Tensor([1],"float32"), name="Cauchy_entropy", )
2025-03-03 17:02:18.343200 test begin: paddle.add(Tensor([0],"float32"), Tensor([3],"float32"), name="Cauchy_entropy", )

[paddle error] paddle.add(Tensor([0],"float32"), Tensor([3],"float32"), name="Cauchy_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.345187 test begin: paddle.add(Tensor([0],"float64"), Tensor([0],"float64"), name="Normal_entropy", )

[Pass] paddle.add(Tensor([0],"float64"), Tensor([0],"float64"), name="Normal_entropy", )
2025-03-03 17:02:18.347357 test begin: paddle.add(Tensor([0],"float64"), Tensor([0],"float64"), name="Normal_kl_divergence", )

[Pass] paddle.add(Tensor([0],"float64"), Tensor([0],"float64"), name="Normal_kl_divergence", )
2025-03-03 17:02:18.377027 test begin: paddle.add(Tensor([0],"float64"), Tensor([2],"float64"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([0],"float64"), Tensor([2],"float64"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [2]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.388046 test begin: paddle.add(Tensor([0],"float64"), Tensor([2],"float64"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([0],"float64"), Tensor([2],"float64"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [2]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.389664 test begin: paddle.add(Tensor([0],"int64"), Tensor([0],"int64"), )

[Pass] paddle.add(Tensor([0],"int64"), Tensor([0],"int64"), )
2025-03-03 17:02:18.391329 test begin: paddle.add(Tensor([0],"int64"), Tensor([168],"int64"), )

[paddle error] paddle.add(Tensor([0],"int64"), Tensor([168],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [168]. Received [0] in X is not equal to [168] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.392741 test begin: paddle.add(Tensor([0],"int64"), Tensor([300, 40],"int64"), )

[paddle error] paddle.add(Tensor([0],"int64"), Tensor([300, 40],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [300, 40]. Received [0] in X is not equal to [40] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:18.395320 test begin: paddle.add(Tensor([1, 0, 192],"float16"), Tensor([192],"float16"), )

[Pass] paddle.add(Tensor([1, 0, 192],"float16"), Tensor([192],"float16"), )
2025-03-03 17:02:18.397393 test begin: paddle.add(Tensor([1, 0, 200, 272],"float32"), Tensor([1, 0, 200, 272],"float32"), )

[Pass] paddle.add(Tensor([1, 0, 200, 272],"float32"), Tensor([1, 0, 200, 272],"float32"), )
2025-03-03 17:02:18.398974 test begin: paddle.add(Tensor([1, 0, 200, 272],"float32"), Tensor([1, 128, 200, 272],"float32"), )

[paddle error] paddle.add(Tensor([1, 0, 200, 272],"float32"), Tensor([1, 128, 200, 272],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0, 200, 272] and the shape of Y = [1, 128, 200, 272]. Received [0] in X is not equal to [128] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.003518 test begin: paddle.add(Tensor([1, 0, 32, 64],"float32"), Tensor([1, 0, 32, 64],"float32"), )

[Pass] paddle.add(Tensor([1, 0, 32, 64],"float32"), Tensor([1, 0, 32, 64],"float32"), )
2025-03-03 17:02:19.007857 test begin: paddle.add(Tensor([1, 0, 32, 64],"float32"), Tensor([1, 112, 32, 64],"float32"), )

[paddle error] paddle.add(Tensor([1, 0, 32, 64],"float32"), Tensor([1, 112, 32, 64],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0, 32, 64] and the shape of Y = [1, 112, 32, 64]. Received [0] in X is not equal to [112] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.057025 test begin: paddle.add(Tensor([1, 0, 4],"float32"), Tensor([8, 4],"float32"), )

[paddle error] paddle.add(Tensor([1, 0, 4],"float32"), Tensor([8, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0, 4] and the shape of Y = [8, 4]. Received [0] in X is not equal to [8] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.063625 test begin: paddle.add(Tensor([1, 0, 62, 64],"float32"), Tensor([1, 0, 62, 64],"float32"), None, )

[Pass] paddle.add(Tensor([1, 0, 62, 64],"float32"), Tensor([1, 0, 62, 64],"float32"), None, )
2025-03-03 17:02:19.069764 test begin: paddle.add(Tensor([1, 0, 62, 64],"float32"), Tensor([1, 1024, 62, 64],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 0, 62, 64],"float32"), Tensor([1, 1024, 62, 64],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0, 62, 64] and the shape of Y = [1, 1024, 62, 64]. Received [0] in X is not equal to [1024] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.163652 test begin: paddle.add(Tensor([1, 0, 64, 128],"float32"), Tensor([1, 0, 64, 128],"float32"), None, )

[Pass] paddle.add(Tensor([1, 0, 64, 128],"float32"), Tensor([1, 0, 64, 128],"float32"), None, )
2025-03-03 17:02:19.177193 test begin: paddle.add(Tensor([1, 0, 64, 128],"float32"), Tensor([1, 1024, 64, 128],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 0, 64, 128],"float32"), Tensor([1, 1024, 64, 128],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0, 64, 128] and the shape of Y = [1, 1024, 64, 128]. Received [0] in X is not equal to [1024] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.301111 test begin: paddle.add(Tensor([1, 0],"int64"), Tensor([3, 0],"int64"), )

[Pass] paddle.add(Tensor([1, 0],"int64"), Tensor([3, 0],"int64"), )
2025-03-03 17:02:19.306958 test begin: paddle.add(Tensor([1, 0],"int64"), Tensor([3, 3],"int64"), )

[paddle error] paddle.add(Tensor([1, 0],"int64"), Tensor([3, 3],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0] and the shape of Y = [3, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.339181 test begin: paddle.add(Tensor([1, 1, 0],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.add(Tensor([1, 1, 0],"float16"), Tensor([192],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1, 0] and the shape of Y = [192]. Received [0] in X is not equal to [192] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.348290 test begin: paddle.add(Tensor([1, 1, 192],"float16"), Tensor([0],"float16"), )

[paddle error] paddle.add(Tensor([1, 1, 192],"float16"), Tensor([0],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1, 192] and the shape of Y = [0]. Received [192] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.351530 test begin: paddle.add(Tensor([1, 1024, 0, 128],"float32"), Tensor([1, 1024, 0, 128],"float32"), None, )

[Pass] paddle.add(Tensor([1, 1024, 0, 128],"float32"), Tensor([1, 1024, 0, 128],"float32"), None, )
2025-03-03 17:02:19.363983 test begin: paddle.add(Tensor([1, 1024, 0, 128],"float32"), Tensor([1, 1024, 64, 128],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 0, 128],"float32"), Tensor([1, 1024, 64, 128],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 0, 128] and the shape of Y = [1, 1024, 64, 128]. Received [0] in X is not equal to [64] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.494414 test begin: paddle.add(Tensor([1, 1024, 0, 64],"float32"), Tensor([1, 1024, 0, 64],"float32"), None, )

[Pass] paddle.add(Tensor([1, 1024, 0, 64],"float32"), Tensor([1, 1024, 0, 64],"float32"), None, )
2025-03-03 17:02:19.501050 test begin: paddle.add(Tensor([1, 1024, 0, 64],"float32"), Tensor([1, 1024, 62, 64],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 0, 64],"float32"), Tensor([1, 1024, 62, 64],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 0, 64] and the shape of Y = [1, 1024, 62, 64]. Received [0] in X is not equal to [62] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.547952 test begin: paddle.add(Tensor([1, 1024, 62, 0],"float32"), Tensor([1, 1024, 62, 0],"float32"), None, )

[Pass] paddle.add(Tensor([1, 1024, 62, 0],"float32"), Tensor([1, 1024, 62, 0],"float32"), None, )
2025-03-03 17:02:19.552655 test begin: paddle.add(Tensor([1, 1024, 62, 0],"float32"), Tensor([1, 1024, 62, 64],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 62, 0],"float32"), Tensor([1, 1024, 62, 64],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 62, 0] and the shape of Y = [1, 1024, 62, 64]. Received [0] in X is not equal to [64] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.600738 test begin: paddle.add(Tensor([1, 1024, 62, 64],"float32"), Tensor([0, 1024, 62, 64],"float32"), None, )

[Pass] paddle.add(Tensor([1, 1024, 62, 64],"float32"), Tensor([0, 1024, 62, 64],"float32"), None, )
2025-03-03 17:02:19.665267 test begin: paddle.add(Tensor([1, 1024, 62, 64],"float32"), Tensor([1, 0, 62, 64],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 62, 64],"float32"), Tensor([1, 0, 62, 64],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 62, 64] and the shape of Y = [1, 0, 62, 64]. Received [1024] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.730856 test begin: paddle.add(Tensor([1, 1024, 62, 64],"float32"), Tensor([1, 1024, 0, 64],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 62, 64],"float32"), Tensor([1, 1024, 0, 64],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 62, 64] and the shape of Y = [1, 1024, 0, 64]. Received [62] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.778964 test begin: paddle.add(Tensor([1, 1024, 62, 64],"float32"), Tensor([1, 1024, 62, 0],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 62, 64],"float32"), Tensor([1, 1024, 62, 0],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 62, 64] and the shape of Y = [1, 1024, 62, 0]. Received [64] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.825770 test begin: paddle.add(Tensor([1, 1024, 64, 0],"float32"), Tensor([1, 1024, 64, 0],"float32"), None, )

[Pass] paddle.add(Tensor([1, 1024, 64, 0],"float32"), Tensor([1, 1024, 64, 0],"float32"), None, )
2025-03-03 17:02:19.828073 test begin: paddle.add(Tensor([1, 1024, 64, 0],"float32"), Tensor([1, 1024, 64, 128],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 64, 0],"float32"), Tensor([1, 1024, 64, 128],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 64, 0] and the shape of Y = [1, 1024, 64, 128]. Received [0] in X is not equal to [128] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:19.993518 test begin: paddle.add(Tensor([1, 1024, 64, 128],"float32"), Tensor([0, 1024, 64, 128],"float32"), None, )

[Pass] paddle.add(Tensor([1, 1024, 64, 128],"float32"), Tensor([0, 1024, 64, 128],"float32"), None, )
2025-03-03 17:02:20.114963 test begin: paddle.add(Tensor([1, 1024, 64, 128],"float32"), Tensor([1, 0, 64, 128],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 64, 128],"float32"), Tensor([1, 0, 64, 128],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 64, 128] and the shape of Y = [1, 0, 64, 128]. Received [1024] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.233296 test begin: paddle.add(Tensor([1, 1024, 64, 128],"float32"), Tensor([1, 1024, 0, 128],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 64, 128],"float32"), Tensor([1, 1024, 0, 128],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 64, 128] and the shape of Y = [1, 1024, 0, 128]. Received [64] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.351888 test begin: paddle.add(Tensor([1, 1024, 64, 128],"float32"), Tensor([1, 1024, 64, 0],"float32"), None, )

[paddle error] paddle.add(Tensor([1, 1024, 64, 128],"float32"), Tensor([1, 1024, 64, 0],"float32"), None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 64, 128] and the shape of Y = [1, 1024, 64, 0]. Received [128] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.467734 test begin: paddle.add(Tensor([1, 112, 0, 64],"float32"), Tensor([1, 112, 0, 64],"float32"), )

[Pass] paddle.add(Tensor([1, 112, 0, 64],"float32"), Tensor([1, 112, 0, 64],"float32"), )
2025-03-03 17:02:20.470151 test begin: paddle.add(Tensor([1, 112, 0, 64],"float32"), Tensor([1, 112, 32, 64],"float32"), )

[paddle error] paddle.add(Tensor([1, 112, 0, 64],"float32"), Tensor([1, 112, 32, 64],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 112, 0, 64] and the shape of Y = [1, 112, 32, 64]. Received [0] in X is not equal to [32] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.474235 test begin: paddle.add(Tensor([1, 112, 32, 0],"float32"), Tensor([1, 112, 32, 0],"float32"), )

[Pass] paddle.add(Tensor([1, 112, 32, 0],"float32"), Tensor([1, 112, 32, 0],"float32"), )
2025-03-03 17:02:20.476140 test begin: paddle.add(Tensor([1, 112, 32, 0],"float32"), Tensor([1, 112, 32, 64],"float32"), )

[paddle error] paddle.add(Tensor([1, 112, 32, 0],"float32"), Tensor([1, 112, 32, 64],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 112, 32, 0] and the shape of Y = [1, 112, 32, 64]. Received [0] in X is not equal to [64] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.479871 test begin: paddle.add(Tensor([1, 112, 32, 64],"float32"), Tensor([0, 112, 32, 64],"float32"), )

[Pass] paddle.add(Tensor([1, 112, 32, 64],"float32"), Tensor([0, 112, 32, 64],"float32"), )
2025-03-03 17:02:20.483605 test begin: paddle.add(Tensor([1, 112, 32, 64],"float32"), Tensor([1, 0, 32, 64],"float32"), )

[paddle error] paddle.add(Tensor([1, 112, 32, 64],"float32"), Tensor([1, 0, 32, 64],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 112, 32, 64] and the shape of Y = [1, 0, 32, 64]. Received [112] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.486791 test begin: paddle.add(Tensor([1, 112, 32, 64],"float32"), Tensor([1, 112, 0, 64],"float32"), )

[paddle error] paddle.add(Tensor([1, 112, 32, 64],"float32"), Tensor([1, 112, 0, 64],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 112, 32, 64] and the shape of Y = [1, 112, 0, 64]. Received [32] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.489904 test begin: paddle.add(Tensor([1, 112, 32, 64],"float32"), Tensor([1, 112, 32, 0],"float32"), )

[paddle error] paddle.add(Tensor([1, 112, 32, 64],"float32"), Tensor([1, 112, 32, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 112, 32, 64] and the shape of Y = [1, 112, 32, 0]. Received [64] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.493070 test begin: paddle.add(Tensor([1, 128, 0, 272],"float32"), Tensor([1, 128, 0, 272],"float32"), )

[Pass] paddle.add(Tensor([1, 128, 0, 272],"float32"), Tensor([1, 128, 0, 272],"float32"), )
2025-03-03 17:02:20.494379 test begin: paddle.add(Tensor([1, 128, 0, 272],"float32"), Tensor([1, 128, 200, 272],"float32"), )

[paddle error] paddle.add(Tensor([1, 128, 0, 272],"float32"), Tensor([1, 128, 200, 272],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 128, 0, 272] and the shape of Y = [1, 128, 200, 272]. Received [0] in X is not equal to [200] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.599677 test begin: paddle.add(Tensor([1, 128, 200, 0],"float32"), Tensor([1, 128, 200, 0],"float32"), )

[Pass] paddle.add(Tensor([1, 128, 200, 0],"float32"), Tensor([1, 128, 200, 0],"float32"), )
2025-03-03 17:02:20.601914 test begin: paddle.add(Tensor([1, 128, 200, 0],"float32"), Tensor([1, 128, 200, 272],"float32"), )

[paddle error] paddle.add(Tensor([1, 128, 200, 0],"float32"), Tensor([1, 128, 200, 272],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 128, 200, 0] and the shape of Y = [1, 128, 200, 272]. Received [0] in X is not equal to [272] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.704850 test begin: paddle.add(Tensor([1, 128, 200, 272],"float32"), Tensor([0, 128, 200, 272],"float32"), )

[Pass] paddle.add(Tensor([1, 128, 200, 272],"float32"), Tensor([0, 128, 200, 272],"float32"), )
2025-03-03 17:02:20.808715 test begin: paddle.add(Tensor([1, 128, 200, 272],"float32"), Tensor([1, 0, 200, 272],"float32"), )

[paddle error] paddle.add(Tensor([1, 128, 200, 272],"float32"), Tensor([1, 0, 200, 272],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 128, 200, 272] and the shape of Y = [1, 0, 200, 272]. Received [128] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:20.907126 test begin: paddle.add(Tensor([1, 128, 200, 272],"float32"), Tensor([1, 128, 0, 272],"float32"), )

[paddle error] paddle.add(Tensor([1, 128, 200, 272],"float32"), Tensor([1, 128, 0, 272],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 128, 200, 272] and the shape of Y = [1, 128, 0, 272]. Received [200] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.051681 test begin: paddle.add(Tensor([1, 128, 200, 272],"float32"), Tensor([1, 128, 200, 0],"float32"), )

[paddle error] paddle.add(Tensor([1, 128, 200, 272],"float32"), Tensor([1, 128, 200, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 128, 200, 272] and the shape of Y = [1, 128, 200, 0]. Received [272] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.156119 test begin: paddle.add(Tensor([1, 3],"int64"), Tensor([0, 3],"int64"), )

[Pass] paddle.add(Tensor([1, 3],"int64"), Tensor([0, 3],"int64"), )
2025-03-03 17:02:21.159577 test begin: paddle.add(Tensor([1, 3],"int64"), Tensor([3, 0],"int64"), )

[paddle error] paddle.add(Tensor([1, 3],"int64"), Tensor([3, 0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 3] and the shape of Y = [3, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.161696 test begin: paddle.add(Tensor([1, 8, 0],"float32"), Tensor([8, 4],"float32"), )

[paddle error] paddle.add(Tensor([1, 8, 0],"float32"), Tensor([8, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 8, 0] and the shape of Y = [8, 4]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.162965 test begin: paddle.add(Tensor([1, 8, 4],"float32"), Tensor([0, 4],"float32"), )

[paddle error] paddle.add(Tensor([1, 8, 4],"float32"), Tensor([0, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 8, 4] and the shape of Y = [0, 4]. Received [8] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.164219 test begin: paddle.add(Tensor([1, 8, 4],"float32"), Tensor([8, 0],"float32"), )

[paddle error] paddle.add(Tensor([1, 8, 4],"float32"), Tensor([8, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 8, 4] and the shape of Y = [8, 0]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.165405 test begin: paddle.add(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )

[Pass] paddle.add(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )
2025-03-03 17:02:21.166813 test begin: paddle.add(Tensor([10, 0],"float32"), Tensor([10, 12],"float32"), )

[paddle error] paddle.add(Tensor([10, 0],"float32"), Tensor([10, 12],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 0] and the shape of Y = [10, 12]. Received [0] in X is not equal to [12] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.167816 test begin: paddle.add(Tensor([10, 12],"float32"), Tensor([0, 12],"float32"), )

[paddle error] paddle.add(Tensor([10, 12],"float32"), Tensor([0, 12],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 12] and the shape of Y = [0, 12]. Received [10] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.168857 test begin: paddle.add(Tensor([10, 12],"float32"), Tensor([10, 0],"float32"), )

[paddle error] paddle.add(Tensor([10, 12],"float32"), Tensor([10, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 12] and the shape of Y = [10, 0]. Received [12] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.169886 test begin: paddle.add(Tensor([100, 0, 1],"float32"), Tensor([100, 0, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0, 1],"float32"), Tensor([100, 0, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.171113 test begin: paddle.add(Tensor([100, 0, 1],"float32"), Tensor([100, 0, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0, 1],"float32"), Tensor([100, 0, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.172654 test begin: paddle.add(Tensor([100, 0, 1],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 0, 1],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 0, 1] and the shape of Y = [100, 2, 1]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.173731 test begin: paddle.add(Tensor([100, 0, 1],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 0, 1],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 0, 1] and the shape of Y = [100, 2, 1]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.174729 test begin: paddle.add(Tensor([100, 0, 2, 2],"float32"), Tensor([100, 0, 2, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0, 2, 2],"float32"), Tensor([100, 0, 2, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.175952 test begin: paddle.add(Tensor([100, 0, 2, 2],"float32"), Tensor([100, 0, 2, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0, 2, 2],"float32"), Tensor([100, 0, 2, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.177044 test begin: paddle.add(Tensor([100, 0, 2, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0, 2, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.178414 test begin: paddle.add(Tensor([100, 0, 2, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0, 2, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.180066 test begin: paddle.add(Tensor([100, 0, 2],"float32"), Tensor([100, 0, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0, 2],"float32"), Tensor([100, 0, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.181119 test begin: paddle.add(Tensor([100, 0, 2],"float32"), Tensor([100, 0, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0, 2],"float32"), Tensor([100, 0, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.182153 test begin: paddle.add(Tensor([100, 0, 2],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0, 2],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.183446 test begin: paddle.add(Tensor([100, 0, 2],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0, 2],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.184951 test begin: paddle.add(Tensor([100, 0, 3, 1],"float32"), Tensor([100, 0, 3, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0, 3, 1],"float32"), Tensor([100, 0, 3, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.186039 test begin: paddle.add(Tensor([100, 0, 3, 1],"float32"), Tensor([100, 0, 3, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0, 3, 1],"float32"), Tensor([100, 0, 3, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.187102 test begin: paddle.add(Tensor([100, 0, 3, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 0, 3, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 0, 3, 1] and the shape of Y = [100, 2, 3, 1]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.188182 test begin: paddle.add(Tensor([100, 0, 3, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 0, 3, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 0, 3, 1] and the shape of Y = [100, 2, 3, 1]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.189249 test begin: paddle.add(Tensor([100, 0],"float16"), Tensor([1024],"float16"), )

[paddle error] paddle.add(Tensor([100, 0],"float16"), Tensor([1024],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 0] and the shape of Y = [1024]. Received [0] in X is not equal to [1024] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.190401 test begin: paddle.add(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.191480 test begin: paddle.add(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0],"float32"), Tensor([100, 0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.192571 test begin: paddle.add(Tensor([100, 0],"float32"), Tensor([100, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0],"float32"), Tensor([100, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.194206 test begin: paddle.add(Tensor([100, 0],"float32"), Tensor([100, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0],"float32"), Tensor([100, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.199543 test begin: paddle.add(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), name="Cauchy_rsample", )
2025-03-03 17:02:21.202912 test begin: paddle.add(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0],"float64"), Tensor([100, 0],"float64"), name="Cauchy_sample", )
2025-03-03 17:02:21.206488 test begin: paddle.add(Tensor([100, 0],"float64"), Tensor([100, 1],"float64"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 0],"float64"), Tensor([100, 1],"float64"), name="Cauchy_rsample", )
2025-03-03 17:02:21.211609 test begin: paddle.add(Tensor([100, 0],"float64"), Tensor([100, 1],"float64"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 0],"float64"), Tensor([100, 1],"float64"), name="Cauchy_sample", )
2025-03-03 17:02:21.214662 test begin: paddle.add(Tensor([100, 1, 0, 2],"float32"), Tensor([100, 1, 0, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 1, 0, 2],"float32"), Tensor([100, 1, 0, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.218801 test begin: paddle.add(Tensor([100, 1, 0, 2],"float32"), Tensor([100, 1, 0, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 1, 0, 2],"float32"), Tensor([100, 1, 0, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.221457 test begin: paddle.add(Tensor([100, 1, 0, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1, 0, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 0, 2] and the shape of Y = [100, 1, 2, 2]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.224483 test begin: paddle.add(Tensor([100, 1, 0, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1, 0, 2],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 0, 2] and the shape of Y = [100, 1, 2, 2]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.226076 test begin: paddle.add(Tensor([100, 1, 0],"float32"), Tensor([100, 1, 0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 1, 0],"float32"), Tensor([100, 1, 0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.227429 test begin: paddle.add(Tensor([100, 1, 0],"float32"), Tensor([100, 1, 0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 1, 0],"float32"), Tensor([100, 1, 0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.228711 test begin: paddle.add(Tensor([100, 1, 0],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1, 0],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 0] and the shape of Y = [100, 1, 2]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.229971 test begin: paddle.add(Tensor([100, 1, 0],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1, 0],"float32"), Tensor([100, 1, 2],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 0] and the shape of Y = [100, 1, 2]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.231232 test begin: paddle.add(Tensor([100, 1, 2, 0],"float32"), Tensor([100, 1, 2, 0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 1, 2, 0],"float32"), Tensor([100, 1, 2, 0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.232499 test begin: paddle.add(Tensor([100, 1, 2, 0],"float32"), Tensor([100, 1, 2, 0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 1, 2, 0],"float32"), Tensor([100, 1, 2, 0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.233821 test begin: paddle.add(Tensor([100, 1, 2, 0],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1, 2, 0],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2, 0] and the shape of Y = [100, 1, 2, 2]. Received [0] in X is not equal to [2] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.235064 test begin: paddle.add(Tensor([100, 1, 2, 0],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1, 2, 0],"float32"), Tensor([100, 1, 2, 2],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2, 0] and the shape of Y = [100, 1, 2, 2]. Received [0] in X is not equal to [2] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.236443 test begin: paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2, 2] and the shape of Y = [0, 1, 2, 2]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.237572 test begin: paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([0, 1, 2, 2],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2, 2] and the shape of Y = [0, 1, 2, 2]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.238758 test begin: paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 0, 2, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 0, 2, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.240265 test begin: paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 0, 2, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 0, 2, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.241632 test begin: paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 1, 0, 2],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 1, 0, 2],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2, 2] and the shape of Y = [100, 1, 0, 2]. Received [2] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.242692 test begin: paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 1, 0, 2],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 1, 0, 2],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2, 2] and the shape of Y = [100, 1, 0, 2]. Received [2] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.243873 test begin: paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 1, 2, 0],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 1, 2, 0],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2, 2] and the shape of Y = [100, 1, 2, 0]. Received [2] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.245352 test begin: paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 1, 2, 0],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1, 2, 2],"float32"), Tensor([100, 1, 2, 0],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2, 2] and the shape of Y = [100, 1, 2, 0]. Received [2] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.246411 test begin: paddle.add(Tensor([100, 1, 2],"float32"), Tensor([0, 1, 2],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1, 2],"float32"), Tensor([0, 1, 2],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2] and the shape of Y = [0, 1, 2]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.247407 test begin: paddle.add(Tensor([100, 1, 2],"float32"), Tensor([0, 1, 2],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1, 2],"float32"), Tensor([0, 1, 2],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2] and the shape of Y = [0, 1, 2]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.248466 test begin: paddle.add(Tensor([100, 1, 2],"float32"), Tensor([100, 0, 2],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 1, 2],"float32"), Tensor([100, 0, 2],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.249899 test begin: paddle.add(Tensor([100, 1, 2],"float32"), Tensor([100, 0, 2],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 1, 2],"float32"), Tensor([100, 0, 2],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.251513 test begin: paddle.add(Tensor([100, 1, 2],"float32"), Tensor([100, 1, 0],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1, 2],"float32"), Tensor([100, 1, 0],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2] and the shape of Y = [100, 1, 0]. Received [2] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.252751 test begin: paddle.add(Tensor([100, 1, 2],"float32"), Tensor([100, 1, 0],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1, 2],"float32"), Tensor([100, 1, 0],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1, 2] and the shape of Y = [100, 1, 0]. Received [2] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.253783 test begin: paddle.add(Tensor([100, 1024],"float16"), Tensor([0],"float16"), )

[paddle error] paddle.add(Tensor([100, 1024],"float16"), Tensor([0],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1024] and the shape of Y = [0]. Received [1024] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.256261 test begin: paddle.add(Tensor([100, 1],"float32"), Tensor([0, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1],"float32"), Tensor([0, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1] and the shape of Y = [0, 1]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.257556 test begin: paddle.add(Tensor([100, 1],"float32"), Tensor([0, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1],"float32"), Tensor([0, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1] and the shape of Y = [0, 1]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.258558 test begin: paddle.add(Tensor([100, 1],"float32"), Tensor([100, 0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 1],"float32"), Tensor([100, 0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.259920 test begin: paddle.add(Tensor([100, 1],"float32"), Tensor([100, 0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 1],"float32"), Tensor([100, 0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.261216 test begin: paddle.add(Tensor([100, 1],"float64"), Tensor([0, 1],"float64"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 1],"float64"), Tensor([0, 1],"float64"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1] and the shape of Y = [0, 1]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.262245 test begin: paddle.add(Tensor([100, 1],"float64"), Tensor([0, 1],"float64"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 1],"float64"), Tensor([0, 1],"float64"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 1] and the shape of Y = [0, 1]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.263217 test begin: paddle.add(Tensor([100, 1],"float64"), Tensor([100, 0],"float64"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 1],"float64"), Tensor([100, 0],"float64"), name="Cauchy_rsample", )
2025-03-03 17:02:21.264603 test begin: paddle.add(Tensor([100, 1],"float64"), Tensor([100, 0],"float64"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 1],"float64"), Tensor([100, 0],"float64"), name="Cauchy_sample", )
2025-03-03 17:02:21.265922 test begin: paddle.add(Tensor([100, 2, 0, 1],"float32"), Tensor([100, 2, 0, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 2, 0, 1],"float32"), Tensor([100, 2, 0, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.266996 test begin: paddle.add(Tensor([100, 2, 0, 1],"float32"), Tensor([100, 2, 0, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 2, 0, 1],"float32"), Tensor([100, 2, 0, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.268134 test begin: paddle.add(Tensor([100, 2, 0, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 2, 0, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 0, 1] and the shape of Y = [100, 2, 3, 1]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.269149 test begin: paddle.add(Tensor([100, 2, 0, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 2, 0, 1],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 0, 1] and the shape of Y = [100, 2, 3, 1]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.270202 test begin: paddle.add(Tensor([100, 2, 0],"float32"), Tensor([100, 2, 0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 2, 0],"float32"), Tensor([100, 2, 0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.271284 test begin: paddle.add(Tensor([100, 2, 0],"float32"), Tensor([100, 2, 0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 2, 0],"float32"), Tensor([100, 2, 0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.272301 test begin: paddle.add(Tensor([100, 2, 0],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 2, 0],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.273587 test begin: paddle.add(Tensor([100, 2, 0],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 2, 0],"float32"), Tensor([100, 2, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.274883 test begin: paddle.add(Tensor([100, 2, 1],"float32"), Tensor([0, 2, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 2, 1],"float32"), Tensor([0, 2, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 1] and the shape of Y = [0, 2, 1]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.276038 test begin: paddle.add(Tensor([100, 2, 1],"float32"), Tensor([0, 2, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 2, 1],"float32"), Tensor([0, 2, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 1] and the shape of Y = [0, 2, 1]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.277079 test begin: paddle.add(Tensor([100, 2, 1],"float32"), Tensor([100, 0, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 2, 1],"float32"), Tensor([100, 0, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 1] and the shape of Y = [100, 0, 1]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.278072 test begin: paddle.add(Tensor([100, 2, 1],"float32"), Tensor([100, 0, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 2, 1],"float32"), Tensor([100, 0, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 1] and the shape of Y = [100, 0, 1]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.279077 test begin: paddle.add(Tensor([100, 2, 1],"float32"), Tensor([100, 2, 0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 2, 1],"float32"), Tensor([100, 2, 0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.280411 test begin: paddle.add(Tensor([100, 2, 1],"float32"), Tensor([100, 2, 0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 2, 1],"float32"), Tensor([100, 2, 0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.281744 test begin: paddle.add(Tensor([100, 2, 3, 0],"float32"), Tensor([100, 2, 3, 0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 2, 3, 0],"float32"), Tensor([100, 2, 3, 0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.282989 test begin: paddle.add(Tensor([100, 2, 3, 0],"float32"), Tensor([100, 2, 3, 0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 2, 3, 0],"float32"), Tensor([100, 2, 3, 0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.284093 test begin: paddle.add(Tensor([100, 2, 3, 0],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 2, 3, 0],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.285342 test begin: paddle.add(Tensor([100, 2, 3, 0],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 2, 3, 0],"float32"), Tensor([100, 2, 3, 1],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.286603 test begin: paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([0, 2, 3, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([0, 2, 3, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 3, 1] and the shape of Y = [0, 2, 3, 1]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.287676 test begin: paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([0, 2, 3, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([0, 2, 3, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 3, 1] and the shape of Y = [0, 2, 3, 1]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.288676 test begin: paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 0, 3, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 0, 3, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 3, 1] and the shape of Y = [100, 0, 3, 1]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.289862 test begin: paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 0, 3, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 0, 3, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 3, 1] and the shape of Y = [100, 0, 3, 1]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.290852 test begin: paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 2, 0, 1],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 2, 0, 1],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 3, 1] and the shape of Y = [100, 2, 0, 1]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.291916 test begin: paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 2, 0, 1],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 2, 0, 1],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100, 2, 3, 1] and the shape of Y = [100, 2, 0, 1]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.292907 test begin: paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 2, 3, 0],"float32"), name="Cauchy_rsample", )

[Pass] paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 2, 3, 0],"float32"), name="Cauchy_rsample", )
2025-03-03 17:02:21.294262 test begin: paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 2, 3, 0],"float32"), name="Cauchy_sample", )

[Pass] paddle.add(Tensor([100, 2, 3, 1],"float32"), Tensor([100, 2, 3, 0],"float32"), name="Cauchy_sample", )
2025-03-03 17:02:21.295591 test begin: paddle.add(Tensor([10000, 0, 3],"float64"), Tensor([5, 3],"float64"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([10000, 0, 3],"float64"), Tensor([5, 3],"float64"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10000, 0, 3] and the shape of Y = [5, 3]. Received [0] in X is not equal to [5] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.296551 test begin: paddle.add(Tensor([10000, 0],"float64"), Tensor([4],"float64"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([10000, 0],"float64"), Tensor([4],"float64"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10000, 0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.297573 test begin: paddle.add(Tensor([10000, 4],"float64"), Tensor([0],"float64"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([10000, 4],"float64"), Tensor([0],"float64"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10000, 4] and the shape of Y = [0]. Received [4] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.299325 test begin: paddle.add(Tensor([10000, 5, 0],"float64"), Tensor([5, 3],"float64"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([10000, 5, 0],"float64"), Tensor([5, 3],"float64"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10000, 5, 0] and the shape of Y = [5, 3]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.300274 test begin: paddle.add(Tensor([10000, 5, 3],"float64"), Tensor([0, 3],"float64"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([10000, 5, 3],"float64"), Tensor([0, 3],"float64"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10000, 5, 3] and the shape of Y = [0, 3]. Received [5] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.303096 test begin: paddle.add(Tensor([10000, 5, 3],"float64"), Tensor([5, 0],"float64"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([10000, 5, 3],"float64"), Tensor([5, 0],"float64"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10000, 5, 3] and the shape of Y = [5, 0]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.305478 test begin: paddle.add(Tensor([100000, 0],"complex128"), Tensor([4],"complex64"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([100000, 0],"complex128"), Tensor([4],"complex64"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100000, 0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.306618 test begin: paddle.add(Tensor([100000, 0],"float64"), Tensor([4],"float64"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([100000, 0],"float64"), Tensor([4],"float64"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100000, 0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.307555 test begin: paddle.add(Tensor([100000, 4],"complex128"), Tensor([0],"complex64"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([100000, 4],"complex128"), Tensor([0],"complex64"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100000, 4] and the shape of Y = [0]. Received [4] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.313508 test begin: paddle.add(Tensor([100000, 4],"float64"), Tensor([0],"float64"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([100000, 4],"float64"), Tensor([0],"float64"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100000, 4] and the shape of Y = [0]. Received [4] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.318705 test begin: paddle.add(Tensor([100],"float32"), Tensor([0],"float32"), name="Cauchy_rsample", )

[paddle error] paddle.add(Tensor([100],"float32"), Tensor([0],"float32"), name="Cauchy_rsample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100] and the shape of Y = [0]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.319813 test begin: paddle.add(Tensor([100],"float32"), Tensor([0],"float32"), name="Cauchy_sample", )

[paddle error] paddle.add(Tensor([100],"float32"), Tensor([0],"float32"), name="Cauchy_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [100] and the shape of Y = [0]. Received [100] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.320966 test begin: paddle.add(Tensor([101, 0],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.add(Tensor([101, 0],"float16"), Tensor([192],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [101, 0] and the shape of Y = [192]. Received [0] in X is not equal to [192] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.322027 test begin: paddle.add(Tensor([101, 192],"float16"), Tensor([0],"float16"), )

[paddle error] paddle.add(Tensor([101, 192],"float16"), Tensor([0],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [101, 192] and the shape of Y = [0]. Received [192] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.323343 test begin: paddle.add(Tensor([12544, 0, 24],"float16"), Tensor([12544, 0, 24],"float16"), )

[Pass] paddle.add(Tensor([12544, 0, 24],"float16"), Tensor([12544, 0, 24],"float16"), )
2025-03-03 17:02:21.324524 test begin: paddle.add(Tensor([12544, 0, 24],"float16"), Tensor([12544, 16, 24],"float16"), )

[paddle error] paddle.add(Tensor([12544, 0, 24],"float16"), Tensor([12544, 16, 24],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 0, 24] and the shape of Y = [12544, 16, 24]. Received [0] in X is not equal to [16] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.391674 test begin: paddle.add(Tensor([12544, 0, 24],"float32"), Tensor([12544, 0, 24],"float32"), )

[Pass] paddle.add(Tensor([12544, 0, 24],"float32"), Tensor([12544, 0, 24],"float32"), )
2025-03-03 17:02:21.393230 test begin: paddle.add(Tensor([12544, 0, 24],"float32"), Tensor([12544, 16, 24],"float32"), )

[paddle error] paddle.add(Tensor([12544, 0, 24],"float32"), Tensor([12544, 16, 24],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 0, 24] and the shape of Y = [12544, 16, 24]. Received [0] in X is not equal to [16] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.444380 test begin: paddle.add(Tensor([12544, 16, 0],"float16"), Tensor([12544, 16, 0],"float16"), )

[Pass] paddle.add(Tensor([12544, 16, 0],"float16"), Tensor([12544, 16, 0],"float16"), )
2025-03-03 17:02:21.445949 test begin: paddle.add(Tensor([12544, 16, 0],"float16"), Tensor([12544, 16, 24],"float16"), )

[paddle error] paddle.add(Tensor([12544, 16, 0],"float16"), Tensor([12544, 16, 24],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 16, 0] and the shape of Y = [12544, 16, 24]. Received [0] in X is not equal to [24] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.511995 test begin: paddle.add(Tensor([12544, 16, 0],"float32"), Tensor([12544, 16, 0],"float32"), )

[Pass] paddle.add(Tensor([12544, 16, 0],"float32"), Tensor([12544, 16, 0],"float32"), )
2025-03-03 17:02:21.513542 test begin: paddle.add(Tensor([12544, 16, 0],"float32"), Tensor([12544, 16, 24],"float32"), )

[paddle error] paddle.add(Tensor([12544, 16, 0],"float32"), Tensor([12544, 16, 24],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 16, 0] and the shape of Y = [12544, 16, 24]. Received [0] in X is not equal to [24] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.568100 test begin: paddle.add(Tensor([12544, 16, 24],"float16"), Tensor([0, 16, 24],"float16"), )

[paddle error] paddle.add(Tensor([12544, 16, 24],"float16"), Tensor([0, 16, 24],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 16, 24] and the shape of Y = [0, 16, 24]. Received [12544] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.641577 test begin: paddle.add(Tensor([12544, 16, 24],"float16"), Tensor([12544, 0, 24],"float16"), )

[paddle error] paddle.add(Tensor([12544, 16, 24],"float16"), Tensor([12544, 0, 24],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 16, 24] and the shape of Y = [12544, 0, 24]. Received [16] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.715337 test begin: paddle.add(Tensor([12544, 16, 24],"float16"), Tensor([12544, 16, 0],"float16"), )

[paddle error] paddle.add(Tensor([12544, 16, 24],"float16"), Tensor([12544, 16, 0],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 16, 24] and the shape of Y = [12544, 16, 0]. Received [24] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.784052 test begin: paddle.add(Tensor([12544, 16, 24],"float32"), Tensor([0, 16, 24],"float32"), )

[paddle error] paddle.add(Tensor([12544, 16, 24],"float32"), Tensor([0, 16, 24],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 16, 24] and the shape of Y = [0, 16, 24]. Received [12544] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.832487 test begin: paddle.add(Tensor([12544, 16, 24],"float32"), Tensor([12544, 0, 24],"float32"), )

[paddle error] paddle.add(Tensor([12544, 16, 24],"float32"), Tensor([12544, 0, 24],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 16, 24] and the shape of Y = [12544, 0, 24]. Received [16] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.881698 test begin: paddle.add(Tensor([12544, 16, 24],"float32"), Tensor([12544, 16, 0],"float32"), )

[paddle error] paddle.add(Tensor([12544, 16, 24],"float32"), Tensor([12544, 16, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [12544, 16, 24] and the shape of Y = [12544, 16, 0]. Received [24] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.940065 test begin: paddle.add(Tensor([13, 0, 99],"float32"), Tensor([99],"float32"), )

[Pass] paddle.add(Tensor([13, 0, 99],"float32"), Tensor([99],"float32"), )
2025-03-03 17:02:21.942481 test begin: paddle.add(Tensor([13, 7, 0],"float32"), Tensor([99],"float32"), )

[paddle error] paddle.add(Tensor([13, 7, 0],"float32"), Tensor([99],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 7, 0] and the shape of Y = [99]. Received [0] in X is not equal to [99] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.943612 test begin: paddle.add(Tensor([13, 7, 99],"float32"), Tensor([0],"float32"), )

[paddle error] paddle.add(Tensor([13, 7, 99],"float32"), Tensor([0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 7, 99] and the shape of Y = [0]. Received [99] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.945534 test begin: paddle.add(Tensor([16, 0, 25, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([16, 0, 25, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )
2025-03-03 17:02:21.947531 test begin: paddle.add(Tensor([16, 0, 3, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([16, 0, 3, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )
2025-03-03 17:02:21.949230 test begin: paddle.add(Tensor([16, 1, 0, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([16, 1, 0, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )
2025-03-03 17:02:21.950910 test begin: paddle.add(Tensor([16, 1, 25, 0],"float32"), Tensor([1],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([16, 1, 25, 0],"float32"), Tensor([1],"float32"), name="Normal_sample", )
2025-03-03 17:02:21.952496 test begin: paddle.add(Tensor([16, 1, 25, 1],"float32"), Tensor([0],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([16, 1, 25, 1],"float32"), Tensor([0],"float32"), name="Normal_sample", )
2025-03-03 17:02:21.954180 test begin: paddle.add(Tensor([16, 2, 0, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([16, 2, 0, 1],"float32"), Tensor([1],"float32"), name="Normal_sample", )
2025-03-03 17:02:21.955732 test begin: paddle.add(Tensor([16, 2, 3, 0],"float32"), Tensor([1],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([16, 2, 3, 0],"float32"), Tensor([1],"float32"), name="Normal_sample", )
2025-03-03 17:02:21.957404 test begin: paddle.add(Tensor([16, 2, 3, 1],"float32"), Tensor([0],"float32"), name="Normal_sample", )

[Pass] paddle.add(Tensor([16, 2, 3, 1],"float32"), Tensor([0],"float32"), name="Normal_sample", )
2025-03-03 17:02:21.959215 test begin: paddle.add(Tensor([168],"int64"), Tensor([0],"int64"), )

[paddle error] paddle.add(Tensor([168],"int64"), Tensor([0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [168] and the shape of Y = [0]. Received [168] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.960634 test begin: paddle.add(Tensor([1],"float32"), Tensor([0],"float32"), )

[Pass] paddle.add(Tensor([1],"float32"), Tensor([0],"float32"), )
2025-03-03 17:02:21.962249 test begin: paddle.add(Tensor([1],"float32"), Tensor([0],"float32"), name="Cauchy_entropy", )

[Pass] paddle.add(Tensor([1],"float32"), Tensor([0],"float32"), name="Cauchy_entropy", )
2025-03-03 17:02:21.963771 test begin: paddle.add(Tensor([2, 0, 1, 1000],"float32"), Tensor([1000],"float32"), )

[Pass] paddle.add(Tensor([2, 0, 1, 1000],"float32"), Tensor([1000],"float32"), )
2025-03-03 17:02:21.965416 test begin: paddle.add(Tensor([2, 0, 1, 10],"float32"), Tensor([10],"float32"), )

[Pass] paddle.add(Tensor([2, 0, 1, 10],"float32"), Tensor([10],"float32"), )
2025-03-03 17:02:21.966984 test begin: paddle.add(Tensor([2, 0, 3, 4],"float32"), Tensor([3, 1, 1],"float32"), )

[paddle error] paddle.add(Tensor([2, 0, 3, 4],"float32"), Tensor([3, 1, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0, 3, 4] and the shape of Y = [3, 1, 1]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.968351 test begin: paddle.add(Tensor([2, 0, 3, 4],"float32"), Tensor([3, 1, 4],"float32"), )

[paddle error] paddle.add(Tensor([2, 0, 3, 4],"float32"), Tensor([3, 1, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0, 3, 4] and the shape of Y = [3, 1, 4]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.970251 test begin: paddle.add(Tensor([2, 0, 4, 5],"complex128"), Tensor([4, 5],"float64"), )

[Pass] paddle.add(Tensor([2, 0, 4, 5],"complex128"), Tensor([4, 5],"float64"), )
2025-03-03 17:02:21.972208 test begin: paddle.add(Tensor([2, 0, 4, 5],"complex64"), Tensor([4, 5],"float32"), )

[Pass] paddle.add(Tensor([2, 0, 4, 5],"complex64"), Tensor([4, 5],"float32"), )
2025-03-03 17:02:21.974148 test begin: paddle.add(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), name="Normal_entropy", )

[Pass] paddle.add(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), name="Normal_entropy", )
2025-03-03 17:02:21.975300 test begin: paddle.add(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), name="Normal_kl_divergence", )

[Pass] paddle.add(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), name="Normal_kl_divergence", )
2025-03-03 17:02:21.976378 test begin: paddle.add(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), name=None, )

[Pass] paddle.add(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), name=None, )
2025-03-03 17:02:21.977534 test begin: paddle.add(Tensor([2, 0],"float32"), Tensor([2, 3],"float32"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([2, 0],"float32"), Tensor([2, 3],"float32"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.978650 test begin: paddle.add(Tensor([2, 0],"float32"), Tensor([2, 3],"float32"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([2, 0],"float32"), Tensor([2, 3],"float32"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.979652 test begin: paddle.add(Tensor([2, 0],"float32"), Tensor([2, 3],"float32"), name=None, )

[paddle error] paddle.add(Tensor([2, 0],"float32"), Tensor([2, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.981079 test begin: paddle.add(Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), name="Normal_entropy", )

[Pass] paddle.add(Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), name="Normal_entropy", )
2025-03-03 17:02:21.982568 test begin: paddle.add(Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), name="Normal_kl_divergence", )

[Pass] paddle.add(Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), name="Normal_kl_divergence", )
2025-03-03 17:02:21.983936 test begin: paddle.add(Tensor([2, 0],"float64"), Tensor([2, 2],"float64"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([2, 0],"float64"), Tensor([2, 2],"float64"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 2]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.985224 test begin: paddle.add(Tensor([2, 0],"float64"), Tensor([2, 3],"float64"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([2, 0],"float64"), Tensor([2, 3],"float64"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.986845 test begin: paddle.add(Tensor([2, 2],"float64"), Tensor([0, 2],"float64"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([2, 2],"float64"), Tensor([0, 2],"float64"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 2] and the shape of Y = [0, 2]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.988079 test begin: paddle.add(Tensor([2, 2],"float64"), Tensor([2, 0],"float64"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([2, 2],"float64"), Tensor([2, 0],"float64"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 2] and the shape of Y = [2, 0]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.989191 test begin: paddle.add(Tensor([2, 3, 0, 1000],"float32"), Tensor([1000],"float32"), )

[Pass] paddle.add(Tensor([2, 3, 0, 1000],"float32"), Tensor([1000],"float32"), )
2025-03-03 17:02:21.990528 test begin: paddle.add(Tensor([2, 3, 0, 10],"float32"), Tensor([10],"float32"), )

[Pass] paddle.add(Tensor([2, 3, 0, 10],"float32"), Tensor([10],"float32"), )
2025-03-03 17:02:21.991858 test begin: paddle.add(Tensor([2, 3, 0, 4],"float32"), Tensor([3, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([2, 3, 0, 4],"float32"), Tensor([3, 1, 1],"float32"), )
2025-03-03 17:02:21.993215 test begin: paddle.add(Tensor([2, 3, 0, 4],"float32"), Tensor([3, 1, 4],"float32"), )

[Pass] paddle.add(Tensor([2, 3, 0, 4],"float32"), Tensor([3, 1, 4],"float32"), )
2025-03-03 17:02:21.994541 test begin: paddle.add(Tensor([2, 3, 0, 5],"complex128"), Tensor([4, 5],"float64"), )

[paddle error] paddle.add(Tensor([2, 3, 0, 5],"complex128"), Tensor([4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 0, 5] and the shape of Y = [4, 5]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.995852 test begin: paddle.add(Tensor([2, 3, 0, 5],"complex64"), Tensor([4, 5],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 0, 5],"complex64"), Tensor([4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 0, 5] and the shape of Y = [4, 5]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.997055 test begin: paddle.add(Tensor([2, 3, 1, 0],"float32"), Tensor([1000],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 1, 0],"float32"), Tensor([1000],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 1, 0] and the shape of Y = [1000]. Received [0] in X is not equal to [1000] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.998111 test begin: paddle.add(Tensor([2, 3, 1, 0],"float32"), Tensor([10],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 1, 0],"float32"), Tensor([10],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 1, 0] and the shape of Y = [10]. Received [0] in X is not equal to [10] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:21.999321 test begin: paddle.add(Tensor([2, 3, 1, 1000],"float32"), Tensor([0],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 1, 1000],"float32"), Tensor([0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 1, 1000] and the shape of Y = [0]. Received [1000] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.000594 test begin: paddle.add(Tensor([2, 3, 1, 10],"float32"), Tensor([0],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 1, 10],"float32"), Tensor([0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 1, 10] and the shape of Y = [0]. Received [10] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.001709 test begin: paddle.add(Tensor([2, 3, 3, 0],"float32"), Tensor([3, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([2, 3, 3, 0],"float32"), Tensor([3, 1, 1],"float32"), )
2025-03-03 17:02:22.003302 test begin: paddle.add(Tensor([2, 3, 3, 0],"float32"), Tensor([3, 1, 4],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 3, 0],"float32"), Tensor([3, 1, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 0] and the shape of Y = [3, 1, 4]. Received [0] in X is not equal to [4] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.004370 test begin: paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([0, 1, 1],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([0, 1, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 4] and the shape of Y = [0, 1, 1]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.005423 test begin: paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([0, 1, 4],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([0, 1, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 4] and the shape of Y = [0, 1, 4]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.006449 test begin: paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 0, 1],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 0, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 4] and the shape of Y = [3, 0, 1]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.007490 test begin: paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 0, 4],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 0, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 4] and the shape of Y = [3, 0, 4]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.008511 test begin: paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 1, 0],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 1, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 4] and the shape of Y = [3, 1, 0]. Received [4] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.009542 test begin: paddle.add(Tensor([2, 3, 4, 0],"complex128"), Tensor([4, 5],"float64"), )

[paddle error] paddle.add(Tensor([2, 3, 4, 0],"complex128"), Tensor([4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 0] and the shape of Y = [4, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.010602 test begin: paddle.add(Tensor([2, 3, 4, 0],"complex64"), Tensor([4, 5],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 4, 0],"complex64"), Tensor([4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 0] and the shape of Y = [4, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.012772 test begin: paddle.add(Tensor([2, 3, 4, 5],"complex128"), Tensor([0, 5],"float64"), )

[paddle error] paddle.add(Tensor([2, 3, 4, 5],"complex128"), Tensor([0, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.014225 test begin: paddle.add(Tensor([2, 3, 4, 5],"complex128"), Tensor([4, 0],"float64"), )

[paddle error] paddle.add(Tensor([2, 3, 4, 5],"complex128"), Tensor([4, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [4, 0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.015516 test begin: paddle.add(Tensor([2, 3, 4, 5],"complex64"), Tensor([0, 5],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 4, 5],"complex64"), Tensor([0, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.016562 test begin: paddle.add(Tensor([2, 3, 4, 5],"complex64"), Tensor([4, 0],"float32"), )

[paddle error] paddle.add(Tensor([2, 3, 4, 5],"complex64"), Tensor([4, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [4, 0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.017602 test begin: paddle.add(Tensor([2, 3],"float32"), Tensor([0, 3],"float32"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([2, 3],"float32"), Tensor([0, 3],"float32"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [0, 3]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.018615 test begin: paddle.add(Tensor([2, 3],"float32"), Tensor([0, 3],"float32"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([2, 3],"float32"), Tensor([0, 3],"float32"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [0, 3]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.019657 test begin: paddle.add(Tensor([2, 3],"float32"), Tensor([0, 3],"float32"), name=None, )

[paddle error] paddle.add(Tensor([2, 3],"float32"), Tensor([0, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [0, 3]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.020709 test begin: paddle.add(Tensor([2, 3],"float32"), Tensor([2, 0],"float32"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([2, 3],"float32"), Tensor([2, 0],"float32"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [2, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.022023 test begin: paddle.add(Tensor([2, 3],"float32"), Tensor([2, 0],"float32"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([2, 3],"float32"), Tensor([2, 0],"float32"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [2, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.023102 test begin: paddle.add(Tensor([2, 3],"float32"), Tensor([2, 0],"float32"), name=None, )

[paddle error] paddle.add(Tensor([2, 3],"float32"), Tensor([2, 0],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [2, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.024423 test begin: paddle.add(Tensor([2, 3],"float64"), Tensor([0, 3],"float64"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([2, 3],"float64"), Tensor([0, 3],"float64"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [0, 3]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.025418 test begin: paddle.add(Tensor([2, 3],"float64"), Tensor([2, 0],"float64"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([2, 3],"float64"), Tensor([2, 0],"float64"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [2, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.026385 test begin: paddle.add(Tensor([2],"float64"), Tensor([0],"float64"), name="Normal_entropy", )

[paddle error] paddle.add(Tensor([2],"float64"), Tensor([0],"float64"), name="Normal_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2] and the shape of Y = [0]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.027343 test begin: paddle.add(Tensor([2],"float64"), Tensor([0],"float64"), name="Normal_kl_divergence", )

[paddle error] paddle.add(Tensor([2],"float64"), Tensor([0],"float64"), name="Normal_kl_divergence", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2] and the shape of Y = [0]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.028951 test begin: paddle.add(Tensor([3],"float32"), Tensor([0],"float32"), name="Cauchy_entropy", )

[paddle error] paddle.add(Tensor([3],"float32"), Tensor([0],"float32"), name="Cauchy_entropy", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3] and the shape of Y = [0]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.030295 test begin: paddle.add(Tensor([40, 0],"bfloat16"), Tensor([40, 0],"bfloat16"), name=None, )

[Pass] paddle.add(Tensor([40, 0],"bfloat16"), Tensor([40, 0],"bfloat16"), name=None, )
2025-03-03 17:02:22.031894 test begin: paddle.add(Tensor([40, 0],"bfloat16"), Tensor([40, 40],"bfloat16"), name=None, )

[paddle error] paddle.add(Tensor([40, 0],"bfloat16"), Tensor([40, 40],"bfloat16"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [40, 0] and the shape of Y = [40, 40]. Received [0] in X is not equal to [40] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.033330 test begin: paddle.add(Tensor([40, 40],"bfloat16"), Tensor([0, 40],"bfloat16"), name=None, )

[paddle error] paddle.add(Tensor([40, 40],"bfloat16"), Tensor([0, 40],"bfloat16"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [40, 40] and the shape of Y = [0, 40]. Received [40] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.034864 test begin: paddle.add(Tensor([40, 40],"bfloat16"), Tensor([40, 0],"bfloat16"), name=None, )

[paddle error] paddle.add(Tensor([40, 40],"bfloat16"), Tensor([40, 0],"bfloat16"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [40, 40] and the shape of Y = [40, 0]. Received [40] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.036356 test begin: paddle.add(Tensor([40],"int64"), Tensor([0, 40],"int64"), )

[Pass] paddle.add(Tensor([40],"int64"), Tensor([0, 40],"int64"), )
2025-03-03 17:02:22.038200 test begin: paddle.add(Tensor([40],"int64"), Tensor([300, 0],"int64"), )

[paddle error] paddle.add(Tensor([40],"int64"), Tensor([300, 0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [40] and the shape of Y = [300, 0]. Received [40] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.039565 test begin: paddle.add(Tensor([5, 0, 15, 20],"float32"), Tensor([5, 0, 15, 20],"float32"), name=None, )

[Pass] paddle.add(Tensor([5, 0, 15, 20],"float32"), Tensor([5, 0, 15, 20],"float32"), name=None, )
2025-03-03 17:02:22.040996 test begin: paddle.add(Tensor([5, 0, 15, 20],"float32"), Tensor([5, 10, 15, 20],"float32"), name=None, )

[paddle error] paddle.add(Tensor([5, 0, 15, 20],"float32"), Tensor([5, 10, 15, 20],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 0, 15, 20] and the shape of Y = [5, 10, 15, 20]. Received [0] in X is not equal to [10] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.042568 test begin: paddle.add(Tensor([5, 10, 0, 20],"float32"), Tensor([5, 10, 0, 20],"float32"), name=None, )

[Pass] paddle.add(Tensor([5, 10, 0, 20],"float32"), Tensor([5, 10, 0, 20],"float32"), name=None, )
2025-03-03 17:02:22.044146 test begin: paddle.add(Tensor([5, 10, 0, 20],"float32"), Tensor([5, 10, 15, 20],"float32"), name=None, )

[paddle error] paddle.add(Tensor([5, 10, 0, 20],"float32"), Tensor([5, 10, 15, 20],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 10, 0, 20] and the shape of Y = [5, 10, 15, 20]. Received [0] in X is not equal to [15] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.045550 test begin: paddle.add(Tensor([5, 10, 15, 0],"float32"), Tensor([5, 10, 15, 0],"float32"), name=None, )

[Pass] paddle.add(Tensor([5, 10, 15, 0],"float32"), Tensor([5, 10, 15, 0],"float32"), name=None, )
2025-03-03 17:02:22.046661 test begin: paddle.add(Tensor([5, 10, 15, 0],"float32"), Tensor([5, 10, 15, 20],"float32"), name=None, )

[paddle error] paddle.add(Tensor([5, 10, 15, 0],"float32"), Tensor([5, 10, 15, 20],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 10, 15, 0] and the shape of Y = [5, 10, 15, 20]. Received [0] in X is not equal to [20] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.048208 test begin: paddle.add(Tensor([5, 10, 15, 20],"float32"), Tensor([0, 10, 15, 20],"float32"), name=None, )

[paddle error] paddle.add(Tensor([5, 10, 15, 20],"float32"), Tensor([0, 10, 15, 20],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 10, 15, 20] and the shape of Y = [0, 10, 15, 20]. Received [5] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.049364 test begin: paddle.add(Tensor([5, 10, 15, 20],"float32"), Tensor([5, 0, 15, 20],"float32"), name=None, )

[paddle error] paddle.add(Tensor([5, 10, 15, 20],"float32"), Tensor([5, 0, 15, 20],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 10, 15, 20] and the shape of Y = [5, 0, 15, 20]. Received [10] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.050456 test begin: paddle.add(Tensor([5, 10, 15, 20],"float32"), Tensor([5, 10, 0, 20],"float32"), name=None, )

[paddle error] paddle.add(Tensor([5, 10, 15, 20],"float32"), Tensor([5, 10, 0, 20],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 10, 15, 20] and the shape of Y = [5, 10, 0, 20]. Received [15] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.051546 test begin: paddle.add(Tensor([5, 10, 15, 20],"float32"), Tensor([5, 10, 15, 0],"float32"), name=None, )

[paddle error] paddle.add(Tensor([5, 10, 15, 20],"float32"), Tensor([5, 10, 15, 0],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [5, 10, 15, 20] and the shape of Y = [5, 10, 15, 0]. Received [20] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.052649 test begin: paddle.add(Tensor([64, 0, 512, 1, 40],"float16"), Tensor([64, 0, 512, 1, 1],"float16"), )

[Pass] paddle.add(Tensor([64, 0, 512, 1, 40],"float16"), Tensor([64, 0, 512, 1, 1],"float16"), )
2025-03-03 17:02:22.053776 test begin: paddle.add(Tensor([64, 0, 512, 1, 40],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), )

[paddle error] paddle.add(Tensor([64, 0, 512, 1, 40],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 0, 512, 1, 40] and the shape of Y = [64, 26, 512, 1, 1]. Received [0] in X is not equal to [26] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.068087 test begin: paddle.add(Tensor([64, 0, 512, 1, 40],"float32"), Tensor([64, 0, 512, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([64, 0, 512, 1, 40],"float32"), Tensor([64, 0, 512, 1, 1],"float32"), )
2025-03-03 17:02:22.070395 test begin: paddle.add(Tensor([64, 0, 512, 1, 40],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), )

[paddle error] paddle.add(Tensor([64, 0, 512, 1, 40],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 0, 512, 1, 40] and the shape of Y = [64, 26, 512, 1, 1]. Received [0] in X is not equal to [26] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.081848 test begin: paddle.add(Tensor([64, 1, 0, 1, 40],"float16"), Tensor([64, 26, 0, 1, 1],"float16"), )

[Pass] paddle.add(Tensor([64, 1, 0, 1, 40],"float16"), Tensor([64, 26, 0, 1, 1],"float16"), )
2025-03-03 17:02:22.083914 test begin: paddle.add(Tensor([64, 1, 0, 1, 40],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), )

[paddle error] paddle.add(Tensor([64, 1, 0, 1, 40],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 1, 0, 1, 40] and the shape of Y = [64, 26, 512, 1, 1]. Received [0] in X is not equal to [512] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.098746 test begin: paddle.add(Tensor([64, 1, 0, 1, 40],"float32"), Tensor([64, 26, 0, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([64, 1, 0, 1, 40],"float32"), Tensor([64, 26, 0, 1, 1],"float32"), )
2025-03-03 17:02:22.101061 test begin: paddle.add(Tensor([64, 1, 0, 1, 40],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), )

[paddle error] paddle.add(Tensor([64, 1, 0, 1, 40],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 1, 0, 1, 40] and the shape of Y = [64, 26, 512, 1, 1]. Received [0] in X is not equal to [512] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.111788 test begin: paddle.add(Tensor([64, 1, 512, 0, 40],"float16"), Tensor([64, 26, 512, 0, 1],"float16"), )

[Pass] paddle.add(Tensor([64, 1, 512, 0, 40],"float16"), Tensor([64, 26, 512, 0, 1],"float16"), )
2025-03-03 17:02:22.113433 test begin: paddle.add(Tensor([64, 1, 512, 0, 40],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), )

[Pass] paddle.add(Tensor([64, 1, 512, 0, 40],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), )
2025-03-03 17:02:22.132224 test begin: paddle.add(Tensor([64, 1, 512, 0, 40],"float32"), Tensor([64, 26, 512, 0, 1],"float32"), )

[Pass] paddle.add(Tensor([64, 1, 512, 0, 40],"float32"), Tensor([64, 26, 512, 0, 1],"float32"), )
2025-03-03 17:02:22.133756 test begin: paddle.add(Tensor([64, 1, 512, 0, 40],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([64, 1, 512, 0, 40],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), )
2025-03-03 17:02:22.144315 test begin: paddle.add(Tensor([64, 1, 512, 1, 0],"float16"), Tensor([64, 26, 512, 1, 0],"float16"), )

[Pass] paddle.add(Tensor([64, 1, 512, 1, 0],"float16"), Tensor([64, 26, 512, 1, 0],"float16"), )
2025-03-03 17:02:22.145602 test begin: paddle.add(Tensor([64, 1, 512, 1, 0],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), )

[Pass] paddle.add(Tensor([64, 1, 512, 1, 0],"float16"), Tensor([64, 26, 512, 1, 1],"float16"), )
2025-03-03 17:02:22.160132 test begin: paddle.add(Tensor([64, 1, 512, 1, 0],"float32"), Tensor([64, 26, 512, 1, 0],"float32"), )

[Pass] paddle.add(Tensor([64, 1, 512, 1, 0],"float32"), Tensor([64, 26, 512, 1, 0],"float32"), )
2025-03-03 17:02:22.161423 test begin: paddle.add(Tensor([64, 1, 512, 1, 0],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([64, 1, 512, 1, 0],"float32"), Tensor([64, 26, 512, 1, 1],"float32"), )
2025-03-03 17:02:22.172748 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([0, 26, 512, 1, 1],"float16"), )

[paddle error] paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([0, 26, 512, 1, 1],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 1, 512, 1, 40] and the shape of Y = [0, 26, 512, 1, 1]. Received [64] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.192605 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([64, 0, 512, 1, 1],"float16"), )

[Pass] paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([64, 0, 512, 1, 1],"float16"), )
2025-03-03 17:02:22.215557 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([64, 26, 0, 1, 1],"float16"), )

[paddle error] paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([64, 26, 0, 1, 1],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 1, 512, 1, 40] and the shape of Y = [64, 26, 0, 1, 1]. Received [512] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.251362 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([64, 26, 512, 0, 1],"float16"), )

[Pass] paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([64, 26, 512, 0, 1],"float16"), )
2025-03-03 17:02:22.278650 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([64, 26, 512, 1, 0],"float16"), )

[paddle error] paddle.add(Tensor([64, 1, 512, 1, 40],"float16"), Tensor([64, 26, 512, 1, 0],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 1, 512, 1, 40] and the shape of Y = [64, 26, 512, 1, 0]. Received [40] in X is not equal to [0] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.303419 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([0, 26, 512, 1, 1],"float32"), )

[paddle error] paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([0, 26, 512, 1, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 1, 512, 1, 40] and the shape of Y = [0, 26, 512, 1, 1]. Received [64] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.322366 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([64, 0, 512, 1, 1],"float32"), )

[Pass] paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([64, 0, 512, 1, 1],"float32"), )
2025-03-03 17:02:22.344375 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([64, 26, 0, 1, 1],"float32"), )

[paddle error] paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([64, 26, 0, 1, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 1, 512, 1, 40] and the shape of Y = [64, 26, 0, 1, 1]. Received [512] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.364379 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([64, 26, 512, 0, 1],"float32"), )

[Pass] paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([64, 26, 512, 0, 1],"float32"), )
2025-03-03 17:02:22.387113 test begin: paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([64, 26, 512, 1, 0],"float32"), )

[paddle error] paddle.add(Tensor([64, 1, 512, 1, 40],"float32"), Tensor([64, 26, 512, 1, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [64, 1, 512, 1, 40] and the shape of Y = [64, 26, 512, 1, 0]. Received [40] in X is not equal to [0] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.405792 test begin: paddle.add(Tensor([7, 0, 3],"complex128"), Tensor([2, 3],"complex128"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([7, 0, 3],"complex128"), Tensor([2, 3],"complex128"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.409659 test begin: paddle.add(Tensor([7, 0, 3],"complex64"), Tensor([2, 3],"complex64"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([7, 0, 3],"complex64"), Tensor([2, 3],"complex64"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.413319 test begin: paddle.add(Tensor([7, 0, 6],"float32"), Tensor([5, 6],"float32"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([7, 0, 6],"float32"), Tensor([5, 6],"float32"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 0, 6] and the shape of Y = [5, 6]. Received [0] in X is not equal to [5] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.416984 test begin: paddle.add(Tensor([7, 2, 0],"complex128"), Tensor([2, 3],"complex128"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([7, 2, 0],"complex128"), Tensor([2, 3],"complex128"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.419391 test begin: paddle.add(Tensor([7, 2, 0],"complex64"), Tensor([2, 3],"complex64"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([7, 2, 0],"complex64"), Tensor([2, 3],"complex64"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.421846 test begin: paddle.add(Tensor([7, 2, 3],"complex128"), Tensor([0, 3],"complex128"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([7, 2, 3],"complex128"), Tensor([0, 3],"complex128"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 2, 3] and the shape of Y = [0, 3]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.424408 test begin: paddle.add(Tensor([7, 2, 3],"complex128"), Tensor([2, 0],"complex128"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([7, 2, 3],"complex128"), Tensor([2, 0],"complex128"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 2, 3] and the shape of Y = [2, 0]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.428544 test begin: paddle.add(Tensor([7, 2, 3],"complex64"), Tensor([0, 3],"complex64"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([7, 2, 3],"complex64"), Tensor([0, 3],"complex64"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 2, 3] and the shape of Y = [0, 3]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.431923 test begin: paddle.add(Tensor([7, 2, 3],"complex64"), Tensor([2, 0],"complex64"), name="Normal_sample", )

[paddle error] paddle.add(Tensor([7, 2, 3],"complex64"), Tensor([2, 0],"complex64"), name="Normal_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 2, 3] and the shape of Y = [2, 0]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.434301 test begin: paddle.add(Tensor([7, 5, 0],"float32"), Tensor([5, 6],"float32"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([7, 5, 0],"float32"), Tensor([5, 6],"float32"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 5, 0] and the shape of Y = [5, 6]. Received [0] in X is not equal to [6] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.436683 test begin: paddle.add(Tensor([7, 5, 6],"float32"), Tensor([0, 6],"float32"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([7, 5, 6],"float32"), Tensor([0, 6],"float32"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 5, 6] and the shape of Y = [0, 6]. Received [5] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.439269 test begin: paddle.add(Tensor([7, 5, 6],"float32"), Tensor([5, 0],"float32"), name="Uniform_sample", )

[paddle error] paddle.add(Tensor([7, 5, 6],"float32"), Tensor([5, 0],"float32"), name="Uniform_sample", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [7, 5, 6] and the shape of Y = [5, 0]. Received [6] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.441742 test begin: paddle.add(x=Tensor([0, 1024, 14, 14],"float32"), y=Tensor([0, 1024, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([0, 1024, 14, 14],"float32"), y=Tensor([0, 1024, 14, 14],"float32"), )
2025-03-03 17:02:22.444042 test begin: paddle.add(x=Tensor([0, 1024, 14, 14],"float32"), y=Tensor([1, 1024, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([0, 1024, 14, 14],"float32"), y=Tensor([1, 1024, 14, 14],"float32"), )
2025-03-03 17:02:22.452008 test begin: paddle.add(x=Tensor([0, 1024, 36, 50],"float32"), y=Tensor([0, 1024, 36, 50],"float32"), )

[Pass] paddle.add(x=Tensor([0, 1024, 36, 50],"float32"), y=Tensor([0, 1024, 36, 50],"float32"), )
2025-03-03 17:02:22.455334 test begin: paddle.add(x=Tensor([0, 1024, 36, 50],"float32"), y=Tensor([1, 1024, 36, 50],"float32"), )

[Pass] paddle.add(x=Tensor([0, 1024, 36, 50],"float32"), y=Tensor([1, 1024, 36, 50],"float32"), )
2025-03-03 17:02:22.486380 test begin: paddle.add(x=Tensor([0, 1024, 4, 14, 14],"float32"), y=Tensor([0, 1024, 4, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([0, 1024, 4, 14, 14],"float32"), y=Tensor([0, 1024, 4, 14, 14],"float32"), )
2025-03-03 17:02:22.490796 test begin: paddle.add(x=Tensor([0, 1024, 4, 14, 14],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([0, 1024, 4, 14, 14],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1024, 4, 14, 14] and the shape of Y = [8, 1024, 4, 14, 14]. Received [0] in X is not equal to [8] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.618758 test begin: paddle.add(x=Tensor([0, 128, 32, 14, 14],"float32"), y=Tensor([0, 128, 32, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([0, 128, 32, 14, 14],"float32"), y=Tensor([0, 128, 32, 14, 14],"float32"), )
2025-03-03 17:02:22.622723 test begin: paddle.add(x=Tensor([0, 128, 32, 14, 14],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([0, 128, 32, 14, 14],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 128, 32, 14, 14] and the shape of Y = [8, 128, 32, 14, 14]. Received [0] in X is not equal to [8] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.754620 test begin: paddle.add(x=Tensor([0, 24],"float32"), y=Tensor([0, 24],"float32"), )

[Pass] paddle.add(x=Tensor([0, 24],"float32"), y=Tensor([0, 24],"float32"), )
2025-03-03 17:02:22.764576 test begin: paddle.add(x=Tensor([0, 24],"float32"), y=Tensor([2, 24],"float32"), )

[paddle error] paddle.add(x=Tensor([0, 24],"float32"), y=Tensor([2, 24],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 24] and the shape of Y = [2, 24]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.768548 test begin: paddle.add(x=Tensor([0, 3, 3],"float32"), y=Tensor([0, 3, 3],"float32"), )

[Pass] paddle.add(x=Tensor([0, 3, 3],"float32"), y=Tensor([0, 3, 3],"float32"), )
2025-03-03 17:02:22.771954 test begin: paddle.add(x=Tensor([0, 3, 3],"float32"), y=Tensor([3, 3, 3],"float32"), )

[paddle error] paddle.add(x=Tensor([0, 3, 3],"float32"), y=Tensor([3, 3, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 3] and the shape of Y = [3, 3, 3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.783221 test begin: paddle.add(x=Tensor([0, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), )

[Pass] paddle.add(x=Tensor([0, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), )
2025-03-03 17:02:22.785232 test begin: paddle.add(x=Tensor([0, 3, 3],"float64"), y=Tensor([1, 3, 3],"float64"), )

[Pass] paddle.add(x=Tensor([0, 3, 3],"float64"), y=Tensor([1, 3, 3],"float64"), )
2025-03-03 17:02:22.787212 test begin: paddle.add(x=Tensor([1, 0, 14, 14],"float32"), y=Tensor([1, 0, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([1, 0, 14, 14],"float32"), y=Tensor([1, 0, 14, 14],"float32"), )
2025-03-03 17:02:22.788884 test begin: paddle.add(x=Tensor([1, 0, 14, 14],"float32"), y=Tensor([1, 1024, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 0, 14, 14],"float32"), y=Tensor([1, 1024, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0, 14, 14] and the shape of Y = [1, 1024, 14, 14]. Received [0] in X is not equal to [1024] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.792538 test begin: paddle.add(x=Tensor([1, 0, 36, 50],"float32"), y=Tensor([1, 0, 36, 50],"float32"), )

[Pass] paddle.add(x=Tensor([1, 0, 36, 50],"float32"), y=Tensor([1, 0, 36, 50],"float32"), )
2025-03-03 17:02:22.794035 test begin: paddle.add(x=Tensor([1, 0, 36, 50],"float32"), y=Tensor([1, 1024, 36, 50],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 0, 36, 50],"float32"), y=Tensor([1, 1024, 36, 50],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0, 36, 50] and the shape of Y = [1, 1024, 36, 50]. Received [0] in X is not equal to [1024] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.816846 test begin: paddle.add(x=Tensor([1, 1024, 0, 14],"float32"), y=Tensor([1, 1024, 0, 14],"float32"), )

[Pass] paddle.add(x=Tensor([1, 1024, 0, 14],"float32"), y=Tensor([1, 1024, 0, 14],"float32"), )
2025-03-03 17:02:22.819454 test begin: paddle.add(x=Tensor([1, 1024, 0, 14],"float32"), y=Tensor([1, 1024, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 0, 14],"float32"), y=Tensor([1, 1024, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 0, 14] and the shape of Y = [1, 1024, 14, 14]. Received [0] in X is not equal to [14] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.823237 test begin: paddle.add(x=Tensor([1, 1024, 0, 50],"float32"), y=Tensor([1, 1024, 0, 50],"float32"), )

[Pass] paddle.add(x=Tensor([1, 1024, 0, 50],"float32"), y=Tensor([1, 1024, 0, 50],"float32"), )
2025-03-03 17:02:22.825153 test begin: paddle.add(x=Tensor([1, 1024, 0, 50],"float32"), y=Tensor([1, 1024, 36, 50],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 0, 50],"float32"), y=Tensor([1, 1024, 36, 50],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 0, 50] and the shape of Y = [1, 1024, 36, 50]. Received [0] in X is not equal to [36] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.848360 test begin: paddle.add(x=Tensor([1, 1024, 14, 0],"float32"), y=Tensor([1, 1024, 14, 0],"float32"), )

[Pass] paddle.add(x=Tensor([1, 1024, 14, 0],"float32"), y=Tensor([1, 1024, 14, 0],"float32"), )
2025-03-03 17:02:22.850877 test begin: paddle.add(x=Tensor([1, 1024, 14, 0],"float32"), y=Tensor([1, 1024, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 14, 0],"float32"), y=Tensor([1, 1024, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 14, 0] and the shape of Y = [1, 1024, 14, 14]. Received [0] in X is not equal to [14] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.855170 test begin: paddle.add(x=Tensor([1, 1024, 14, 14],"float32"), y=Tensor([0, 1024, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([1, 1024, 14, 14],"float32"), y=Tensor([0, 1024, 14, 14],"float32"), )
2025-03-03 17:02:22.859347 test begin: paddle.add(x=Tensor([1, 1024, 14, 14],"float32"), y=Tensor([1, 0, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 14, 14],"float32"), y=Tensor([1, 0, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 14, 14] and the shape of Y = [1, 0, 14, 14]. Received [1024] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.863180 test begin: paddle.add(x=Tensor([1, 1024, 14, 14],"float32"), y=Tensor([1, 1024, 0, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 14, 14],"float32"), y=Tensor([1, 1024, 0, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 14, 14] and the shape of Y = [1, 1024, 0, 14]. Received [14] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.866823 test begin: paddle.add(x=Tensor([1, 1024, 14, 14],"float32"), y=Tensor([1, 1024, 14, 0],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 14, 14],"float32"), y=Tensor([1, 1024, 14, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 14, 14] and the shape of Y = [1, 1024, 14, 0]. Received [14] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.870157 test begin: paddle.add(x=Tensor([1, 1024, 36, 0],"float32"), y=Tensor([1, 1024, 36, 0],"float32"), )

[Pass] paddle.add(x=Tensor([1, 1024, 36, 0],"float32"), y=Tensor([1, 1024, 36, 0],"float32"), )
2025-03-03 17:02:22.872061 test begin: paddle.add(x=Tensor([1, 1024, 36, 0],"float32"), y=Tensor([1, 1024, 36, 50],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 36, 0],"float32"), y=Tensor([1, 1024, 36, 50],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 36, 0] and the shape of Y = [1, 1024, 36, 50]. Received [0] in X is not equal to [50] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.894150 test begin: paddle.add(x=Tensor([1, 1024, 36, 50],"float32"), y=Tensor([0, 1024, 36, 50],"float32"), )

[Pass] paddle.add(x=Tensor([1, 1024, 36, 50],"float32"), y=Tensor([0, 1024, 36, 50],"float32"), )
2025-03-03 17:02:22.921166 test begin: paddle.add(x=Tensor([1, 1024, 36, 50],"float32"), y=Tensor([1, 0, 36, 50],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 36, 50],"float32"), y=Tensor([1, 0, 36, 50],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 36, 50] and the shape of Y = [1, 0, 36, 50]. Received [1024] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.953841 test begin: paddle.add(x=Tensor([1, 1024, 36, 50],"float32"), y=Tensor([1, 1024, 0, 50],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 36, 50],"float32"), y=Tensor([1, 1024, 0, 50],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 36, 50] and the shape of Y = [1, 1024, 0, 50]. Received [36] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:22.986349 test begin: paddle.add(x=Tensor([1, 1024, 36, 50],"float32"), y=Tensor([1, 1024, 36, 0],"float32"), )

[paddle error] paddle.add(x=Tensor([1, 1024, 36, 50],"float32"), y=Tensor([1, 1024, 36, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 1024, 36, 50] and the shape of Y = [1, 1024, 36, 0]. Received [50] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.017603 test begin: paddle.add(x=Tensor([2, 0],"float32"), y=Tensor([2, 0],"float32"), )

[Pass] paddle.add(x=Tensor([2, 0],"float32"), y=Tensor([2, 0],"float32"), )
2025-03-03 17:02:23.020163 test begin: paddle.add(x=Tensor([2, 0],"float32"), y=Tensor([2, 24],"float32"), )

[paddle error] paddle.add(x=Tensor([2, 0],"float32"), y=Tensor([2, 24],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 24]. Received [0] in X is not equal to [24] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.021885 test begin: paddle.add(x=Tensor([2, 24],"float32"), y=Tensor([0, 24],"float32"), )

[paddle error] paddle.add(x=Tensor([2, 24],"float32"), y=Tensor([0, 24],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 24] and the shape of Y = [0, 24]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.023684 test begin: paddle.add(x=Tensor([2, 24],"float32"), y=Tensor([2, 0],"float32"), )

[paddle error] paddle.add(x=Tensor([2, 24],"float32"), y=Tensor([2, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 24] and the shape of Y = [2, 0]. Received [24] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.025522 test begin: paddle.add(x=Tensor([3, 0, 3],"float32"), y=Tensor([3, 0, 3],"float32"), )

[Pass] paddle.add(x=Tensor([3, 0, 3],"float32"), y=Tensor([3, 0, 3],"float32"), )
2025-03-03 17:02:23.027444 test begin: paddle.add(x=Tensor([3, 0, 3],"float32"), y=Tensor([3, 3, 3],"float32"), )

[paddle error] paddle.add(x=Tensor([3, 0, 3],"float32"), y=Tensor([3, 3, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 3] and the shape of Y = [3, 3, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.029398 test begin: paddle.add(x=Tensor([3, 0, 3],"float64"), y=Tensor([1, 0, 3],"float64"), )

[Pass] paddle.add(x=Tensor([3, 0, 3],"float64"), y=Tensor([1, 0, 3],"float64"), )
2025-03-03 17:02:23.031301 test begin: paddle.add(x=Tensor([3, 0, 3],"float64"), y=Tensor([1, 3, 3],"float64"), )

[paddle error] paddle.add(x=Tensor([3, 0, 3],"float64"), y=Tensor([1, 3, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 3] and the shape of Y = [1, 3, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.033176 test begin: paddle.add(x=Tensor([3, 3, 0],"float32"), y=Tensor([3, 3, 0],"float32"), )

[Pass] paddle.add(x=Tensor([3, 3, 0],"float32"), y=Tensor([3, 3, 0],"float32"), )
2025-03-03 17:02:23.034959 test begin: paddle.add(x=Tensor([3, 3, 0],"float32"), y=Tensor([3, 3, 3],"float32"), )

[paddle error] paddle.add(x=Tensor([3, 3, 0],"float32"), y=Tensor([3, 3, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 0] and the shape of Y = [3, 3, 3]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.036818 test begin: paddle.add(x=Tensor([3, 3, 0],"float64"), y=Tensor([1, 3, 0],"float64"), )

[Pass] paddle.add(x=Tensor([3, 3, 0],"float64"), y=Tensor([1, 3, 0],"float64"), )
2025-03-03 17:02:23.038668 test begin: paddle.add(x=Tensor([3, 3, 0],"float64"), y=Tensor([1, 3, 3],"float64"), )

[paddle error] paddle.add(x=Tensor([3, 3, 0],"float64"), y=Tensor([1, 3, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 0] and the shape of Y = [1, 3, 3]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.040402 test begin: paddle.add(x=Tensor([3, 3, 3],"float32"), y=Tensor([0, 3, 3],"float32"), )

[paddle error] paddle.add(x=Tensor([3, 3, 3],"float32"), y=Tensor([0, 3, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3] and the shape of Y = [0, 3, 3]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.042739 test begin: paddle.add(x=Tensor([3, 3, 3],"float32"), y=Tensor([3, 0, 3],"float32"), )

[paddle error] paddle.add(x=Tensor([3, 3, 3],"float32"), y=Tensor([3, 0, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3] and the shape of Y = [3, 0, 3]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.044751 test begin: paddle.add(x=Tensor([3, 3, 3],"float32"), y=Tensor([3, 3, 0],"float32"), )

[paddle error] paddle.add(x=Tensor([3, 3, 3],"float32"), y=Tensor([3, 3, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3] and the shape of Y = [3, 3, 0]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.046730 test begin: paddle.add(x=Tensor([3, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), )

[paddle error] paddle.add(x=Tensor([3, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3] and the shape of Y = [0, 3, 3]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.049297 test begin: paddle.add(x=Tensor([3, 3, 3],"float64"), y=Tensor([1, 0, 3],"float64"), )

[paddle error] paddle.add(x=Tensor([3, 3, 3],"float64"), y=Tensor([1, 0, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3] and the shape of Y = [1, 0, 3]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.051020 test begin: paddle.add(x=Tensor([3, 3, 3],"float64"), y=Tensor([1, 3, 0],"float64"), )

[paddle error] paddle.add(x=Tensor([3, 3, 3],"float64"), y=Tensor([1, 3, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3] and the shape of Y = [1, 3, 0]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.052722 test begin: paddle.add(x=Tensor([8, 0, 32, 14, 14],"float32"), y=Tensor([8, 0, 32, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([8, 0, 32, 14, 14],"float32"), y=Tensor([8, 0, 32, 14, 14],"float32"), )
2025-03-03 17:02:23.054484 test begin: paddle.add(x=Tensor([8, 0, 32, 14, 14],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 0, 32, 14, 14],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 0, 32, 14, 14] and the shape of Y = [8, 128, 32, 14, 14]. Received [0] in X is not equal to [128] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.144234 test begin: paddle.add(x=Tensor([8, 0, 4, 14, 14],"float32"), y=Tensor([8, 0, 4, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([8, 0, 4, 14, 14],"float32"), y=Tensor([8, 0, 4, 14, 14],"float32"), )
2025-03-03 17:02:23.146973 test begin: paddle.add(x=Tensor([8, 0, 4, 14, 14],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 0, 4, 14, 14],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 0, 4, 14, 14] and the shape of Y = [8, 1024, 4, 14, 14]. Received [0] in X is not equal to [1024] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.266375 test begin: paddle.add(x=Tensor([8, 1024, 0, 14, 14],"float32"), y=Tensor([8, 1024, 0, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([8, 1024, 0, 14, 14],"float32"), y=Tensor([8, 1024, 0, 14, 14],"float32"), )
2025-03-03 17:02:23.269489 test begin: paddle.add(x=Tensor([8, 1024, 0, 14, 14],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 1024, 0, 14, 14],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 1024, 0, 14, 14] and the shape of Y = [8, 1024, 4, 14, 14]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.360335 test begin: paddle.add(x=Tensor([8, 1024, 4, 0, 14],"float32"), y=Tensor([8, 1024, 4, 0, 14],"float32"), )

[Pass] paddle.add(x=Tensor([8, 1024, 4, 0, 14],"float32"), y=Tensor([8, 1024, 4, 0, 14],"float32"), )
2025-03-03 17:02:23.363699 test begin: paddle.add(x=Tensor([8, 1024, 4, 0, 14],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 1024, 4, 0, 14],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 1024, 4, 0, 14] and the shape of Y = [8, 1024, 4, 14, 14]. Received [0] in X is not equal to [14] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.477281 test begin: paddle.add(x=Tensor([8, 1024, 4, 14, 0],"float32"), y=Tensor([8, 1024, 4, 14, 0],"float32"), )

[Pass] paddle.add(x=Tensor([8, 1024, 4, 14, 0],"float32"), y=Tensor([8, 1024, 4, 14, 0],"float32"), )
2025-03-03 17:02:23.479757 test begin: paddle.add(x=Tensor([8, 1024, 4, 14, 0],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 1024, 4, 14, 0],"float32"), y=Tensor([8, 1024, 4, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 1024, 4, 14, 0] and the shape of Y = [8, 1024, 4, 14, 14]. Received [0] in X is not equal to [14] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.578380 test begin: paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([0, 1024, 4, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([0, 1024, 4, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 1024, 4, 14, 14] and the shape of Y = [0, 1024, 4, 14, 14]. Received [8] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.675670 test begin: paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([8, 0, 4, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([8, 0, 4, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 1024, 4, 14, 14] and the shape of Y = [8, 0, 4, 14, 14]. Received [1024] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.801744 test begin: paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([8, 1024, 0, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([8, 1024, 0, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 1024, 4, 14, 14] and the shape of Y = [8, 1024, 0, 14, 14]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:23.929950 test begin: paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([8, 1024, 4, 0, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([8, 1024, 4, 0, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 1024, 4, 14, 14] and the shape of Y = [8, 1024, 4, 0, 14]. Received [14] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.089515 test begin: paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([8, 1024, 4, 14, 0],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 1024, 4, 14, 14],"float32"), y=Tensor([8, 1024, 4, 14, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 1024, 4, 14, 14] and the shape of Y = [8, 1024, 4, 14, 0]. Received [14] in X is not equal to [0] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.209738 test begin: paddle.add(x=Tensor([8, 128, 0, 14, 14],"float32"), y=Tensor([8, 128, 0, 14, 14],"float32"), )

[Pass] paddle.add(x=Tensor([8, 128, 0, 14, 14],"float32"), y=Tensor([8, 128, 0, 14, 14],"float32"), )
2025-03-03 17:02:24.213791 test begin: paddle.add(x=Tensor([8, 128, 0, 14, 14],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 128, 0, 14, 14],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 128, 0, 14, 14] and the shape of Y = [8, 128, 32, 14, 14]. Received [0] in X is not equal to [32] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.333011 test begin: paddle.add(x=Tensor([8, 128, 32, 0, 14],"float32"), y=Tensor([8, 128, 32, 0, 14],"float32"), )

[Pass] paddle.add(x=Tensor([8, 128, 32, 0, 14],"float32"), y=Tensor([8, 128, 32, 0, 14],"float32"), )
2025-03-03 17:02:24.335272 test begin: paddle.add(x=Tensor([8, 128, 32, 0, 14],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 128, 32, 0, 14],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 128, 32, 0, 14] and the shape of Y = [8, 128, 32, 14, 14]. Received [0] in X is not equal to [14] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.435882 test begin: paddle.add(x=Tensor([8, 128, 32, 14, 0],"float32"), y=Tensor([8, 128, 32, 14, 0],"float32"), )

[Pass] paddle.add(x=Tensor([8, 128, 32, 14, 0],"float32"), y=Tensor([8, 128, 32, 14, 0],"float32"), )
2025-03-03 17:02:24.438297 test begin: paddle.add(x=Tensor([8, 128, 32, 14, 0],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 128, 32, 14, 0],"float32"), y=Tensor([8, 128, 32, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 128, 32, 14, 0] and the shape of Y = [8, 128, 32, 14, 14]. Received [0] in X is not equal to [14] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.551004 test begin: paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([0, 128, 32, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([0, 128, 32, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 128, 32, 14, 14] and the shape of Y = [0, 128, 32, 14, 14]. Received [8] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.646560 test begin: paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([8, 0, 32, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([8, 0, 32, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 128, 32, 14, 14] and the shape of Y = [8, 0, 32, 14, 14]. Received [128] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.735592 test begin: paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([8, 128, 0, 14, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([8, 128, 0, 14, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 128, 32, 14, 14] and the shape of Y = [8, 128, 0, 14, 14]. Received [32] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.824722 test begin: paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([8, 128, 32, 0, 14],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([8, 128, 32, 0, 14],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 128, 32, 14, 14] and the shape of Y = [8, 128, 32, 0, 14]. Received [14] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:24.914385 test begin: paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([8, 128, 32, 14, 0],"float32"), )

[paddle error] paddle.add(x=Tensor([8, 128, 32, 14, 14],"float32"), y=Tensor([8, 128, 32, 14, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [8, 128, 32, 14, 14] and the shape of Y = [8, 128, 32, 14, 0]. Received [14] in X is not equal to [0] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:25.062498 test begin: paddle.add_n(Tensor([0, 13, 11],"float32"), )

[Pass] paddle.add_n(Tensor([0, 13, 11],"float32"), )
2025-03-03 17:02:25.065775 test begin: paddle.add_n(Tensor([2, 0, 11],"float32"), )

[Pass] paddle.add_n(Tensor([2, 0, 11],"float32"), )
2025-03-03 17:02:25.067267 test begin: paddle.add_n(Tensor([2, 13, 0],"float32"), )

[Pass] paddle.add_n(Tensor([2, 13, 0],"float32"), )
2025-03-03 17:02:25.068488 test begin: paddle.add_n(inputs=Tensor([0, 4, 4, 4],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([0, 4, 4, 4],"float64"), )
2025-03-03 17:02:25.070260 test begin: paddle.add_n(inputs=Tensor([0, 4, 4],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([0, 4, 4],"float64"), )
2025-03-03 17:02:25.071568 test begin: paddle.add_n(inputs=Tensor([0, 4],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([0, 4],"float64"), )
2025-03-03 17:02:25.072872 test begin: paddle.add_n(inputs=Tensor([0],"float32"), )

[Pass] paddle.add_n(inputs=Tensor([0],"float32"), )
2025-03-03 17:02:25.074737 test begin: paddle.add_n(inputs=Tensor([0],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([0],"float64"), )
2025-03-03 17:02:25.075978 test begin: paddle.add_n(inputs=Tensor([4, 0, 4, 4],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([4, 0, 4, 4],"float64"), )
2025-03-03 17:02:25.077125 test begin: paddle.add_n(inputs=Tensor([4, 0, 4],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([4, 0, 4],"float64"), )
2025-03-03 17:02:25.078278 test begin: paddle.add_n(inputs=Tensor([4, 0],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([4, 0],"float64"), )
2025-03-03 17:02:25.079377 test begin: paddle.add_n(inputs=Tensor([4, 4, 0, 4],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([4, 4, 0, 4],"float64"), )
2025-03-03 17:02:25.080980 test begin: paddle.add_n(inputs=Tensor([4, 4, 0],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([4, 4, 0],"float64"), )
2025-03-03 17:02:25.082107 test begin: paddle.add_n(inputs=Tensor([4, 4, 4, 0],"float64"), )

[Pass] paddle.add_n(inputs=Tensor([4, 4, 4, 0],"float64"), )
2025-03-03 17:02:25.083294 test begin: paddle.add_n(list[Tensor([0, 1, 10, 5, 5],"float32"),Tensor([0, 1, 10, 5, 5],"float32"),Tensor([0, 1, 10, 5, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0, 1, 10, 5, 5],"float32"),Tensor([0, 1, 10, 5, 5],"float32"),Tensor([0, 1, 10, 5, 5],"float32"),], )
2025-03-03 17:02:25.085876 test begin: paddle.add_n(list[Tensor([0, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )
2025-03-03 17:02:25.088356 test begin: paddle.add_n(list[Tensor([0, 128, 32, 32],"float16"),Tensor([0, 128, 32, 32],"float16"),], )

[Pass] paddle.add_n(list[Tensor([0, 128, 32, 32],"float16"),Tensor([0, 128, 32, 32],"float16"),], )
2025-03-03 17:02:25.089831 test begin: paddle.add_n(list[Tensor([0, 128, 32, 32],"float16"),Tensor([64, 128, 32, 32],"float16"),], )

[Pass] paddle.add_n(list[Tensor([0, 128, 32, 32],"float16"),Tensor([64, 128, 32, 32],"float16"),], )
2025-03-03 17:02:25.241824 test begin: paddle.add_n(list[Tensor([0, 128, 32, 32],"float32"),Tensor([0, 128, 32, 32],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0, 128, 32, 32],"float32"),Tensor([0, 128, 32, 32],"float32"),], )
2025-03-03 17:02:25.244223 test begin: paddle.add_n(list[Tensor([0, 128, 32, 32],"float32"),Tensor([64, 128, 32, 32],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0, 128, 32, 32],"float32"),Tensor([64, 128, 32, 32],"float32"),], )
2025-03-03 17:02:25.376281 test begin: paddle.add_n(list[Tensor([0, 200],"int32"),Tensor([0, 200],"int32"),], )

[Pass] paddle.add_n(list[Tensor([0, 200],"int32"),Tensor([0, 200],"int32"),], )
2025-03-03 17:02:25.378979 test begin: paddle.add_n(list[Tensor([0, 200],"int32"),Tensor([100, 200],"int32"),], )

[Pass] paddle.add_n(list[Tensor([0, 200],"int32"),Tensor([100, 200],"int32"),], )
2025-03-03 17:02:25.380915 test begin: paddle.add_n(list[Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),], )

[Pass] paddle.add_n(list[Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([0, 256],"complex128"),], )
2025-03-03 17:02:25.384907 test begin: paddle.add_n(list[Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[Pass] paddle.add_n(list[Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )
2025-03-03 17:02:25.394023 test begin: paddle.add_n(list[Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),], )

[Pass] paddle.add_n(list[Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([0, 256],"complex64"),], )
2025-03-03 17:02:25.398602 test begin: paddle.add_n(list[Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[Pass] paddle.add_n(list[Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )
2025-03-03 17:02:25.407596 test begin: paddle.add_n(list[Tensor([0, 40],"float64"),Tensor([0, 40],"float64"),Tensor([0, 40],"float64"),], )

[Pass] paddle.add_n(list[Tensor([0, 40],"float64"),Tensor([0, 40],"float64"),Tensor([0, 40],"float64"),], )
2025-03-03 17:02:25.409950 test begin: paddle.add_n(list[Tensor([0, 40],"float64"),Tensor([3, 40],"float64"),Tensor([3, 40],"float64"),], )

[Pass] paddle.add_n(list[Tensor([0, 40],"float64"),Tensor([3, 40],"float64"),Tensor([3, 40],"float64"),], )
2025-03-03 17:02:25.412530 test begin: paddle.add_n(list[Tensor([0, 4],"float64"),Tensor([0, 4],"float64"),Tensor([0, 4],"float64"),], )

[Pass] paddle.add_n(list[Tensor([0, 4],"float64"),Tensor([0, 4],"float64"),Tensor([0, 4],"float64"),], )
2025-03-03 17:02:25.414312 test begin: paddle.add_n(list[Tensor([0, 4],"float64"),Tensor([0, 4],"float64"),], )

[Pass] paddle.add_n(list[Tensor([0, 4],"float64"),Tensor([0, 4],"float64"),], )
2025-03-03 17:02:25.415983 test begin: paddle.add_n(list[Tensor([0, 4],"float64"),Tensor([4, 4],"float64"),Tensor([4, 4],"float64"),], )

[Pass] paddle.add_n(list[Tensor([0, 4],"float64"),Tensor([4, 4],"float64"),Tensor([4, 4],"float64"),], )
2025-03-03 17:02:25.418019 test begin: paddle.add_n(list[Tensor([0, 4],"float64"),Tensor([4, 4],"float64"),], )

[Pass] paddle.add_n(list[Tensor([0, 4],"float64"),Tensor([4, 4],"float64"),], )
2025-03-03 17:02:25.419567 test begin: paddle.add_n(list[Tensor([0],"complex128"),Tensor([0],"complex128"),], )

[Pass] paddle.add_n(list[Tensor([0],"complex128"),Tensor([0],"complex128"),], )
2025-03-03 17:02:25.420890 test begin: paddle.add_n(list[Tensor([0],"complex128"),Tensor([3],"complex128"),], )

[Pass] paddle.add_n(list[Tensor([0],"complex128"),Tensor([3],"complex128"),], )
2025-03-03 17:02:25.422503 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:25.424997 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:25.426922 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:25.428465 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:25.429953 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:25.431368 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:25.432685 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:25.433885 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:25.439570 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:25.442475 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:25.444856 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:25.447250 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:25.449447 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:25.451092 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:25.452738 test begin: paddle.add_n(list[Tensor([0],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),], )

[Pass] paddle.add_n(list[Tensor([0],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),], )
2025-03-03 17:02:25.454662 test begin: paddle.add_n(list[Tensor([10, 0, 10, 5, 5],"float32"),Tensor([10, 0, 10, 5, 5],"float32"),Tensor([10, 0, 10, 5, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([10, 0, 10, 5, 5],"float32"),Tensor([10, 0, 10, 5, 5],"float32"),Tensor([10, 0, 10, 5, 5],"float32"),], )
2025-03-03 17:02:25.456603 test begin: paddle.add_n(list[Tensor([10, 0, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([10, 0, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )
2025-03-03 17:02:25.460822 test begin: paddle.add_n(list[Tensor([10, 1, 0, 5, 5],"float32"),Tensor([10, 1, 0, 5, 5],"float32"),Tensor([10, 1, 0, 5, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([10, 1, 0, 5, 5],"float32"),Tensor([10, 1, 0, 5, 5],"float32"),Tensor([10, 1, 0, 5, 5],"float32"),], )
2025-03-03 17:02:25.464904 test begin: paddle.add_n(list[Tensor([10, 1, 0, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([10, 1, 0, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )
2025-03-03 17:02:25.466816 test begin: paddle.add_n(list[Tensor([10, 1, 10, 0, 5],"float32"),Tensor([10, 1, 10, 0, 5],"float32"),Tensor([10, 1, 10, 0, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([10, 1, 10, 0, 5],"float32"),Tensor([10, 1, 10, 0, 5],"float32"),Tensor([10, 1, 10, 0, 5],"float32"),], )
2025-03-03 17:02:25.470343 test begin: paddle.add_n(list[Tensor([10, 1, 10, 0, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([10, 1, 10, 0, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )
2025-03-03 17:02:25.475290 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 0],"float32"),Tensor([10, 1, 10, 5, 0],"float32"),Tensor([10, 1, 10, 5, 0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([10, 1, 10, 5, 0],"float32"),Tensor([10, 1, 10, 5, 0],"float32"),Tensor([10, 1, 10, 5, 0],"float32"),], )
2025-03-03 17:02:25.477437 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 0],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[Pass] paddle.add_n(list[Tensor([10, 1, 10, 5, 0],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )
2025-03-03 17:02:25.484069 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([0, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([0, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[1]'s shape = [0, 1, 10, 5, 5].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:0, 1, 10, 5, 5.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.486991 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 0, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 0, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[1]'s shape = [10, 0, 10, 5, 5].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:10, 0, 10, 5, 5.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.497862 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 0, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 0, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[1]'s shape = [10, 1, 0, 5, 5].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:10, 1, 0, 5, 5.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.500030 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 0, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 0, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[1]'s shape = [10, 1, 10, 0, 5].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:10, 1, 10, 0, 5.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.502325 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 0],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 0],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[1]'s shape = [10, 1, 10, 5, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:10, 1, 10, 5, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.526021 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([0, 1, 10, 5, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([0, 1, 10, 5, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[2]'s shape = [0, 1, 10, 5, 5].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:0, 1, 10, 5, 5.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.528677 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 0, 10, 5, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 0, 10, 5, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[2]'s shape = [10, 0, 10, 5, 5].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:10, 0, 10, 5, 5.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.532458 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 0, 5, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 0, 5, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[2]'s shape = [10, 1, 0, 5, 5].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:10, 1, 0, 5, 5.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.546219 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 0, 5],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 0, 5],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[2]'s shape = [10, 1, 10, 0, 5].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:10, 1, 10, 0, 5.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.548240 test begin: paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 5],"float32"),Tensor([10, 1, 10, 5, 0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [10, 1, 10, 5, 5], X[2]'s shape = [10, 1, 10, 5, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:10, 1, 10, 5, 5 != x_dim:10, 1, 10, 5, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.551528 test begin: paddle.add_n(list[Tensor([100, 0],"int32"),Tensor([100, 0],"int32"),], )

[Pass] paddle.add_n(list[Tensor([100, 0],"int32"),Tensor([100, 0],"int32"),], )
2025-03-03 17:02:25.565650 test begin: paddle.add_n(list[Tensor([100, 0],"int32"),Tensor([100, 200],"int32"),], )

[Pass] paddle.add_n(list[Tensor([100, 0],"int32"),Tensor([100, 200],"int32"),], )
2025-03-03 17:02:25.583712 test begin: paddle.add_n(list[Tensor([100, 200],"int32"),Tensor([0, 200],"int32"),], )

[paddle error] paddle.add_n(list[Tensor([100, 200],"int32"),Tensor([0, 200],"int32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [100, 200], X[1]'s shape = [0, 200].
  [Hint: Expected in_dim == x_dim, but received in_dim:100, 200 != x_dim:0, 200.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.588683 test begin: paddle.add_n(list[Tensor([100, 200],"int32"),Tensor([100, 0],"int32"),], )

[paddle error] paddle.add_n(list[Tensor([100, 200],"int32"),Tensor([100, 0],"int32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [100, 200], X[1]'s shape = [100, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:100, 200 != x_dim:100, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.591699 test begin: paddle.add_n(list[Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),], )

[Pass] paddle.add_n(list[Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 0],"complex128"),], )
2025-03-03 17:02:25.596606 test begin: paddle.add_n(list[Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[Pass] paddle.add_n(list[Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )
2025-03-03 17:02:25.610975 test begin: paddle.add_n(list[Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),], )

[Pass] paddle.add_n(list[Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 0],"complex64"),], )
2025-03-03 17:02:25.615008 test begin: paddle.add_n(list[Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[Pass] paddle.add_n(list[Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )
2025-03-03 17:02:25.621624 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[1]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.628706 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[1]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.636466 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[2]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.643442 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[2]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.650218 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[3]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.657209 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[3]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.670480 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[4]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.680991 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[4]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.689358 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[5]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.697445 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[5]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.705442 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[6]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.712731 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[6]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.724888 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[7]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.732368 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[7]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.740141 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[8]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.750611 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[8]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.757864 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[9]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.764690 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[9]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.771575 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[10]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.778314 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[10]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.785306 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[11]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.792169 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[11]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.799014 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[12]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.806038 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[12]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.812814 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[13]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.819512 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[13]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.826283 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[14]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.833371 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[14]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.840950 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[15]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.848040 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[15]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.856209 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[16]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.864428 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[16]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.871465 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[17]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.879670 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[17]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.887086 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[18]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.895276 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[18]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.902517 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[19]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.911092 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[19]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.918515 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[20]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.925558 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[20]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.933434 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[21]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.941795 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[21]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.949073 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[22]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.956321 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[22]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.964627 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[23]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:25.972595 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[23]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.015312 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[24]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.024242 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[24]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.032564 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[25]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.042713 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[25]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.050374 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[26]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.059593 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[26]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.068292 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[27]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.076857 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[27]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.084035 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[28]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.091347 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[28]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.098952 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[29]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.106545 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[29]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.114010 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[30]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.121605 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),Tensor([16, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[30]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.129361 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([0, 256],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[31]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.136716 test begin: paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 256],"complex128"),Tensor([16, 0],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[31]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.144274 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[1]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.152830 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[1]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.161058 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[2]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.170346 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[2]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.181474 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[3]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.189062 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[3]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.198240 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[4]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.207214 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[4]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.215565 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[5]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.223086 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[5]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.234580 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[6]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.245402 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[6]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.253517 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[7]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.260810 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[7]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.268537 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[8]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.275759 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[8]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.283291 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[9]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.290831 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[9]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.298803 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[10]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.307276 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[10]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.314709 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[11]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.323492 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[11]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.330915 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[12]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.343110 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[12]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.354435 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[13]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.362734 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[13]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.371218 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[14]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.380307 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[14]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.388196 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[15]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.396298 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[15]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.403637 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[16]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.411353 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[16]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.419287 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[17]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.426186 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[17]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.433611 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[18]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.441174 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[18]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.448752 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[19]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.456382 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[19]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.464143 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[20]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.471262 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[20]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.479409 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[21]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.488072 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[21]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.496792 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[22]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.503796 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[22]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.511322 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[23]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.527681 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[23]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.535000 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[24]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.540833 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[24]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.546624 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[25]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.553256 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[25]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.559025 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[26]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.565569 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[26]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.571506 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[27]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.578045 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[27]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.583890 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[28]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.589661 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[28]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.597766 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[29]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.610598 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[29]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.617650 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[30]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.642274 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),Tensor([16, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[30]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.660946 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([0, 256],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[31]'s shape = [0, 256].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:0, 256.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.668952 test begin: paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),], )

[paddle error] paddle.add_n(list[Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 256],"complex64"),Tensor([16, 0],"complex64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [16, 256], X[31]'s shape = [16, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:16, 256 != x_dim:16, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.679317 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.684401 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.688506 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.690609 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.692400 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.694694 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.697364 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.699057 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[2]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.703433 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[2]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.705932 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[2]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.708077 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[2]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.711179 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[2]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.712677 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[2]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.714013 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[3]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.717552 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[3]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.720033 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[3]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.722667 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[3]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.724806 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[3]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.726641 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[4]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.730464 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[4]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.732565 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[4]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.734194 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[4]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.735806 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[4]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.738114 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[5]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.741909 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[5]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.744493 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[5]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.746285 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[5]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.747849 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[6]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.751003 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[6]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.755996 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[6]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.758057 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[7]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.762644 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[7]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.764739 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[8]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.768301 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[8]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.770454 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[9]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.773755 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[9]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.776381 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[10]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.779793 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[10]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.781768 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[11]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.786197 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[11]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.789187 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[12]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.791997 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[13]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.795477 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[14]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.798518 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[15]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.801358 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[16]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.804451 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[17]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.807187 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[18]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.810254 test begin: paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [1], X[19]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:1 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.812991 test begin: paddle.add_n(list[Tensor([2],"float32"),Tensor([0],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([2],"float32"),Tensor([0],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [2], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:2 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.814679 test begin: paddle.add_n(list[Tensor([2],"float32"),Tensor([2],"float32"),Tensor([0],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([2],"float32"),Tensor([2],"float32"),Tensor([0],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [2], X[2]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:2 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.816477 test begin: paddle.add_n(list[Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([0],"float32"),Tensor([2],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([0],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [2], X[3]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:2 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.818318 test begin: paddle.add_n(list[Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([2],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [2], X[4]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:2 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.819975 test begin: paddle.add_n(list[Tensor([3, 0],"float64"),Tensor([3, 0],"float64"),Tensor([3, 0],"float64"),], )

[Pass] paddle.add_n(list[Tensor([3, 0],"float64"),Tensor([3, 0],"float64"),Tensor([3, 0],"float64"),], )
2025-03-03 17:02:26.821091 test begin: paddle.add_n(list[Tensor([3, 0],"float64"),Tensor([3, 40],"float64"),Tensor([3, 40],"float64"),], )

[Pass] paddle.add_n(list[Tensor([3, 0],"float64"),Tensor([3, 40],"float64"),Tensor([3, 40],"float64"),], )
2025-03-03 17:02:26.822317 test begin: paddle.add_n(list[Tensor([3, 40],"float64"),Tensor([0, 40],"float64"),Tensor([3, 40],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([3, 40],"float64"),Tensor([0, 40],"float64"),Tensor([3, 40],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [3, 40], X[1]'s shape = [0, 40].
  [Hint: Expected in_dim == x_dim, but received in_dim:3, 40 != x_dim:0, 40.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.824176 test begin: paddle.add_n(list[Tensor([3, 40],"float64"),Tensor([3, 0],"float64"),Tensor([3, 40],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([3, 40],"float64"),Tensor([3, 0],"float64"),Tensor([3, 40],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [3, 40], X[1]'s shape = [3, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:3, 40 != x_dim:3, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.825662 test begin: paddle.add_n(list[Tensor([3, 40],"float64"),Tensor([3, 40],"float64"),Tensor([0, 40],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([3, 40],"float64"),Tensor([3, 40],"float64"),Tensor([0, 40],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [3, 40], X[2]'s shape = [0, 40].
  [Hint: Expected in_dim == x_dim, but received in_dim:3, 40 != x_dim:0, 40.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.826836 test begin: paddle.add_n(list[Tensor([3, 40],"float64"),Tensor([3, 40],"float64"),Tensor([3, 0],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([3, 40],"float64"),Tensor([3, 40],"float64"),Tensor([3, 0],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [3, 40], X[2]'s shape = [3, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:3, 40 != x_dim:3, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.827972 test begin: paddle.add_n(list[Tensor([3],"complex128"),Tensor([0],"complex128"),], )

[paddle error] paddle.add_n(list[Tensor([3],"complex128"),Tensor([0],"complex128"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [3], X[1]'s shape = [0].
  [Hint: Expected in_dim == x_dim, but received in_dim:3 != x_dim:0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.829600 test begin: paddle.add_n(list[Tensor([4, 0],"float64"),Tensor([4, 0],"float64"),Tensor([4, 0],"float64"),], )

[Pass] paddle.add_n(list[Tensor([4, 0],"float64"),Tensor([4, 0],"float64"),Tensor([4, 0],"float64"),], )
2025-03-03 17:02:26.830828 test begin: paddle.add_n(list[Tensor([4, 0],"float64"),Tensor([4, 0],"float64"),], )

[Pass] paddle.add_n(list[Tensor([4, 0],"float64"),Tensor([4, 0],"float64"),], )
2025-03-03 17:02:26.831688 test begin: paddle.add_n(list[Tensor([4, 0],"float64"),Tensor([4, 4],"float64"),Tensor([4, 4],"float64"),], )

[Pass] paddle.add_n(list[Tensor([4, 0],"float64"),Tensor([4, 4],"float64"),Tensor([4, 4],"float64"),], )
2025-03-03 17:02:26.833168 test begin: paddle.add_n(list[Tensor([4, 0],"float64"),Tensor([4, 4],"float64"),], )

[Pass] paddle.add_n(list[Tensor([4, 0],"float64"),Tensor([4, 4],"float64"),], )
2025-03-03 17:02:26.834157 test begin: paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([0, 4],"float64"),Tensor([4, 4],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([0, 4],"float64"),Tensor([4, 4],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [4, 4], X[1]'s shape = [0, 4].
  [Hint: Expected in_dim == x_dim, but received in_dim:4, 4 != x_dim:0, 4.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.835653 test begin: paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([0, 4],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([0, 4],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [4, 4], X[1]'s shape = [0, 4].
  [Hint: Expected in_dim == x_dim, but received in_dim:4, 4 != x_dim:0, 4.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.836577 test begin: paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([4, 0],"float64"),Tensor([4, 4],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([4, 0],"float64"),Tensor([4, 4],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [4, 4], X[1]'s shape = [4, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:4, 4 != x_dim:4, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.837749 test begin: paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([4, 0],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([4, 0],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [4, 4], X[1]'s shape = [4, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:4, 4 != x_dim:4, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.839436 test begin: paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([4, 4],"float64"),Tensor([0, 4],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([4, 4],"float64"),Tensor([0, 4],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [4, 4], X[2]'s shape = [0, 4].
  [Hint: Expected in_dim == x_dim, but received in_dim:4, 4 != x_dim:0, 4.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.840986 test begin: paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([4, 4],"float64"),Tensor([4, 0],"float64"),], )

[paddle error] paddle.add_n(list[Tensor([4, 4],"float64"),Tensor([4, 4],"float64"),Tensor([4, 0],"float64"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [4, 4], X[2]'s shape = [4, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:4, 4 != x_dim:4, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:26.842059 test begin: paddle.add_n(list[Tensor([64, 0, 32, 32],"float16"),Tensor([64, 0, 32, 32],"float16"),], )

[Pass] paddle.add_n(list[Tensor([64, 0, 32, 32],"float16"),Tensor([64, 0, 32, 32],"float16"),], )
2025-03-03 17:02:26.843057 test begin: paddle.add_n(list[Tensor([64, 0, 32, 32],"float16"),Tensor([64, 128, 32, 32],"float16"),], )

[Pass] paddle.add_n(list[Tensor([64, 0, 32, 32],"float16"),Tensor([64, 128, 32, 32],"float16"),], )
2025-03-03 17:02:27.009412 test begin: paddle.add_n(list[Tensor([64, 0, 32, 32],"float32"),Tensor([64, 0, 32, 32],"float32"),], )

[Pass] paddle.add_n(list[Tensor([64, 0, 32, 32],"float32"),Tensor([64, 0, 32, 32],"float32"),], )
2025-03-03 17:02:27.010733 test begin: paddle.add_n(list[Tensor([64, 0, 32, 32],"float32"),Tensor([64, 128, 32, 32],"float32"),], )

[Pass] paddle.add_n(list[Tensor([64, 0, 32, 32],"float32"),Tensor([64, 128, 32, 32],"float32"),], )
2025-03-03 17:02:27.149123 test begin: paddle.add_n(list[Tensor([64, 128, 0, 32],"float16"),Tensor([64, 128, 0, 32],"float16"),], )

[Pass] paddle.add_n(list[Tensor([64, 128, 0, 32],"float16"),Tensor([64, 128, 0, 32],"float16"),], )
2025-03-03 17:02:27.150650 test begin: paddle.add_n(list[Tensor([64, 128, 0, 32],"float16"),Tensor([64, 128, 32, 32],"float16"),], )

[Pass] paddle.add_n(list[Tensor([64, 128, 0, 32],"float16"),Tensor([64, 128, 32, 32],"float16"),], )
2025-03-03 17:02:27.301356 test begin: paddle.add_n(list[Tensor([64, 128, 0, 32],"float32"),Tensor([64, 128, 0, 32],"float32"),], )

[Pass] paddle.add_n(list[Tensor([64, 128, 0, 32],"float32"),Tensor([64, 128, 0, 32],"float32"),], )
2025-03-03 17:02:27.307336 test begin: paddle.add_n(list[Tensor([64, 128, 0, 32],"float32"),Tensor([64, 128, 32, 32],"float32"),], )

[Pass] paddle.add_n(list[Tensor([64, 128, 0, 32],"float32"),Tensor([64, 128, 32, 32],"float32"),], )
2025-03-03 17:02:27.434930 test begin: paddle.add_n(list[Tensor([64, 128, 32, 0],"float16"),Tensor([64, 128, 32, 0],"float16"),], )

[Pass] paddle.add_n(list[Tensor([64, 128, 32, 0],"float16"),Tensor([64, 128, 32, 0],"float16"),], )
2025-03-03 17:02:27.436693 test begin: paddle.add_n(list[Tensor([64, 128, 32, 0],"float16"),Tensor([64, 128, 32, 32],"float16"),], )

[Pass] paddle.add_n(list[Tensor([64, 128, 32, 0],"float16"),Tensor([64, 128, 32, 32],"float16"),], )
2025-03-03 17:02:27.644728 test begin: paddle.add_n(list[Tensor([64, 128, 32, 0],"float32"),Tensor([64, 128, 32, 0],"float32"),], )

[Pass] paddle.add_n(list[Tensor([64, 128, 32, 0],"float32"),Tensor([64, 128, 32, 0],"float32"),], )
2025-03-03 17:02:27.647243 test begin: paddle.add_n(list[Tensor([64, 128, 32, 0],"float32"),Tensor([64, 128, 32, 32],"float32"),], )

[Pass] paddle.add_n(list[Tensor([64, 128, 32, 0],"float32"),Tensor([64, 128, 32, 32],"float32"),], )
2025-03-03 17:02:27.773178 test begin: paddle.add_n(list[Tensor([64, 128, 32, 32],"float16"),Tensor([0, 128, 32, 32],"float16"),], )

[paddle error] paddle.add_n(list[Tensor([64, 128, 32, 32],"float16"),Tensor([0, 128, 32, 32],"float16"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [64, 128, 32, 32], X[1]'s shape = [0, 128, 32, 32].
  [Hint: Expected in_dim == x_dim, but received in_dim:64, 128, 32, 32 != x_dim:0, 128, 32, 32.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:27.919076 test begin: paddle.add_n(list[Tensor([64, 128, 32, 32],"float16"),Tensor([64, 0, 32, 32],"float16"),], )

[paddle error] paddle.add_n(list[Tensor([64, 128, 32, 32],"float16"),Tensor([64, 0, 32, 32],"float16"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [64, 128, 32, 32], X[1]'s shape = [64, 0, 32, 32].
  [Hint: Expected in_dim == x_dim, but received in_dim:64, 128, 32, 32 != x_dim:64, 0, 32, 32.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:28.063761 test begin: paddle.add_n(list[Tensor([64, 128, 32, 32],"float16"),Tensor([64, 128, 0, 32],"float16"),], )

[paddle error] paddle.add_n(list[Tensor([64, 128, 32, 32],"float16"),Tensor([64, 128, 0, 32],"float16"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [64, 128, 32, 32], X[1]'s shape = [64, 128, 0, 32].
  [Hint: Expected in_dim == x_dim, but received in_dim:64, 128, 32, 32 != x_dim:64, 128, 0, 32.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:28.248340 test begin: paddle.add_n(list[Tensor([64, 128, 32, 32],"float16"),Tensor([64, 128, 32, 0],"float16"),], )

[paddle error] paddle.add_n(list[Tensor([64, 128, 32, 32],"float16"),Tensor([64, 128, 32, 0],"float16"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [64, 128, 32, 32], X[1]'s shape = [64, 128, 32, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:64, 128, 32, 32 != x_dim:64, 128, 32, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:28.395167 test begin: paddle.add_n(list[Tensor([64, 128, 32, 32],"float32"),Tensor([0, 128, 32, 32],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([64, 128, 32, 32],"float32"),Tensor([0, 128, 32, 32],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [64, 128, 32, 32], X[1]'s shape = [0, 128, 32, 32].
  [Hint: Expected in_dim == x_dim, but received in_dim:64, 128, 32, 32 != x_dim:0, 128, 32, 32.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:28.506991 test begin: paddle.add_n(list[Tensor([64, 128, 32, 32],"float32"),Tensor([64, 0, 32, 32],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([64, 128, 32, 32],"float32"),Tensor([64, 0, 32, 32],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [64, 128, 32, 32], X[1]'s shape = [64, 0, 32, 32].
  [Hint: Expected in_dim == x_dim, but received in_dim:64, 128, 32, 32 != x_dim:64, 0, 32, 32.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:28.619377 test begin: paddle.add_n(list[Tensor([64, 128, 32, 32],"float32"),Tensor([64, 128, 0, 32],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([64, 128, 32, 32],"float32"),Tensor([64, 128, 0, 32],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [64, 128, 32, 32], X[1]'s shape = [64, 128, 0, 32].
  [Hint: Expected in_dim == x_dim, but received in_dim:64, 128, 32, 32 != x_dim:64, 128, 0, 32.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:28.737324 test begin: paddle.add_n(list[Tensor([64, 128, 32, 32],"float32"),Tensor([64, 128, 32, 0],"float32"),], )

[paddle error] paddle.add_n(list[Tensor([64, 128, 32, 32],"float32"),Tensor([64, 128, 32, 0],"float32"),], ) 
 (InvalidArgument) The input tensor X of AddNOp must have same shape. But received X[0]'s shape = [64, 128, 32, 32], X[1]'s shape = [64, 128, 32, 0].
  [Hint: Expected in_dim == x_dim, but received in_dim:64, 128, 32, 32 != x_dim:64, 128, 32, 0.] (at ../paddle/phi/infermeta/multiary.cc:427)

2025-03-03 17:02:28.862740 test begin: paddle.allclose(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), rtol=1e-05, atol=1e-08, )

[cuda error] paddle.allclose(Tensor([0, 10],"float64"), Tensor([0, 10],"float64"), rtol=1e-05, atol=1e-08, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.864553 test begin: paddle.allclose(Tensor([0, 10],"float64"), Tensor([10, 10],"float64"), rtol=1e-05, atol=1e-08, )

[paddle error] paddle.allclose(Tensor([0, 10],"float64"), Tensor([10, 10],"float64"), rtol=1e-05, atol=1e-08, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 10.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:10.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.866555 test begin: paddle.allclose(Tensor([0, 13, 128],"float32"), Tensor([0, 13, 128],"float32"), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(Tensor([0, 13, 128],"float32"), Tensor([0, 13, 128],"float32"), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.868024 test begin: paddle.allclose(Tensor([0, 13, 128],"float32"), Tensor([13, 13, 128],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([0, 13, 128],"float32"), Tensor([13, 13, 128],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.869654 test begin: paddle.allclose(Tensor([0, 13, 32],"float32"), Tensor([0, 13, 32],"float32"), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(Tensor([0, 13, 32],"float32"), Tensor([0, 13, 32],"float32"), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.870999 test begin: paddle.allclose(Tensor([0, 13, 32],"float32"), Tensor([13, 13, 32],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([0, 13, 32],"float32"), Tensor([13, 13, 32],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.872130 test begin: paddle.allclose(Tensor([0, 13],"float32"), Tensor([0, 13],"float32"), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(Tensor([0, 13],"float32"), Tensor([0, 13],"float32"), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.874390 test begin: paddle.allclose(Tensor([0, 13],"float32"), Tensor([13, 13],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([0, 13],"float32"), Tensor([13, 13],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.875829 test begin: paddle.allclose(Tensor([0, 16],"float32"), Tensor([0, 16],"float32"), )

[cuda error] paddle.allclose(Tensor([0, 16],"float32"), Tensor([0, 16],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.877204 test begin: paddle.allclose(Tensor([0, 16],"float32"), Tensor([0, 16],"float32"), atol=1e-05, rtol=1e-05, )

[cuda error] paddle.allclose(Tensor([0, 16],"float32"), Tensor([0, 16],"float32"), atol=1e-05, rtol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.878277 test begin: paddle.allclose(Tensor([0, 16],"float32"), Tensor([16, 16],"float32"), )

[paddle error] paddle.allclose(Tensor([0, 16],"float32"), Tensor([16, 16],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 16.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:16.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.879357 test begin: paddle.allclose(Tensor([0, 16],"float32"), Tensor([64, 16],"float32"), atol=1e-05, rtol=1e-05, )

[paddle error] paddle.allclose(Tensor([0, 16],"float32"), Tensor([64, 16],"float32"), atol=1e-05, rtol=1e-05, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 64.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:64.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.880902 test begin: paddle.allclose(Tensor([0, 20, 200],"float32"), Tensor([0, 20, 200],"float32"), )

[cuda error] paddle.allclose(Tensor([0, 20, 200],"float32"), Tensor([0, 20, 200],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.882013 test begin: paddle.allclose(Tensor([0, 20, 200],"float32"), Tensor([1, 20, 200],"float32"), )

[paddle error] paddle.allclose(Tensor([0, 20, 200],"float32"), Tensor([1, 20, 200],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.883132 test begin: paddle.allclose(Tensor([0, 20, 32],"float32"), Tensor([0, 20, 32],"float32"), )

[cuda error] paddle.allclose(Tensor([0, 20, 32],"float32"), Tensor([0, 20, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.884204 test begin: paddle.allclose(Tensor([0, 20, 32],"float32"), Tensor([1, 20, 32],"float32"), )

[paddle error] paddle.allclose(Tensor([0, 20, 32],"float32"), Tensor([1, 20, 32],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.885375 test begin: paddle.allclose(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.886344 test begin: paddle.allclose(Tensor([0, 2],"float32"), Tensor([13, 2],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([0, 2],"float32"), Tensor([13, 2],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.887905 test begin: paddle.allclose(Tensor([0, 3, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), )

[cuda error] paddle.allclose(Tensor([0, 3, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.888977 test begin: paddle.allclose(Tensor([0, 3, 8, 8],"float32"), Tensor([2, 3, 8, 8],"float32"), )

[paddle error] paddle.allclose(Tensor([0, 3, 8, 8],"float32"), Tensor([2, 3, 8, 8],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.890318 test begin: paddle.allclose(Tensor([0, 32],"float32"), Tensor([0, 32],"float32"), )

[cuda error] paddle.allclose(Tensor([0, 32],"float32"), Tensor([0, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.891377 test begin: paddle.allclose(Tensor([0, 32],"float32"), Tensor([1124, 32],"float32"), )

[paddle error] paddle.allclose(Tensor([0, 32],"float32"), Tensor([1124, 32],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1124.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1124.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.892823 test begin: paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.0001, )

[cuda error] paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.894925 test begin: paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.001, )

[cuda error] paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.896541 test begin: paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.01, )

[cuda error] paddle.allclose(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), atol=0.01, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.897913 test begin: paddle.allclose(Tensor([0, 3],"float32"), Tensor([13, 3],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([0, 3],"float32"), Tensor([13, 3],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.899795 test begin: paddle.allclose(Tensor([0, 3],"float32"), Tensor([13, 3],"float32"), atol=0.01, )

[paddle error] paddle.allclose(Tensor([0, 3],"float32"), Tensor([13, 3],"float32"), atol=0.01, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.901201 test begin: paddle.allclose(Tensor([0, 3],"float32"), Tensor([14, 3],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([0, 3],"float32"), Tensor([14, 3],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 14.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:14.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.902596 test begin: paddle.allclose(Tensor([0, 3],"float32"), Tensor([2, 3],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([0, 3],"float32"), Tensor([2, 3],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.903961 test begin: paddle.allclose(Tensor([0, 5, 32],"float32"), Tensor([0, 5, 32],"float32"), atol=1e-05, )

[cuda error] paddle.allclose(Tensor([0, 5, 32],"float32"), Tensor([0, 5, 32],"float32"), atol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.905291 test begin: paddle.allclose(Tensor([0, 5, 32],"float32"), Tensor([1, 5, 32],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([0, 5, 32],"float32"), Tensor([1, 5, 32],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.906495 test begin: paddle.allclose(Tensor([0, 5],"float32"), Tensor([0, 5],"float32"), atol=1e-06, )

[cuda error] paddle.allclose(Tensor([0, 5],"float32"), Tensor([0, 5],"float32"), atol=1e-06, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.908958 test begin: paddle.allclose(Tensor([0, 5],"float32"), Tensor([4, 5],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([0, 5],"float32"), Tensor([4, 5],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 4.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:4.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.910468 test begin: paddle.allclose(Tensor([0, 64, 16],"float32"), Tensor([0, 64, 16],"float32"), atol=1e-05, )

[cuda error] paddle.allclose(Tensor([0, 64, 16],"float32"), Tensor([0, 64, 16],"float32"), atol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.911576 test begin: paddle.allclose(Tensor([0, 64, 16],"float32"), Tensor([14, 64, 16],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([0, 64, 16],"float32"), Tensor([14, 64, 16],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 14.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:14.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.912714 test begin: paddle.allclose(Tensor([0, 8, 32],"float32"), Tensor([0, 8, 32],"float32"), atol=0.0001, )

[cuda error] paddle.allclose(Tensor([0, 8, 32],"float32"), Tensor([0, 8, 32],"float32"), atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.913751 test begin: paddle.allclose(Tensor([0, 8, 32],"float32"), Tensor([13, 8, 32],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([0, 8, 32],"float32"), Tensor([13, 8, 32],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.914753 test begin: paddle.allclose(Tensor([0, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, )

[cuda error] paddle.allclose(Tensor([0, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.915880 test begin: paddle.allclose(Tensor([0, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, rtol=1e-06, )

[cuda error] paddle.allclose(Tensor([0, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, rtol=1e-06, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.917103 test begin: paddle.allclose(Tensor([0, 8],"float32"), Tensor([1, 8],"float32"), atol=1e-06, rtol=1e-06, )

[paddle error] paddle.allclose(Tensor([0, 8],"float32"), Tensor([1, 8],"float32"), atol=1e-06, rtol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.918171 test begin: paddle.allclose(Tensor([0, 8],"float32"), Tensor([2, 8],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([0, 8],"float32"), Tensor([2, 8],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.919195 test begin: paddle.allclose(Tensor([0],"bool"), Tensor([0],"bool"), 0.0, 0.0, False, )

[cuda error] paddle.allclose(Tensor([0],"bool"), Tensor([0],"bool"), 0.0, 0.0, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.921037 test begin: paddle.allclose(Tensor([0],"bool"), Tensor([1],"bool"), 0.0, 0.0, False, )

[paddle error] paddle.allclose(Tensor([0],"bool"), Tensor([1],"bool"), 0.0, 0.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.922495 test begin: paddle.allclose(Tensor([0],"float16"), Tensor([0],"float16"), atol=0.001, )

[cuda error] paddle.allclose(Tensor([0],"float16"), Tensor([0],"float16"), atol=0.001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.923919 test begin: paddle.allclose(Tensor([0],"float16"), Tensor([14],"float16"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([0],"float16"), Tensor([14],"float16"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 14.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:14.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.925020 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.926074 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.0001, )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.927119 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.001, )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.928197 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.5, )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=0.5, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.929394 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-05, rtol=1e-05, )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-05, rtol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.930527 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-06, )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-06, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.931550 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-06, rtol=1e-06, )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), atol=1e-06, rtol=1e-06, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.932610 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=0.01, atol=0.0, name="test_7", )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=0.01, atol=0.0, name="test_7", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.933742 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_1", )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_1", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.934974 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_3", )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_3", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.936406 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_5", )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_5", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.947760 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_2", )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_2", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.956925 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_4", )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_4", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.959762 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_6", )

[cuda error] paddle.allclose(Tensor([0],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_6", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.961260 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([13],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([13],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.963604 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([14],"float32"), atol=0.5, )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([14],"float32"), atol=0.5, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 14.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:14.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.964949 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([1],"float32"), rtol=0.01, atol=0.0, name="test_7", )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([1],"float32"), rtol=0.01, atol=0.0, name="test_7", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.968127 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_1", )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_1", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.970197 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_3", )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_3", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.972219 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_5", )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_5", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.973632 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_2", )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_2", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.977592 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_4", )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_4", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.978818 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_6", )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([2],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_6", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.980373 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([3072],"float32"), )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([3072],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 3072.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3072.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.981421 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([30],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([30],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 30.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:30.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.982400 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([30],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([30],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 30.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:30.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.983391 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([30],"float32"), atol=1e-06, rtol=1e-06, )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([30],"float32"), atol=1e-06, rtol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 30.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:30.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.984430 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([32],"float32"), )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([32],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 32.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:32.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.985307 test begin: paddle.allclose(Tensor([0],"float32"), Tensor([64],"float32"), atol=1e-05, rtol=1e-05, )

[paddle error] paddle.allclose(Tensor([0],"float32"), Tensor([64],"float32"), atol=1e-05, rtol=1e-05, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 64.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:64.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.986304 test begin: paddle.allclose(Tensor([0],"float64"), Tensor([0],"float64"), atol=0.0001, rtol=0.0001, )

[cuda error] paddle.allclose(Tensor([0],"float64"), Tensor([0],"float64"), atol=0.0001, rtol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.987401 test begin: paddle.allclose(Tensor([0],"float64"), Tensor([0],"float64"), rtol=0.015, atol=0.0, name="test_8", )

[cuda error] paddle.allclose(Tensor([0],"float64"), Tensor([0],"float64"), rtol=0.015, atol=0.0, name="test_8", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.988394 test begin: paddle.allclose(Tensor([0],"float64"), Tensor([1],"float64"), atol=0.0001, rtol=0.0001, )

[paddle error] paddle.allclose(Tensor([0],"float64"), Tensor([1],"float64"), atol=0.0001, rtol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.989464 test begin: paddle.allclose(Tensor([0],"float64"), Tensor([1],"float64"), rtol=0.015, atol=0.0, name="test_8", )

[paddle error] paddle.allclose(Tensor([0],"float64"), Tensor([1],"float64"), rtol=0.015, atol=0.0, name="test_8", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.990500 test begin: paddle.allclose(Tensor([0],"float64"), Tensor([512],"float64"), atol=0.0001, rtol=0.0001, )

[paddle error] paddle.allclose(Tensor([0],"float64"), Tensor([512],"float64"), atol=0.0001, rtol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 512.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:512.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.991626 test begin: paddle.allclose(Tensor([0],"int32"), Tensor([0],"int32"), 50.0, 48.0, False, )

[cuda error] paddle.allclose(Tensor([0],"int32"), Tensor([0],"int32"), 50.0, 48.0, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.992919 test begin: paddle.allclose(Tensor([0],"int32"), Tensor([0],"int32"), 50.0, 49.0, False, )

[cuda error] paddle.allclose(Tensor([0],"int32"), Tensor([0],"int32"), 50.0, 49.0, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.994058 test begin: paddle.allclose(Tensor([0],"int32"), Tensor([1],"int32"), 50.0, 48.0, False, )

[paddle error] paddle.allclose(Tensor([0],"int32"), Tensor([1],"int32"), 50.0, 48.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.995079 test begin: paddle.allclose(Tensor([0],"int32"), Tensor([1],"int32"), 50.0, 49.0, False, )

[paddle error] paddle.allclose(Tensor([0],"int32"), Tensor([1],"int32"), 50.0, 49.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:28.996189 test begin: paddle.allclose(Tensor([0],"int64"), Tensor([0],"int64"), 50.0, 48.0, False, )

[cuda error] paddle.allclose(Tensor([0],"int64"), Tensor([0],"int64"), 50.0, 48.0, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.998004 test begin: paddle.allclose(Tensor([0],"int64"), Tensor([0],"int64"), 50.0, 49.0, False, )

[cuda error] paddle.allclose(Tensor([0],"int64"), Tensor([0],"int64"), 50.0, 49.0, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:28.999084 test begin: paddle.allclose(Tensor([0],"int64"), Tensor([1],"int64"), 50.0, 48.0, False, )

[paddle error] paddle.allclose(Tensor([0],"int64"), Tensor([1],"int64"), 50.0, 48.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.000238 test begin: paddle.allclose(Tensor([0],"int64"), Tensor([1],"int64"), 50.0, 49.0, False, )

[paddle error] paddle.allclose(Tensor([0],"int64"), Tensor([1],"int64"), 50.0, 49.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.001519 test begin: paddle.allclose(Tensor([1, 0, 200],"float32"), Tensor([1, 0, 200],"float32"), )

[cuda error] paddle.allclose(Tensor([1, 0, 200],"float32"), Tensor([1, 0, 200],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.002463 test begin: paddle.allclose(Tensor([1, 0, 200],"float32"), Tensor([1, 20, 200],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 0, 200],"float32"), Tensor([1, 20, 200],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 20.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:20.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.003677 test begin: paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 0, 32],"float32"), )

[cuda error] paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 0, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.004606 test begin: paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 0, 32],"float32"), atol=1e-05, )

[cuda error] paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 0, 32],"float32"), atol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.005618 test begin: paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 20, 32],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 20, 32],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 20.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:20.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.006806 test begin: paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 5, 32],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([1, 0, 32],"float32"), Tensor([1, 5, 32],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 5.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:5.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.007956 test begin: paddle.allclose(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), atol=1e-06, rtol=1e-06, )

[cuda error] paddle.allclose(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), atol=1e-06, rtol=1e-06, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.009037 test begin: paddle.allclose(Tensor([1, 0],"float32"), Tensor([1, 8],"float32"), atol=1e-06, rtol=1e-06, )

[paddle error] paddle.allclose(Tensor([1, 0],"float32"), Tensor([1, 8],"float32"), atol=1e-06, rtol=1e-06, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 8.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:8.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.010072 test begin: paddle.allclose(Tensor([1, 20, 0],"float32"), Tensor([1, 20, 0],"float32"), )

[cuda error] paddle.allclose(Tensor([1, 20, 0],"float32"), Tensor([1, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.011129 test begin: paddle.allclose(Tensor([1, 20, 0],"float32"), Tensor([1, 20, 200],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 20, 0],"float32"), Tensor([1, 20, 200],"float32"), ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 200.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:200.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.012773 test begin: paddle.allclose(Tensor([1, 20, 0],"float32"), Tensor([1, 20, 32],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 20, 0],"float32"), Tensor([1, 20, 32],"float32"), ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 32.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:32.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.014318 test begin: paddle.allclose(Tensor([1, 20, 200],"float32"), Tensor([0, 20, 200],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 20, 200],"float32"), Tensor([0, 20, 200],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.015281 test begin: paddle.allclose(Tensor([1, 20, 200],"float32"), Tensor([1, 0, 200],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 20, 200],"float32"), Tensor([1, 0, 200],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 20 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:20 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.016450 test begin: paddle.allclose(Tensor([1, 20, 200],"float32"), Tensor([1, 20, 0],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 20, 200],"float32"), Tensor([1, 20, 0],"float32"), ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 200 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:200 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.017622 test begin: paddle.allclose(Tensor([1, 20, 32],"float32"), Tensor([0, 20, 32],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 20, 32],"float32"), Tensor([0, 20, 32],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.019279 test begin: paddle.allclose(Tensor([1, 20, 32],"float32"), Tensor([1, 0, 32],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 20, 32],"float32"), Tensor([1, 0, 32],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 20 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:20 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.020252 test begin: paddle.allclose(Tensor([1, 20, 32],"float32"), Tensor([1, 20, 0],"float32"), )

[paddle error] paddle.allclose(Tensor([1, 20, 32],"float32"), Tensor([1, 20, 0],"float32"), ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 32 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:32 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.021272 test begin: paddle.allclose(Tensor([1, 5, 0],"float32"), Tensor([1, 5, 0],"float32"), atol=1e-05, )

[cuda error] paddle.allclose(Tensor([1, 5, 0],"float32"), Tensor([1, 5, 0],"float32"), atol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.022242 test begin: paddle.allclose(Tensor([1, 5, 0],"float32"), Tensor([1, 5, 32],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([1, 5, 0],"float32"), Tensor([1, 5, 32],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 32.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:32.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.023432 test begin: paddle.allclose(Tensor([1, 5, 32],"float32"), Tensor([0, 5, 32],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([1, 5, 32],"float32"), Tensor([0, 5, 32],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.024477 test begin: paddle.allclose(Tensor([1, 5, 32],"float32"), Tensor([1, 0, 32],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([1, 5, 32],"float32"), Tensor([1, 0, 32],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 5 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:5 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.026012 test begin: paddle.allclose(Tensor([1, 5, 32],"float32"), Tensor([1, 5, 0],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([1, 5, 32],"float32"), Tensor([1, 5, 0],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 32 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:32 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.031300 test begin: paddle.allclose(Tensor([1, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, rtol=1e-06, )

[paddle error] paddle.allclose(Tensor([1, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, rtol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.033728 test begin: paddle.allclose(Tensor([1, 8],"float32"), Tensor([1, 0],"float32"), atol=1e-06, rtol=1e-06, )

[paddle error] paddle.allclose(Tensor([1, 8],"float32"), Tensor([1, 0],"float32"), atol=1e-06, rtol=1e-06, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 8 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:8 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.035680 test begin: paddle.allclose(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), rtol=1e-05, atol=1e-08, )

[cuda error] paddle.allclose(Tensor([10, 0],"float64"), Tensor([10, 0],"float64"), rtol=1e-05, atol=1e-08, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.037203 test begin: paddle.allclose(Tensor([10, 0],"float64"), Tensor([10, 10],"float64"), rtol=1e-05, atol=1e-08, )

[paddle error] paddle.allclose(Tensor([10, 0],"float64"), Tensor([10, 10],"float64"), rtol=1e-05, atol=1e-08, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 10.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:10.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.039422 test begin: paddle.allclose(Tensor([10, 10],"float64"), Tensor([0, 10],"float64"), rtol=1e-05, atol=1e-08, )

[paddle error] paddle.allclose(Tensor([10, 10],"float64"), Tensor([0, 10],"float64"), rtol=1e-05, atol=1e-08, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 10 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:10 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.043990 test begin: paddle.allclose(Tensor([10, 10],"float64"), Tensor([10, 0],"float64"), rtol=1e-05, atol=1e-08, )

[paddle error] paddle.allclose(Tensor([10, 10],"float64"), Tensor([10, 0],"float64"), rtol=1e-05, atol=1e-08, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 10 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:10 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.045666 test begin: paddle.allclose(Tensor([1124, 0],"float32"), Tensor([1124, 0],"float32"), )

[cuda error] paddle.allclose(Tensor([1124, 0],"float32"), Tensor([1124, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.047037 test begin: paddle.allclose(Tensor([1124, 0],"float32"), Tensor([1124, 32],"float32"), )

[paddle error] paddle.allclose(Tensor([1124, 0],"float32"), Tensor([1124, 32],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 32.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:32.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.048868 test begin: paddle.allclose(Tensor([1124, 32],"float32"), Tensor([0, 32],"float32"), )

[paddle error] paddle.allclose(Tensor([1124, 32],"float32"), Tensor([0, 32],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1124 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1124 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.050266 test begin: paddle.allclose(Tensor([1124, 32],"float32"), Tensor([1124, 0],"float32"), )

[paddle error] paddle.allclose(Tensor([1124, 32],"float32"), Tensor([1124, 0],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 32 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:32 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.051549 test begin: paddle.allclose(Tensor([13, 0, 128],"float32"), Tensor([13, 0, 128],"float32"), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(Tensor([13, 0, 128],"float32"), Tensor([13, 0, 128],"float32"), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.052683 test begin: paddle.allclose(Tensor([13, 0, 128],"float32"), Tensor([13, 13, 128],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 0, 128],"float32"), Tensor([13, 13, 128],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.053881 test begin: paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 0, 32],"float32"), atol=0.0001, )

[cuda error] paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 0, 32],"float32"), atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.054961 test begin: paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 0, 32],"float32"), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 0, 32],"float32"), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.056037 test begin: paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 13, 32],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 13, 32],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.057242 test begin: paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 8, 32],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 0, 32],"float32"), Tensor([13, 8, 32],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 8.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:8.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.058321 test begin: paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), atol=0.001, )

[cuda error] paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), atol=0.001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.059465 test begin: paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), atol=0.01, )

[cuda error] paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), atol=0.01, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.061324 test begin: paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 0],"float32"), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.062716 test begin: paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 13],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 13],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.063940 test begin: paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 2],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 2],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.065226 test begin: paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 3],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 3],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.067240 test begin: paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 3],"float32"), atol=0.01, )

[paddle error] paddle.allclose(Tensor([13, 0],"float32"), Tensor([13, 3],"float32"), atol=0.01, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.068380 test begin: paddle.allclose(Tensor([13, 13, 0],"float32"), Tensor([13, 13, 0],"float32"), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(Tensor([13, 13, 0],"float32"), Tensor([13, 13, 0],"float32"), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.069364 test begin: paddle.allclose(Tensor([13, 13, 0],"float32"), Tensor([13, 13, 128],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13, 0],"float32"), Tensor([13, 13, 128],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 128.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:128.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.070647 test begin: paddle.allclose(Tensor([13, 13, 0],"float32"), Tensor([13, 13, 32],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13, 0],"float32"), Tensor([13, 13, 32],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 32.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:32.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.071785 test begin: paddle.allclose(Tensor([13, 13, 128],"float32"), Tensor([0, 13, 128],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13, 128],"float32"), Tensor([0, 13, 128],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.073066 test begin: paddle.allclose(Tensor([13, 13, 128],"float32"), Tensor([13, 0, 128],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13, 128],"float32"), Tensor([13, 0, 128],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.074353 test begin: paddle.allclose(Tensor([13, 13, 128],"float32"), Tensor([13, 13, 0],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13, 128],"float32"), Tensor([13, 13, 0],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 128 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:128 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.076675 test begin: paddle.allclose(Tensor([13, 13, 32],"float32"), Tensor([0, 13, 32],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13, 32],"float32"), Tensor([0, 13, 32],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.077919 test begin: paddle.allclose(Tensor([13, 13, 32],"float32"), Tensor([13, 0, 32],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13, 32],"float32"), Tensor([13, 0, 32],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.080453 test begin: paddle.allclose(Tensor([13, 13, 32],"float32"), Tensor([13, 13, 0],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13, 32],"float32"), Tensor([13, 13, 0],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 32 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:32 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.081974 test begin: paddle.allclose(Tensor([13, 13],"float32"), Tensor([0, 13],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13],"float32"), Tensor([0, 13],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.083495 test begin: paddle.allclose(Tensor([13, 13],"float32"), Tensor([13, 0],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 13],"float32"), Tensor([13, 0],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.084770 test begin: paddle.allclose(Tensor([13, 2],"float32"), Tensor([0, 2],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 2],"float32"), Tensor([0, 2],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.085994 test begin: paddle.allclose(Tensor([13, 2],"float32"), Tensor([13, 0],"float32"), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 2],"float32"), Tensor([13, 0],"float32"), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.087026 test begin: paddle.allclose(Tensor([13, 3],"float32"), Tensor([0, 3],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([13, 3],"float32"), Tensor([0, 3],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.088066 test begin: paddle.allclose(Tensor([13, 3],"float32"), Tensor([0, 3],"float32"), atol=0.01, )

[paddle error] paddle.allclose(Tensor([13, 3],"float32"), Tensor([0, 3],"float32"), atol=0.01, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.089086 test begin: paddle.allclose(Tensor([13, 3],"float32"), Tensor([13, 0],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([13, 3],"float32"), Tensor([13, 0],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.090155 test begin: paddle.allclose(Tensor([13, 3],"float32"), Tensor([13, 0],"float32"), atol=0.01, )

[paddle error] paddle.allclose(Tensor([13, 3],"float32"), Tensor([13, 0],"float32"), atol=0.01, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.091184 test begin: paddle.allclose(Tensor([13, 8, 0],"float32"), Tensor([13, 8, 0],"float32"), atol=0.0001, )

[cuda error] paddle.allclose(Tensor([13, 8, 0],"float32"), Tensor([13, 8, 0],"float32"), atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.092311 test begin: paddle.allclose(Tensor([13, 8, 0],"float32"), Tensor([13, 8, 32],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 8, 0],"float32"), Tensor([13, 8, 32],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 32.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:32.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.093321 test begin: paddle.allclose(Tensor([13, 8, 32],"float32"), Tensor([0, 8, 32],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 8, 32],"float32"), Tensor([0, 8, 32],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.094446 test begin: paddle.allclose(Tensor([13, 8, 32],"float32"), Tensor([13, 0, 32],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 8, 32],"float32"), Tensor([13, 0, 32],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 8 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:8 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.095467 test begin: paddle.allclose(Tensor([13, 8, 32],"float32"), Tensor([13, 8, 0],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([13, 8, 32],"float32"), Tensor([13, 8, 0],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 32 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:32 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.096436 test begin: paddle.allclose(Tensor([13],"float32"), Tensor([0],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([13],"float32"), Tensor([0],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.097501 test begin: paddle.allclose(Tensor([14, 0, 16],"float32"), Tensor([14, 0, 16],"float32"), atol=1e-05, )

[cuda error] paddle.allclose(Tensor([14, 0, 16],"float32"), Tensor([14, 0, 16],"float32"), atol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.099339 test begin: paddle.allclose(Tensor([14, 0, 16],"float32"), Tensor([14, 64, 16],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([14, 0, 16],"float32"), Tensor([14, 64, 16],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 64.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:64.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.101314 test begin: paddle.allclose(Tensor([14, 0],"float32"), Tensor([14, 0],"float32"), atol=0.001, )

[cuda error] paddle.allclose(Tensor([14, 0],"float32"), Tensor([14, 0],"float32"), atol=0.001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.102733 test begin: paddle.allclose(Tensor([14, 0],"float32"), Tensor([14, 3],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([14, 0],"float32"), Tensor([14, 3],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.104701 test begin: paddle.allclose(Tensor([14, 3],"float32"), Tensor([0, 3],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([14, 3],"float32"), Tensor([0, 3],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 14 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:14 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.106188 test begin: paddle.allclose(Tensor([14, 3],"float32"), Tensor([14, 0],"float32"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([14, 3],"float32"), Tensor([14, 0],"float32"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.107722 test begin: paddle.allclose(Tensor([14, 64, 0],"float32"), Tensor([14, 64, 0],"float32"), atol=1e-05, )

[cuda error] paddle.allclose(Tensor([14, 64, 0],"float32"), Tensor([14, 64, 0],"float32"), atol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.109278 test begin: paddle.allclose(Tensor([14, 64, 0],"float32"), Tensor([14, 64, 16],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([14, 64, 0],"float32"), Tensor([14, 64, 16],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 16.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:16.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.110537 test begin: paddle.allclose(Tensor([14, 64, 16],"float32"), Tensor([0, 64, 16],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([14, 64, 16],"float32"), Tensor([0, 64, 16],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 14 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:14 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.111800 test begin: paddle.allclose(Tensor([14, 64, 16],"float32"), Tensor([14, 0, 16],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([14, 64, 16],"float32"), Tensor([14, 0, 16],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 64 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:64 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.113060 test begin: paddle.allclose(Tensor([14, 64, 16],"float32"), Tensor([14, 64, 0],"float32"), atol=1e-05, )

[paddle error] paddle.allclose(Tensor([14, 64, 16],"float32"), Tensor([14, 64, 0],"float32"), atol=1e-05, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 16 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:16 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.114200 test begin: paddle.allclose(Tensor([14],"float16"), Tensor([0],"float16"), atol=0.001, )

[paddle error] paddle.allclose(Tensor([14],"float16"), Tensor([0],"float16"), atol=0.001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 14 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:14 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.115227 test begin: paddle.allclose(Tensor([14],"float32"), Tensor([0],"float32"), atol=0.5, )

[paddle error] paddle.allclose(Tensor([14],"float32"), Tensor([0],"float32"), atol=0.5, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 14 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:14 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.116286 test begin: paddle.allclose(Tensor([16, 0],"float32"), Tensor([16, 0],"float32"), )

[cuda error] paddle.allclose(Tensor([16, 0],"float32"), Tensor([16, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.117320 test begin: paddle.allclose(Tensor([16, 0],"float32"), Tensor([16, 16],"float32"), )

[paddle error] paddle.allclose(Tensor([16, 0],"float32"), Tensor([16, 16],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 16.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:16.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.118282 test begin: paddle.allclose(Tensor([16, 16],"float32"), Tensor([0, 16],"float32"), )

[paddle error] paddle.allclose(Tensor([16, 16],"float32"), Tensor([0, 16],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 16 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:16 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.119319 test begin: paddle.allclose(Tensor([16, 16],"float32"), Tensor([16, 0],"float32"), )

[paddle error] paddle.allclose(Tensor([16, 16],"float32"), Tensor([16, 0],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 16 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:16 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.120342 test begin: paddle.allclose(Tensor([1],"bool"), Tensor([0],"bool"), 0.0, 0.0, False, )

[paddle error] paddle.allclose(Tensor([1],"bool"), Tensor([0],"bool"), 0.0, 0.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.121384 test begin: paddle.allclose(Tensor([1],"float32"), Tensor([0],"float32"), rtol=0.01, atol=0.0, name="test_7", )

[paddle error] paddle.allclose(Tensor([1],"float32"), Tensor([0],"float32"), rtol=0.01, atol=0.0, name="test_7", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.122545 test begin: paddle.allclose(Tensor([1],"float64"), Tensor([0],"float64"), atol=0.0001, rtol=0.0001, )

[paddle error] paddle.allclose(Tensor([1],"float64"), Tensor([0],"float64"), atol=0.0001, rtol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.123603 test begin: paddle.allclose(Tensor([1],"float64"), Tensor([0],"float64"), rtol=0.015, atol=0.0, name="test_8", )

[paddle error] paddle.allclose(Tensor([1],"float64"), Tensor([0],"float64"), rtol=0.015, atol=0.0, name="test_8", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.124612 test begin: paddle.allclose(Tensor([1],"int32"), Tensor([0],"int32"), 50.0, 48.0, False, )

[paddle error] paddle.allclose(Tensor([1],"int32"), Tensor([0],"int32"), 50.0, 48.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.125727 test begin: paddle.allclose(Tensor([1],"int32"), Tensor([0],"int32"), 50.0, 49.0, False, )

[paddle error] paddle.allclose(Tensor([1],"int32"), Tensor([0],"int32"), 50.0, 49.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.126792 test begin: paddle.allclose(Tensor([1],"int64"), Tensor([0],"int64"), 50.0, 48.0, False, )

[paddle error] paddle.allclose(Tensor([1],"int64"), Tensor([0],"int64"), 50.0, 48.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.127787 test begin: paddle.allclose(Tensor([1],"int64"), Tensor([0],"int64"), 50.0, 49.0, False, )

[paddle error] paddle.allclose(Tensor([1],"int64"), Tensor([0],"int64"), 50.0, 49.0, False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.128787 test begin: paddle.allclose(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), )

[cuda error] paddle.allclose(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.129858 test begin: paddle.allclose(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 3, 8, 8],"float32"), )

[paddle error] paddle.allclose(Tensor([2, 0, 8, 8],"float32"), Tensor([2, 3, 8, 8],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.131014 test begin: paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), atol=0.0001, )

[cuda error] paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.132035 test begin: paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), atol=1e-06, )

[cuda error] paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), atol=1e-06, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.133041 test begin: paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 3],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 3],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.134091 test begin: paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 8],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([2, 0],"float32"), Tensor([2, 8],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 8.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:8.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.135028 test begin: paddle.allclose(Tensor([2, 3, 0, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), )

[cuda error] paddle.allclose(Tensor([2, 3, 0, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.135961 test begin: paddle.allclose(Tensor([2, 3, 0, 8],"float32"), Tensor([2, 3, 8, 8],"float32"), )

[paddle error] paddle.allclose(Tensor([2, 3, 0, 8],"float32"), Tensor([2, 3, 8, 8],"float32"), ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 8.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:8.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.136941 test begin: paddle.allclose(Tensor([2, 3, 8, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), )

[cuda error] paddle.allclose(Tensor([2, 3, 8, 0],"float32"), Tensor([2, 3, 8, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.137851 test begin: paddle.allclose(Tensor([2, 3, 8, 0],"float32"), Tensor([2, 3, 8, 8],"float32"), )

[paddle error] paddle.allclose(Tensor([2, 3, 8, 0],"float32"), Tensor([2, 3, 8, 8],"float32"), ) 
 (PreconditionNotMet) The value at dim 3 of Input(Input) is not equal to the Input(Other): 0 != 8.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:8.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.138893 test begin: paddle.allclose(Tensor([2, 3, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), )

[paddle error] paddle.allclose(Tensor([2, 3, 8, 8],"float32"), Tensor([0, 3, 8, 8],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.140124 test begin: paddle.allclose(Tensor([2, 3, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), )

[paddle error] paddle.allclose(Tensor([2, 3, 8, 8],"float32"), Tensor([2, 0, 8, 8],"float32"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.141128 test begin: paddle.allclose(Tensor([2, 3, 8, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), )

[paddle error] paddle.allclose(Tensor([2, 3, 8, 8],"float32"), Tensor([2, 3, 0, 8],"float32"), ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 8 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:8 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.142093 test begin: paddle.allclose(Tensor([2, 3, 8, 8],"float32"), Tensor([2, 3, 8, 0],"float32"), )

[paddle error] paddle.allclose(Tensor([2, 3, 8, 8],"float32"), Tensor([2, 3, 8, 0],"float32"), ) 
 (PreconditionNotMet) The value at dim 3 of Input(Input) is not equal to the Input(Other): 8 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:8 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.143084 test begin: paddle.allclose(Tensor([2, 3],"float32"), Tensor([0, 3],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([2, 3],"float32"), Tensor([0, 3],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.144132 test begin: paddle.allclose(Tensor([2, 3],"float32"), Tensor([2, 0],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([2, 3],"float32"), Tensor([2, 0],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.145140 test begin: paddle.allclose(Tensor([2, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([2, 8],"float32"), Tensor([0, 8],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.146204 test begin: paddle.allclose(Tensor([2, 8],"float32"), Tensor([2, 0],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([2, 8],"float32"), Tensor([2, 0],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 8 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:8 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.147304 test begin: paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_1", )

[paddle error] paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_1", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.148449 test begin: paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_3", )

[paddle error] paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_3", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.149492 test begin: paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_5", )

[paddle error] paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=False, name="test_5", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.150509 test begin: paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_2", )

[paddle error] paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_2", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.151609 test begin: paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_4", )

[paddle error] paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_4", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.152608 test begin: paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_6", )

[paddle error] paddle.allclose(Tensor([2],"float32"), Tensor([0],"float32"), rtol=1e-05, atol=1e-08, equal_nan=True, name="test_6", ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.153711 test begin: paddle.allclose(Tensor([3072],"float32"), Tensor([0],"float32"), )

[paddle error] paddle.allclose(Tensor([3072],"float32"), Tensor([0],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 3072 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3072 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.154697 test begin: paddle.allclose(Tensor([30],"float32"), Tensor([0],"float32"), atol=0.0001, )

[paddle error] paddle.allclose(Tensor([30],"float32"), Tensor([0],"float32"), atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 30 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:30 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.155743 test begin: paddle.allclose(Tensor([30],"float32"), Tensor([0],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([30],"float32"), Tensor([0],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 30 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:30 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.156752 test begin: paddle.allclose(Tensor([30],"float32"), Tensor([0],"float32"), atol=1e-06, rtol=1e-06, )

[paddle error] paddle.allclose(Tensor([30],"float32"), Tensor([0],"float32"), atol=1e-06, rtol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 30 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:30 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.157813 test begin: paddle.allclose(Tensor([32],"float32"), Tensor([0],"float32"), )

[paddle error] paddle.allclose(Tensor([32],"float32"), Tensor([0],"float32"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 32 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:32 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.158767 test begin: paddle.allclose(Tensor([4, 0],"float32"), Tensor([4, 0],"float32"), atol=1e-06, )

[cuda error] paddle.allclose(Tensor([4, 0],"float32"), Tensor([4, 0],"float32"), atol=1e-06, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.159874 test begin: paddle.allclose(Tensor([4, 0],"float32"), Tensor([4, 5],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([4, 0],"float32"), Tensor([4, 5],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 5.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:5.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.160834 test begin: paddle.allclose(Tensor([4, 5],"float32"), Tensor([0, 5],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([4, 5],"float32"), Tensor([0, 5],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 4 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:4 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.161798 test begin: paddle.allclose(Tensor([4, 5],"float32"), Tensor([4, 0],"float32"), atol=1e-06, )

[paddle error] paddle.allclose(Tensor([4, 5],"float32"), Tensor([4, 0],"float32"), atol=1e-06, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 5 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:5 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.162840 test begin: paddle.allclose(Tensor([512],"float64"), Tensor([0],"float64"), atol=0.0001, rtol=0.0001, )

[paddle error] paddle.allclose(Tensor([512],"float64"), Tensor([0],"float64"), atol=0.0001, rtol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 512 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:512 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.163881 test begin: paddle.allclose(Tensor([64, 0],"float32"), Tensor([64, 0],"float32"), atol=1e-05, rtol=1e-05, )

[cuda error] paddle.allclose(Tensor([64, 0],"float32"), Tensor([64, 0],"float32"), atol=1e-05, rtol=1e-05, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.164961 test begin: paddle.allclose(Tensor([64, 0],"float32"), Tensor([64, 16],"float32"), atol=1e-05, rtol=1e-05, )

[paddle error] paddle.allclose(Tensor([64, 0],"float32"), Tensor([64, 16],"float32"), atol=1e-05, rtol=1e-05, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 16.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:16.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.166201 test begin: paddle.allclose(Tensor([64, 16],"float32"), Tensor([0, 16],"float32"), atol=1e-05, rtol=1e-05, )

[paddle error] paddle.allclose(Tensor([64, 16],"float32"), Tensor([0, 16],"float32"), atol=1e-05, rtol=1e-05, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 64 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:64 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.167347 test begin: paddle.allclose(Tensor([64, 16],"float32"), Tensor([64, 0],"float32"), atol=1e-05, rtol=1e-05, )

[paddle error] paddle.allclose(Tensor([64, 16],"float32"), Tensor([64, 0],"float32"), atol=1e-05, rtol=1e-05, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 16 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:16 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.168553 test begin: paddle.allclose(Tensor([64],"float32"), Tensor([0],"float32"), atol=1e-05, rtol=1e-05, )

[paddle error] paddle.allclose(Tensor([64],"float32"), Tensor([0],"float32"), atol=1e-05, rtol=1e-05, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 64 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:64 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.169651 test begin: paddle.allclose(tuple(Tensor([0, 20, 100],"float32"),), tuple(Tensor([0, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([0, 20, 100],"float32"),), tuple(Tensor([0, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.171054 test begin: paddle.allclose(tuple(Tensor([0, 20, 100],"float32"),), tuple(Tensor([2, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([0, 20, 100],"float32"),), tuple(Tensor([2, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.172530 test begin: paddle.allclose(tuple(Tensor([0, 20, 32],"float32"),), tuple(Tensor([0, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([0, 20, 32],"float32"),), tuple(Tensor([0, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.174650 test begin: paddle.allclose(tuple(Tensor([0, 20, 32],"float32"),), tuple(Tensor([2, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([0, 20, 32],"float32"),), tuple(Tensor([2, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.176222 test begin: paddle.allclose(tuple(Tensor([0, 3],"float32"),), tuple(Tensor([0, 3],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([0, 3],"float32"),), tuple(Tensor([0, 3],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.177507 test begin: paddle.allclose(tuple(Tensor([0, 3],"float32"),), tuple(Tensor([2, 3],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([0, 3],"float32"),), tuple(Tensor([2, 3],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.179001 test begin: paddle.allclose(tuple(Tensor([0, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), tuple(Tensor([0, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([0, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), tuple(Tensor([0, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.180595 test begin: paddle.allclose(tuple(Tensor([0, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([0, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.182699 test begin: paddle.allclose(tuple(Tensor([0, 7],"float32"),Tensor([0, 7],"float32"),), tuple(Tensor([0, 7],"float32"),Tensor([0, 7],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([0, 7],"float32"),Tensor([0, 7],"float32"),), tuple(Tensor([0, 7],"float32"),Tensor([0, 7],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.184530 test begin: paddle.allclose(tuple(Tensor([0, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([0, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 13.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:13.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.186325 test begin: paddle.allclose(tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 0, 16],"float32"),), tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 0, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 0, 16],"float32"),), tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 0, 16],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.188005 test begin: paddle.allclose(tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 7.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:7.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.189681 test begin: paddle.allclose(tuple(Tensor([13, 0],"float32"),Tensor([13, 0],"float32"),), tuple(Tensor([13, 0],"float32"),Tensor([13, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([13, 0],"float32"),Tensor([13, 0],"float32"),), tuple(Tensor([13, 0],"float32"),Tensor([13, 0],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.191978 test begin: paddle.allclose(tuple(Tensor([13, 0],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([13, 0],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 7.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:7.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.193618 test begin: paddle.allclose(tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 0],"float32"),), tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 0],"float32"),), tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 0],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.195285 test begin: paddle.allclose(tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 16.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:16.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.197924 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.201113 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 0, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 0, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.203890 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 0],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 0],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.205732 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([0, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([0, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.207250 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 0, 16],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 7 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:7 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.208843 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 0],"float32"),Tensor([13, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 16 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:16 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.210454 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([0, 7, 16],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.212456 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 0, 16],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 0, 16],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.214181 test begin: paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 16],"float32"),), tuple(Tensor([13, 7, 16],"float32"),Tensor([13, 7, 0],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.215615 test begin: paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([0, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([0, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.217179 test begin: paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 0],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 0],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.219188 test begin: paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([0, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([0, 7],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 13 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:13 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.220977 test begin: paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 0],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 0],"float32"),Tensor([13, 7],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 7 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:7 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.222396 test begin: paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([0, 7],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([0, 7],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.224129 test begin: paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[Pass] paddle.allclose(tuple(Tensor([13, 7],"float32"),Tensor([13, 7],"float32"),), tuple(Tensor([13, 7],"float32"),Tensor([13, 0],"float32"),), rtol=0.0001, atol=0.0001, )
2025-03-03 17:02:29.226047 test begin: paddle.allclose(tuple(Tensor([2, 0, 100],"float32"),), tuple(Tensor([2, 0, 100],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([2, 0, 100],"float32"),), tuple(Tensor([2, 0, 100],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.227208 test begin: paddle.allclose(tuple(Tensor([2, 0, 100],"float32"),), tuple(Tensor([2, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 0, 100],"float32"),), tuple(Tensor([2, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 20.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:20.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.228391 test begin: paddle.allclose(tuple(Tensor([2, 0, 32],"float32"),), tuple(Tensor([2, 0, 32],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([2, 0, 32],"float32"),), tuple(Tensor([2, 0, 32],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.229652 test begin: paddle.allclose(tuple(Tensor([2, 0, 32],"float32"),), tuple(Tensor([2, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 0, 32],"float32"),), tuple(Tensor([2, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 20.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:20.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.230860 test begin: paddle.allclose(tuple(Tensor([2, 0],"float32"),), tuple(Tensor([2, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([2, 0],"float32"),), tuple(Tensor([2, 0],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.234877 test begin: paddle.allclose(tuple(Tensor([2, 0],"float32"),), tuple(Tensor([2, 3],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 0],"float32"),), tuple(Tensor([2, 3],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.240208 test begin: paddle.allclose(tuple(Tensor([2, 20, 0],"float32"),), tuple(Tensor([2, 20, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[cuda error] paddle.allclose(tuple(Tensor([2, 20, 0],"float32"),), tuple(Tensor([2, 20, 0],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.246178 test begin: paddle.allclose(tuple(Tensor([2, 20, 0],"float32"),), tuple(Tensor([2, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 20, 0],"float32"),), tuple(Tensor([2, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 100.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:100.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.252068 test begin: paddle.allclose(tuple(Tensor([2, 20, 0],"float32"),), tuple(Tensor([2, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 20, 0],"float32"),), tuple(Tensor([2, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 32.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:32.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.255996 test begin: paddle.allclose(tuple(Tensor([2, 20, 100],"float32"),), tuple(Tensor([0, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 20, 100],"float32"),), tuple(Tensor([0, 20, 100],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.258982 test begin: paddle.allclose(tuple(Tensor([2, 20, 100],"float32"),), tuple(Tensor([2, 0, 100],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 20, 100],"float32"),), tuple(Tensor([2, 0, 100],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 20 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:20 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.260944 test begin: paddle.allclose(tuple(Tensor([2, 20, 100],"float32"),), tuple(Tensor([2, 20, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 20, 100],"float32"),), tuple(Tensor([2, 20, 0],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 100 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:100 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.262448 test begin: paddle.allclose(tuple(Tensor([2, 20, 32],"float32"),), tuple(Tensor([0, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 20, 32],"float32"),), tuple(Tensor([0, 20, 32],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.263832 test begin: paddle.allclose(tuple(Tensor([2, 20, 32],"float32"),), tuple(Tensor([2, 0, 32],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 20, 32],"float32"),), tuple(Tensor([2, 0, 32],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 20 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:20 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.265090 test begin: paddle.allclose(tuple(Tensor([2, 20, 32],"float32"),), tuple(Tensor([2, 20, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 20, 32],"float32"),), tuple(Tensor([2, 20, 0],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 32 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:32 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.266371 test begin: paddle.allclose(tuple(Tensor([2, 3],"float32"),), tuple(Tensor([0, 3],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 3],"float32"),), tuple(Tensor([0, 3],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.267837 test begin: paddle.allclose(tuple(Tensor([2, 3],"float32"),), tuple(Tensor([2, 0],"float32"),), rtol=0.0001, atol=0.0001, )

[paddle error] paddle.allclose(tuple(Tensor([2, 3],"float32"),), tuple(Tensor([2, 0],"float32"),), rtol=0.0001, atol=0.0001, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.269713 test begin: paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), )

[cuda error] paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.271127 test begin: paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )

[cuda error] paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.272485 test begin: paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), )

[paddle error] paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.273674 test begin: paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )

[paddle error] paddle.allclose(x=Tensor([0, 2, 2, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.274683 test begin: paddle.allclose(x=Tensor([0],"float32"), y=Tensor([0],"float32"), rtol=0.01, atol=0.01, equal_nan=False, )

[cuda error] paddle.allclose(x=Tensor([0],"float32"), y=Tensor([0],"float32"), rtol=0.01, atol=0.01, equal_nan=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.276033 test begin: paddle.allclose(x=Tensor([0],"float32"), y=Tensor([1],"float32"), rtol=0.01, atol=0.01, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([0],"float32"), y=Tensor([1],"float32"), rtol=0.01, atol=0.01, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.277537 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.0, equal_nan=False, )

[cuda error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.0, equal_nan=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.282577 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.01, equal_nan=False, )

[cuda error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.01, equal_nan=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.284835 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=False, )

[cuda error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.290813 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=True, )

[cuda error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.293283 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=False, )

[cuda error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.299992 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=True, )

[cuda error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.301783 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), )

[paddle error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.307717 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=0.01, atol=0.0, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=0.01, atol=0.0, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.309687 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=0.01, atol=0.01, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=0.01, atol=0.01, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.311075 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=1e-05, atol=0.001, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=1e-05, atol=0.001, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.312720 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=1e-05, atol=0.001, equal_nan=True, )

[paddle error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=1e-05, atol=0.001, equal_nan=True, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.313925 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=1e-06, atol=0.001, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([1],"float64"), rtol=1e-06, atol=0.001, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 1.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:1.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.315173 test begin: paddle.allclose(x=Tensor([0],"float64"), y=Tensor([3],"float64"), rtol=1e-06, atol=0.001, equal_nan=True, )

[paddle error] paddle.allclose(x=Tensor([0],"float64"), y=Tensor([3],"float64"), rtol=1e-06, atol=0.001, equal_nan=True, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.316416 test begin: paddle.allclose(x=Tensor([1],"float32"), y=Tensor([0],"float32"), rtol=0.01, atol=0.01, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([1],"float32"), y=Tensor([0],"float32"), rtol=0.01, atol=0.01, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.317674 test begin: paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), )

[paddle error] paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.318927 test begin: paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.0, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.0, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.320181 test begin: paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.01, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=0.01, atol=0.01, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.321401 test begin: paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.322598 test begin: paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=True, )

[paddle error] paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=1e-05, atol=0.001, equal_nan=True, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.323751 test begin: paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=False, )

[paddle error] paddle.allclose(x=Tensor([1],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=False, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 1 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:1 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.325060 test begin: paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), )

[cuda error] paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.326307 test begin: paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )

[cuda error] paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.327498 test begin: paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), )

[paddle error] paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.328659 test begin: paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )

[paddle error] paddle.allclose(x=Tensor([2, 0, 2, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.329930 test begin: paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), )

[cuda error] paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.331128 test begin: paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), rtol=-3.0, atol=-2.0, )

[cuda error] paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.332356 test begin: paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), )

[paddle error] paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.333483 test begin: paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )

[paddle error] paddle.allclose(x=Tensor([2, 2, 0, 3],"float64"), y=Tensor([2, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 0 != 2.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:2.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.334670 test begin: paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 0],"float64"), )

[cuda error] paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.335799 test begin: paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 0],"float64"), rtol=-3.0, atol=-2.0, )

[cuda error] paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 0],"float64"), rtol=-3.0, atol=-2.0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:29.336787 test begin: paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 3],"float64"), )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 3],"float64"), ) 
 (PreconditionNotMet) The value at dim 3 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.337714 test begin: paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 0],"float64"), y=Tensor([2, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (PreconditionNotMet) The value at dim 3 of Input(Input) is not equal to the Input(Other): 0 != 3.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:0 != other_dim[i]:3.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.338678 test begin: paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.339671 test begin: paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([0, 2, 2, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.340906 test begin: paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.342156 test begin: paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), rtol=-3.0, atol=-2.0, )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 0, 2, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (PreconditionNotMet) The value at dim 1 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.343249 test begin: paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.344313 test begin: paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), rtol=-3.0, atol=-2.0, )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 2, 0, 3],"float64"), rtol=-3.0, atol=-2.0, ) 
 (PreconditionNotMet) The value at dim 2 of Input(Input) is not equal to the Input(Other): 2 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:2 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.345565 test begin: paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 2, 2, 0],"float64"), )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 2, 2, 0],"float64"), ) 
 (PreconditionNotMet) The value at dim 3 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.346640 test begin: paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 2, 2, 0],"float64"), rtol=-3.0, atol=-2.0, )

[paddle error] paddle.allclose(x=Tensor([2, 2, 2, 3],"float64"), y=Tensor([2, 2, 2, 0],"float64"), rtol=-3.0, atol=-2.0, ) 
 (PreconditionNotMet) The value at dim 3 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.347704 test begin: paddle.allclose(x=Tensor([3],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=True, )

[paddle error] paddle.allclose(x=Tensor([3],"float64"), y=Tensor([0],"float64"), rtol=1e-06, atol=0.001, equal_nan=True, ) 
 (PreconditionNotMet) The value at dim 0 of Input(Input) is not equal to the Input(Other): 3 != 0.
  [Hint: Expected input_dim[i] == other_dim[i], but received input_dim[i]:3 != other_dim[i]:0.] (at ../paddle/phi/infermeta/binary.cc:66)

2025-03-03 17:02:29.348732 test begin: paddle.argmax(Tensor([0, 10],"float32"), axis=-1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([0, 10],"float32"), axis=-1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.349386 test begin: paddle.argmax(Tensor([0, 2, 1024, 1024],"float32"), axis=1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([0, 2, 1024, 1024],"float32"), axis=1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.349885 test begin: paddle.argmax(Tensor([0, 2, 496, 512],"float32"), axis=1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([0, 2, 496, 512],"float32"), axis=1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.350394 test begin: paddle.argmax(Tensor([1, 0, 1024, 1024],"float32"), axis=1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([1, 0, 1024, 1024],"float32"), axis=1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.350928 test begin: paddle.argmax(Tensor([1, 0, 496, 512],"float32"), axis=1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([1, 0, 496, 512],"float32"), axis=1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.351369 test begin: paddle.argmax(Tensor([1, 2, 0, 1024],"float32"), axis=1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([1, 2, 0, 1024],"float32"), axis=1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.351845 test begin: paddle.argmax(Tensor([1, 2, 0, 512],"float32"), axis=1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([1, 2, 0, 512],"float32"), axis=1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.352293 test begin: paddle.argmax(Tensor([1, 2, 1024, 0],"float32"), axis=1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([1, 2, 1024, 0],"float32"), axis=1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.352732 test begin: paddle.argmax(Tensor([1, 2, 496, 0],"float32"), axis=1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([1, 2, 496, 0],"float32"), axis=1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.353214 test begin: paddle.argmax(Tensor([10, 0],"float32"), axis=-1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmax(Tensor([10, 0],"float32"), axis=-1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.353656 test begin: paddle.argmax(x=Tensor([0, 1],"int64"), keepdim=None, )

[paddle error] paddle.argmax(x=Tensor([0, 1],"int64"), keepdim=None, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.354142 test begin: paddle.argmax(x=Tensor([0, 3, 2, 1],"float64"), axis=-1, dtype="int64", )

[paddle error] paddle.argmax(x=Tensor([0, 3, 2, 1],"float64"), axis=-1, dtype="int64", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.354634 test begin: paddle.argmax(x=Tensor([0, 3, 5],"float64"), axis=2, dtype=type(numpy.int64), )

[paddle error] paddle.argmax(x=Tensor([0, 3, 5],"float64"), axis=2, dtype=type(numpy.int64), ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.355714 test begin: paddle.argmax(x=Tensor([0, 3],"float64"), axis=0, dtype=type(numpy.int32), keepdim=False, )

[paddle error] paddle.argmax(x=Tensor([0, 3],"float64"), axis=0, dtype=type(numpy.int32), keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.356745 test begin: paddle.argmax(x=Tensor([0, 3],"int64"), dtype="int32", )

[paddle error] paddle.argmax(x=Tensor([0, 3],"int64"), dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.357359 test begin: paddle.argmax(x=Tensor([3, 0, 2, 1],"float64"), axis=-1, dtype="int64", )

[paddle error] paddle.argmax(x=Tensor([3, 0, 2, 1],"float64"), axis=-1, dtype="int64", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.357828 test begin: paddle.argmax(x=Tensor([3, 0, 5],"float64"), axis=2, dtype=type(numpy.int64), )

[paddle error] paddle.argmax(x=Tensor([3, 0, 5],"float64"), axis=2, dtype=type(numpy.int64), ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.358315 test begin: paddle.argmax(x=Tensor([3, 0],"float64"), axis=0, dtype=type(numpy.int32), keepdim=False, )

[paddle error] paddle.argmax(x=Tensor([3, 0],"float64"), axis=0, dtype=type(numpy.int32), keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.358765 test begin: paddle.argmax(x=Tensor([3, 0],"int64"), dtype="int32", )

[paddle error] paddle.argmax(x=Tensor([3, 0],"int64"), dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.359204 test begin: paddle.argmax(x=Tensor([3, 0],"int64"), keepdim=None, )

[paddle error] paddle.argmax(x=Tensor([3, 0],"int64"), keepdim=None, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.359633 test begin: paddle.argmax(x=Tensor([3, 3, 0, 1],"float64"), axis=-1, dtype="int64", )

[paddle error] paddle.argmax(x=Tensor([3, 3, 0, 1],"float64"), axis=-1, dtype="int64", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.360065 test begin: paddle.argmax(x=Tensor([3, 3, 0],"float64"), axis=2, dtype=type(numpy.int64), )

[paddle error] paddle.argmax(x=Tensor([3, 3, 0],"float64"), axis=2, dtype=type(numpy.int64), ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.360477 test begin: paddle.argmax(x=Tensor([3, 3, 2, 0],"float64"), axis=-1, dtype="int64", )

[paddle error] paddle.argmax(x=Tensor([3, 3, 2, 0],"float64"), axis=-1, dtype="int64", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.360973 test begin: paddle.argmin(Tensor([0, 10],"float32"), axis=-1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmin(Tensor([0, 10],"float32"), axis=-1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.361511 test begin: paddle.argmin(Tensor([10, 0],"float32"), axis=-1, keepdim=True, dtype="int32", )

[paddle error] paddle.argmin(Tensor([10, 0],"float32"), axis=-1, keepdim=True, dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.361977 test begin: paddle.argmin(x=Tensor([0, 1],"int64"), keepdim=None, )

[paddle error] paddle.argmin(x=Tensor([0, 1],"int64"), keepdim=None, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.362445 test begin: paddle.argmin(x=Tensor([0, 3, 2, 1],"float64"), axis=-1, dtype="int64", )

[paddle error] paddle.argmin(x=Tensor([0, 3, 2, 1],"float64"), axis=-1, dtype="int64", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.362894 test begin: paddle.argmin(x=Tensor([0, 3, 5],"float64"), axis=2, dtype=type(numpy.int64), )

[paddle error] paddle.argmin(x=Tensor([0, 3, 5],"float64"), axis=2, dtype=type(numpy.int64), ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.363333 test begin: paddle.argmin(x=Tensor([0, 3],"float64"), axis=0, dtype=type(numpy.int32), keepdim=False, )

[paddle error] paddle.argmin(x=Tensor([0, 3],"float64"), axis=0, dtype=type(numpy.int32), keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.363839 test begin: paddle.argmin(x=Tensor([0, 3],"int64"), dtype="int32", )

[paddle error] paddle.argmin(x=Tensor([0, 3],"int64"), dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.364342 test begin: paddle.argmin(x=Tensor([3, 0, 2, 1],"float64"), axis=-1, dtype="int64", )

[paddle error] paddle.argmin(x=Tensor([3, 0, 2, 1],"float64"), axis=-1, dtype="int64", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.364835 test begin: paddle.argmin(x=Tensor([3, 0, 5],"float64"), axis=2, dtype=type(numpy.int64), )

[paddle error] paddle.argmin(x=Tensor([3, 0, 5],"float64"), axis=2, dtype=type(numpy.int64), ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.366148 test begin: paddle.argmin(x=Tensor([3, 0],"float64"), axis=0, dtype=type(numpy.int32), keepdim=False, )

[paddle error] paddle.argmin(x=Tensor([3, 0],"float64"), axis=0, dtype=type(numpy.int32), keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.367012 test begin: paddle.argmin(x=Tensor([3, 0],"int64"), dtype="int32", )

[paddle error] paddle.argmin(x=Tensor([3, 0],"int64"), dtype="int32", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.367607 test begin: paddle.argmin(x=Tensor([3, 0],"int64"), keepdim=None, )

[paddle error] paddle.argmin(x=Tensor([3, 0],"int64"), keepdim=None, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.368306 test begin: paddle.argmin(x=Tensor([3, 3, 0, 1],"float64"), axis=-1, dtype="int64", )

[paddle error] paddle.argmin(x=Tensor([3, 3, 0, 1],"float64"), axis=-1, dtype="int64", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.368925 test begin: paddle.argmin(x=Tensor([3, 3, 0],"float64"), axis=2, dtype=type(numpy.int64), )

[paddle error] paddle.argmin(x=Tensor([3, 3, 0],"float64"), axis=2, dtype=type(numpy.int64), ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.369491 test begin: paddle.argmin(x=Tensor([3, 3, 2, 0],"float64"), axis=-1, dtype="int64", )

[paddle error] paddle.argmin(x=Tensor([3, 3, 2, 0],"float64"), axis=-1, dtype="int64", ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-03 17:02:29.370107 test begin: paddle.as_strided(Tensor([0, 32],"float16"), shape=tuple(3,), stride=tuple(1,), )

W0303 17:02:29.371632  3820 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([0, 32],"float16"), shape=tuple(3,), stride=tuple(1,), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 6, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):6 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 17:02:29.371982 test begin: paddle.as_strided(Tensor([0, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )

W0303 17:02:29.373201  3823 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([0, 32],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1299)

2025-03-03 17:02:29.373469 test begin: paddle.as_strided(Tensor([0, 32],"float32"), shape=tuple(3,), stride=tuple(1,), )

W0303 17:02:29.374562  3825 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([0, 32],"float32"), shape=tuple(3,), stride=tuple(1,), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 12, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):12 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 17:02:29.374838 test begin: paddle.as_strided(Tensor([0, 32],"float32"), shape=tuple(3,4,), stride=tuple(32,1,), )

W0303 17:02:29.375900  3828 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([0, 32],"float32"), shape=tuple(3,4,), stride=tuple(32,1,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1299)

2025-03-03 17:02:29.376300 test begin: paddle.as_strided(Tensor([0, 4, 6],"float32"), list[8,6,], list[6,1,], )

W0303 17:02:29.377291  3829 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([0, 4, 6],"float32"), list[8,6,], list[6,1,], ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 192, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):192 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 17:02:29.377539 test begin: paddle.as_strided(Tensor([2, 0, 6],"float32"), list[8,6,], list[6,1,], )

W0303 17:02:29.378567  3831 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([2, 0, 6],"float32"), list[8,6,], list[6,1,], ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 192, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):192 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 17:02:29.378833 test begin: paddle.as_strided(Tensor([2, 4, 0],"float32"), list[8,6,], list[6,1,], )

W0303 17:02:29.379815  3833 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([2, 4, 0],"float32"), list[8,6,], list[6,1,], ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 192, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):192 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 17:02:29.380069 test begin: paddle.as_strided(Tensor([32, 0],"float16"), shape=tuple(3,), stride=tuple(1,), )

W0303 17:02:29.381089  3835 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([32, 0],"float16"), shape=tuple(3,), stride=tuple(1,), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 6, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):6 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 17:02:29.381400 test begin: paddle.as_strided(Tensor([32, 0],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), )

W0303 17:02:29.382391  3837 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([32, 0],"float16"), shape=tuple(3,4,), stride=tuple(32,1,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1299)

2025-03-03 17:02:29.382652 test begin: paddle.as_strided(Tensor([32, 0],"float32"), shape=tuple(3,), stride=tuple(1,), )

W0303 17:02:29.383862  3840 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([32, 0],"float32"), shape=tuple(3,), stride=tuple(1,), ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 12, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):12 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 17:02:29.384096 test begin: paddle.as_strided(Tensor([32, 0],"float32"), shape=tuple(3,4,), stride=tuple(32,1,), )

W0303 17:02:29.385066  3842 backward.cc:437] While running Node (AsStridedGradNode) raises an EnforceNotMet exception
[paddle error] paddle.as_strided(Tensor([32, 0],"float32"), shape=tuple(3,4,), stride=tuple(32,1,), ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1299)

2025-03-03 17:02:29.385340 test begin: paddle.assign(0.0, Tensor([0],"float32"), )

[Pass] paddle.assign(0.0, Tensor([0],"float32"), )
2025-03-03 17:02:29.386550 test begin: paddle.assign(0.005, Tensor([0],"float32"), )

[Pass] paddle.assign(0.005, Tensor([0],"float32"), )
2025-03-03 17:02:29.387429 test begin: paddle.assign(0.04380856, Tensor([0],"float32"), )

[Pass] paddle.assign(0.04380856, Tensor([0],"float32"), )
2025-03-03 17:02:29.388359 test begin: paddle.assign(0.05, Tensor([0],"float32"), )

[Pass] paddle.assign(0.05, Tensor([0],"float32"), )
2025-03-03 17:02:29.389289 test begin: paddle.assign(1.0, Tensor([0],"float32"), )

[Pass] paddle.assign(1.0, Tensor([0],"float32"), )
2025-03-03 17:02:29.390217 test begin: paddle.assign(Tensor([0, 1, 1, 11],"float32"), Tensor([0, 1, 1, 11],"float32"), )

[Pass] paddle.assign(Tensor([0, 1, 1, 11],"float32"), Tensor([0, 1, 1, 11],"float32"), )
2025-03-03 17:02:29.391328 test begin: paddle.assign(Tensor([0, 1, 1, 11],"float32"), Tensor([1, 1, 1, 11],"float32"), )

[Pass] paddle.assign(Tensor([0, 1, 1, 11],"float32"), Tensor([1, 1, 1, 11],"float32"), )
2025-03-03 17:02:29.392358 test begin: paddle.assign(Tensor([0, 1, 1, 7],"float32"), Tensor([0, 1, 1, 7],"float32"), )

[Pass] paddle.assign(Tensor([0, 1, 1, 7],"float32"), Tensor([0, 1, 1, 7],"float32"), )
2025-03-03 17:02:29.393288 test begin: paddle.assign(Tensor([0, 1, 1, 7],"float32"), Tensor([1, 1, 1, 7],"float32"), )

[Pass] paddle.assign(Tensor([0, 1, 1, 7],"float32"), Tensor([1, 1, 1, 7],"float32"), )
2025-03-03 17:02:29.394304 test begin: paddle.assign(Tensor([0, 1, 1],"float32"), )

[Pass] paddle.assign(Tensor([0, 1, 1],"float32"), )
2025-03-03 17:02:29.395401 test begin: paddle.assign(Tensor([0, 1, 768],"float32"), Tensor([0, 1, 768],"float32"), )

[Pass] paddle.assign(Tensor([0, 1, 768],"float32"), Tensor([0, 1, 768],"float32"), )
2025-03-03 17:02:29.396319 test begin: paddle.assign(Tensor([0, 1, 768],"float32"), Tensor([1, 1, 768],"float32"), )

[Pass] paddle.assign(Tensor([0, 1, 768],"float32"), Tensor([1, 1, 768],"float32"), )
2025-03-03 17:02:29.397396 test begin: paddle.assign(Tensor([0, 10, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([0, 10, 3, 3],"float32"), )
2025-03-03 17:02:29.398544 test begin: paddle.assign(Tensor([0, 1024],"float32"), output=Tensor([0, 1024],"float32"), )

[Pass] paddle.assign(Tensor([0, 1024],"float32"), output=Tensor([0, 1024],"float32"), )
2025-03-03 17:02:29.399647 test begin: paddle.assign(Tensor([0, 1024],"float32"), output=Tensor([1024, 1024],"float32"), )

[Pass] paddle.assign(Tensor([0, 1024],"float32"), output=Tensor([1024, 1024],"float32"), )
2025-03-03 17:02:29.411790 test begin: paddle.assign(Tensor([0, 102],"float32"), output=Tensor([0, 102],"float32"), )

[Pass] paddle.assign(Tensor([0, 102],"float32"), output=Tensor([0, 102],"float32"), )
2025-03-03 17:02:29.412991 test begin: paddle.assign(Tensor([0, 102],"float32"), output=Tensor([1024, 102],"float32"), )

[Pass] paddle.assign(Tensor([0, 102],"float32"), output=Tensor([1024, 102],"float32"), )
2025-03-03 17:02:29.415204 test begin: paddle.assign(Tensor([0, 13],"int64"), )

[Pass] paddle.assign(Tensor([0, 13],"int64"), )
2025-03-03 17:02:29.416368 test begin: paddle.assign(Tensor([0, 197, 768],"float32"), Tensor([0, 197, 768],"float32"), )

[Pass] paddle.assign(Tensor([0, 197, 768],"float32"), Tensor([0, 197, 768],"float32"), )
2025-03-03 17:02:29.417359 test begin: paddle.assign(Tensor([0, 197, 768],"float32"), Tensor([1, 197, 768],"float32"), )

[Pass] paddle.assign(Tensor([0, 197, 768],"float32"), Tensor([1, 197, 768],"float32"), )
2025-03-03 17:02:29.419887 test begin: paddle.assign(Tensor([0, 1],"float32"), )

[Pass] paddle.assign(Tensor([0, 1],"float32"), )
2025-03-03 17:02:29.421918 test begin: paddle.assign(Tensor([0, 20],"float32"), Tensor([0, 20],"float32"), )

[Pass] paddle.assign(Tensor([0, 20],"float32"), Tensor([0, 20],"float32"), )
2025-03-03 17:02:29.423541 test begin: paddle.assign(Tensor([0, 20],"float32"), Tensor([10, 20],"float32"), )

[Pass] paddle.assign(Tensor([0, 20],"float32"), Tensor([10, 20],"float32"), )
2025-03-03 17:02:29.425251 test begin: paddle.assign(Tensor([0, 3, 14, 14],"float32"), output=Tensor([0, 3, 14, 14],"float32"), )

[Pass] paddle.assign(Tensor([0, 3, 14, 14],"float32"), output=Tensor([0, 3, 14, 14],"float32"), )
2025-03-03 17:02:29.426900 test begin: paddle.assign(Tensor([0, 3, 14, 14],"float32"), output=Tensor([1024, 3, 14, 14],"float32"), )

[Pass] paddle.assign(Tensor([0, 3, 14, 14],"float32"), output=Tensor([1024, 3, 14, 14],"float32"), )
2025-03-03 17:02:29.436063 test begin: paddle.assign(Tensor([0, 3, 16, 16],"float32"), output=Tensor([0, 3, 16, 16],"float32"), )

[Pass] paddle.assign(Tensor([0, 3, 16, 16],"float32"), output=Tensor([0, 3, 16, 16],"float32"), )
2025-03-03 17:02:29.437875 test begin: paddle.assign(Tensor([0, 3, 16, 16],"float32"), output=Tensor([768, 3, 16, 16],"float32"), )

[Pass] paddle.assign(Tensor([0, 3, 16, 16],"float32"), output=Tensor([768, 3, 16, 16],"float32"), )
2025-03-03 17:02:29.449781 test begin: paddle.assign(Tensor([0, 3, 3, 3, 3],"float32"), Tensor([0, 3, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([0, 3, 3, 3, 3],"float32"), Tensor([0, 3, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.451132 test begin: paddle.assign(Tensor([0, 3, 3, 3, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([0, 3, 3, 3, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.456522 test begin: paddle.assign(Tensor([0, 3, 3, 5],"float32"), )

[Pass] paddle.assign(Tensor([0, 3, 3, 5],"float32"), )
2025-03-03 17:02:29.464448 test begin: paddle.assign(Tensor([0, 4, 4, 4, 4],"float32"), Tensor([0, 4, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([0, 4, 4, 4, 4],"float32"), Tensor([0, 4, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.468324 test begin: paddle.assign(Tensor([0, 4, 4, 4, 4],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([0, 4, 4, 4, 4],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.473680 test begin: paddle.assign(Tensor([0, 8],"int64"), Tensor([0, 8],"int64"), )

[Pass] paddle.assign(Tensor([0, 8],"int64"), Tensor([0, 8],"int64"), )
2025-03-03 17:02:29.476313 test begin: paddle.assign(Tensor([0, 8],"int64"), Tensor([13, 8],"int64"), )

[Pass] paddle.assign(Tensor([0, 8],"int64"), Tensor([13, 8],"int64"), )
2025-03-03 17:02:29.477772 test begin: paddle.assign(Tensor([0],"bfloat16"), Tensor([0],"bfloat16"), )

[Pass] paddle.assign(Tensor([0],"bfloat16"), Tensor([0],"bfloat16"), )
2025-03-03 17:02:29.479061 test begin: paddle.assign(Tensor([0],"bfloat16"), Tensor([100],"bfloat16"), )

[Pass] paddle.assign(Tensor([0],"bfloat16"), Tensor([100],"bfloat16"), )
2025-03-03 17:02:29.480516 test begin: paddle.assign(Tensor([0],"float32"), )

[Pass] paddle.assign(Tensor([0],"float32"), )
2025-03-03 17:02:29.482639 test begin: paddle.assign(Tensor([0],"float32"), Tensor([0],"float32"), )

[Pass] paddle.assign(Tensor([0],"float32"), Tensor([0],"float32"), )
2025-03-03 17:02:29.483465 test begin: paddle.assign(Tensor([0],"float32"), Tensor([1000],"float32"), )

[Pass] paddle.assign(Tensor([0],"float32"), Tensor([1000],"float32"), )
2025-03-03 17:02:29.484952 test begin: paddle.assign(Tensor([1, 0, 1, 11],"float32"), Tensor([1, 0, 1, 11],"float32"), )

[Pass] paddle.assign(Tensor([1, 0, 1, 11],"float32"), Tensor([1, 0, 1, 11],"float32"), )
2025-03-03 17:02:29.486172 test begin: paddle.assign(Tensor([1, 0, 1, 11],"float32"), Tensor([1, 1, 1, 11],"float32"), )

[Pass] paddle.assign(Tensor([1, 0, 1, 11],"float32"), Tensor([1, 1, 1, 11],"float32"), )
2025-03-03 17:02:29.487556 test begin: paddle.assign(Tensor([1, 0, 1, 7],"float32"), Tensor([1, 0, 1, 7],"float32"), )

[Pass] paddle.assign(Tensor([1, 0, 1, 7],"float32"), Tensor([1, 0, 1, 7],"float32"), )
2025-03-03 17:02:29.488674 test begin: paddle.assign(Tensor([1, 0, 1, 7],"float32"), Tensor([1, 1, 1, 7],"float32"), )

[Pass] paddle.assign(Tensor([1, 0, 1, 7],"float32"), Tensor([1, 1, 1, 7],"float32"), )
2025-03-03 17:02:29.489861 test begin: paddle.assign(Tensor([1, 0, 3, 5],"float32"), )

[Pass] paddle.assign(Tensor([1, 0, 3, 5],"float32"), )
2025-03-03 17:02:29.491120 test begin: paddle.assign(Tensor([1, 0, 768],"float32"), Tensor([1, 0, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 0, 768],"float32"), Tensor([1, 0, 768],"float32"), )
2025-03-03 17:02:29.492131 test begin: paddle.assign(Tensor([1, 0, 768],"float32"), Tensor([1, 1, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 0, 768],"float32"), Tensor([1, 1, 768],"float32"), )
2025-03-03 17:02:29.493392 test begin: paddle.assign(Tensor([1, 0, 768],"float32"), Tensor([1, 197, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 0, 768],"float32"), Tensor([1, 197, 768],"float32"), )
2025-03-03 17:02:29.496958 test begin: paddle.assign(Tensor([1, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 0],"float32"), )
2025-03-03 17:02:29.498359 test begin: paddle.assign(Tensor([1, 0],"int64"), )

[Pass] paddle.assign(Tensor([1, 0],"int64"), )
2025-03-03 17:02:29.499884 test begin: paddle.assign(Tensor([1, 1, 0, 11],"float32"), Tensor([1, 1, 0, 11],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 0, 11],"float32"), Tensor([1, 1, 0, 11],"float32"), )
2025-03-03 17:02:29.501386 test begin: paddle.assign(Tensor([1, 1, 0, 11],"float32"), Tensor([1, 1, 1, 11],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 0, 11],"float32"), Tensor([1, 1, 1, 11],"float32"), )
2025-03-03 17:02:29.502767 test begin: paddle.assign(Tensor([1, 1, 0, 7],"float32"), Tensor([1, 1, 0, 7],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 0, 7],"float32"), Tensor([1, 1, 0, 7],"float32"), )
2025-03-03 17:02:29.503757 test begin: paddle.assign(Tensor([1, 1, 0, 7],"float32"), Tensor([1, 1, 1, 7],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 0, 7],"float32"), Tensor([1, 1, 1, 7],"float32"), )
2025-03-03 17:02:29.504891 test begin: paddle.assign(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 0],"float32"), )
2025-03-03 17:02:29.505865 test begin: paddle.assign(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 768],"float32"), )
2025-03-03 17:02:29.506885 test begin: paddle.assign(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 0],"float32"), )
2025-03-03 17:02:29.507784 test begin: paddle.assign(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 11],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 11],"float32"), )
2025-03-03 17:02:29.508842 test begin: paddle.assign(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 7],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 7],"float32"), )
2025-03-03 17:02:29.509973 test begin: paddle.assign(Tensor([1, 1, 1, 11],"float32"), Tensor([0, 1, 1, 11],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 11],"float32"), Tensor([0, 1, 1, 11],"float32"), )
2025-03-03 17:02:29.511209 test begin: paddle.assign(Tensor([1, 1, 1, 11],"float32"), Tensor([1, 0, 1, 11],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 11],"float32"), Tensor([1, 0, 1, 11],"float32"), )
2025-03-03 17:02:29.512472 test begin: paddle.assign(Tensor([1, 1, 1, 11],"float32"), Tensor([1, 1, 0, 11],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 11],"float32"), Tensor([1, 1, 0, 11],"float32"), )
2025-03-03 17:02:29.513663 test begin: paddle.assign(Tensor([1, 1, 1, 11],"float32"), Tensor([1, 1, 1, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 11],"float32"), Tensor([1, 1, 1, 0],"float32"), )
2025-03-03 17:02:29.514916 test begin: paddle.assign(Tensor([1, 1, 1, 7],"float32"), Tensor([0, 1, 1, 7],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 7],"float32"), Tensor([0, 1, 1, 7],"float32"), )
2025-03-03 17:02:29.516298 test begin: paddle.assign(Tensor([1, 1, 1, 7],"float32"), Tensor([1, 0, 1, 7],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 7],"float32"), Tensor([1, 0, 1, 7],"float32"), )
2025-03-03 17:02:29.517616 test begin: paddle.assign(Tensor([1, 1, 1, 7],"float32"), Tensor([1, 1, 0, 7],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 7],"float32"), Tensor([1, 1, 0, 7],"float32"), )
2025-03-03 17:02:29.518817 test begin: paddle.assign(Tensor([1, 1, 1, 7],"float32"), Tensor([1, 1, 1, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 1, 7],"float32"), Tensor([1, 1, 1, 0],"float32"), )
2025-03-03 17:02:29.519911 test begin: paddle.assign(Tensor([1, 1, 768],"float32"), Tensor([0, 1, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 768],"float32"), Tensor([0, 1, 768],"float32"), )
2025-03-03 17:02:29.521562 test begin: paddle.assign(Tensor([1, 1, 768],"float32"), Tensor([1, 0, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 768],"float32"), Tensor([1, 0, 768],"float32"), )
2025-03-03 17:02:29.522925 test begin: paddle.assign(Tensor([1, 1, 768],"float32"), Tensor([1, 1, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 1, 768],"float32"), Tensor([1, 1, 0],"float32"), )
2025-03-03 17:02:29.524516 test begin: paddle.assign(Tensor([1, 197, 0],"float32"), Tensor([1, 197, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 197, 0],"float32"), Tensor([1, 197, 0],"float32"), )
2025-03-03 17:02:29.525566 test begin: paddle.assign(Tensor([1, 197, 0],"float32"), Tensor([1, 197, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 197, 0],"float32"), Tensor([1, 197, 768],"float32"), )
2025-03-03 17:02:29.528147 test begin: paddle.assign(Tensor([1, 197, 768],"float32"), Tensor([0, 197, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 197, 768],"float32"), Tensor([0, 197, 768],"float32"), )
2025-03-03 17:02:29.530756 test begin: paddle.assign(Tensor([1, 197, 768],"float32"), Tensor([1, 0, 768],"float32"), )

[Pass] paddle.assign(Tensor([1, 197, 768],"float32"), Tensor([1, 0, 768],"float32"), )
2025-03-03 17:02:29.533194 test begin: paddle.assign(Tensor([1, 197, 768],"float32"), Tensor([1, 197, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 197, 768],"float32"), Tensor([1, 197, 0],"float32"), )
2025-03-03 17:02:29.535479 test begin: paddle.assign(Tensor([1, 3, 0, 5],"float32"), )

[Pass] paddle.assign(Tensor([1, 3, 0, 5],"float32"), )
2025-03-03 17:02:29.536643 test begin: paddle.assign(Tensor([1, 3, 3, 0],"float32"), )

[Pass] paddle.assign(Tensor([1, 3, 3, 0],"float32"), )
2025-03-03 17:02:29.538610 test begin: paddle.assign(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )

[Pass] paddle.assign(Tensor([10, 0],"float32"), Tensor([10, 0],"float32"), )
2025-03-03 17:02:29.541854 test begin: paddle.assign(Tensor([10, 0],"float32"), Tensor([10, 20],"float32"), )

[Pass] paddle.assign(Tensor([10, 0],"float32"), Tensor([10, 20],"float32"), )
2025-03-03 17:02:29.545776 test begin: paddle.assign(Tensor([10, 20],"float32"), Tensor([0, 20],"float32"), )

[Pass] paddle.assign(Tensor([10, 20],"float32"), Tensor([0, 20],"float32"), )
2025-03-03 17:02:29.547197 test begin: paddle.assign(Tensor([10, 20],"float32"), Tensor([10, 0],"float32"), )

[Pass] paddle.assign(Tensor([10, 20],"float32"), Tensor([10, 0],"float32"), )
2025-03-03 17:02:29.548878 test begin: paddle.assign(Tensor([1000],"float32"), Tensor([0],"float32"), )

[Pass] paddle.assign(Tensor([1000],"float32"), Tensor([0],"float32"), )
2025-03-03 17:02:29.552709 test begin: paddle.assign(Tensor([100],"bfloat16"), Tensor([0],"bfloat16"), )

[Pass] paddle.assign(Tensor([100],"bfloat16"), Tensor([0],"bfloat16"), )
2025-03-03 17:02:29.556570 test begin: paddle.assign(Tensor([1024, 0, 14, 14],"float32"), output=Tensor([1024, 0, 14, 14],"float32"), )

[Pass] paddle.assign(Tensor([1024, 0, 14, 14],"float32"), output=Tensor([1024, 0, 14, 14],"float32"), )
2025-03-03 17:02:29.558231 test begin: paddle.assign(Tensor([1024, 0, 14, 14],"float32"), output=Tensor([1024, 3, 14, 14],"float32"), )

[Pass] paddle.assign(Tensor([1024, 0, 14, 14],"float32"), output=Tensor([1024, 3, 14, 14],"float32"), )
2025-03-03 17:02:29.567867 test begin: paddle.assign(Tensor([1024, 0],"float32"), output=Tensor([1024, 0],"float32"), )

[Pass] paddle.assign(Tensor([1024, 0],"float32"), output=Tensor([1024, 0],"float32"), )
2025-03-03 17:02:29.570902 test begin: paddle.assign(Tensor([1024, 0],"float32"), output=Tensor([1024, 1024],"float32"), )

[Pass] paddle.assign(Tensor([1024, 0],"float32"), output=Tensor([1024, 1024],"float32"), )
2025-03-03 17:02:29.585280 test begin: paddle.assign(Tensor([1024, 0],"float32"), output=Tensor([1024, 102],"float32"), )

[Pass] paddle.assign(Tensor([1024, 0],"float32"), output=Tensor([1024, 102],"float32"), )
2025-03-03 17:02:29.600418 test begin: paddle.assign(Tensor([1024, 1024],"float32"), output=Tensor([0, 1024],"float32"), )

[Pass] paddle.assign(Tensor([1024, 1024],"float32"), output=Tensor([0, 1024],"float32"), )
2025-03-03 17:02:29.651667 test begin: paddle.assign(Tensor([1024, 1024],"float32"), output=Tensor([1024, 0],"float32"), )

[Pass] paddle.assign(Tensor([1024, 1024],"float32"), output=Tensor([1024, 0],"float32"), )
2025-03-03 17:02:29.677098 test begin: paddle.assign(Tensor([1024, 102],"float32"), output=Tensor([0, 102],"float32"), )

[Pass] paddle.assign(Tensor([1024, 102],"float32"), output=Tensor([0, 102],"float32"), )
2025-03-03 17:02:29.680941 test begin: paddle.assign(Tensor([1024, 102],"float32"), output=Tensor([1024, 0],"float32"), )

[Pass] paddle.assign(Tensor([1024, 102],"float32"), output=Tensor([1024, 0],"float32"), )
2025-03-03 17:02:29.684385 test begin: paddle.assign(Tensor([1024, 3, 0, 14],"float32"), output=Tensor([1024, 3, 0, 14],"float32"), )

[Pass] paddle.assign(Tensor([1024, 3, 0, 14],"float32"), output=Tensor([1024, 3, 0, 14],"float32"), )
2025-03-03 17:02:29.685555 test begin: paddle.assign(Tensor([1024, 3, 0, 14],"float32"), output=Tensor([1024, 3, 14, 14],"float32"), )

[Pass] paddle.assign(Tensor([1024, 3, 0, 14],"float32"), output=Tensor([1024, 3, 14, 14],"float32"), )
2025-03-03 17:02:29.693346 test begin: paddle.assign(Tensor([1024, 3, 14, 0],"float32"), output=Tensor([1024, 3, 14, 0],"float32"), )

[Pass] paddle.assign(Tensor([1024, 3, 14, 0],"float32"), output=Tensor([1024, 3, 14, 0],"float32"), )
2025-03-03 17:02:29.694559 test begin: paddle.assign(Tensor([1024, 3, 14, 0],"float32"), output=Tensor([1024, 3, 14, 14],"float32"), )

[Pass] paddle.assign(Tensor([1024, 3, 14, 0],"float32"), output=Tensor([1024, 3, 14, 14],"float32"), )
2025-03-03 17:02:29.701852 test begin: paddle.assign(Tensor([1024, 3, 14, 14],"float32"), output=Tensor([0, 3, 14, 14],"float32"), )

[Pass] paddle.assign(Tensor([1024, 3, 14, 14],"float32"), output=Tensor([0, 3, 14, 14],"float32"), )
2025-03-03 17:02:29.716856 test begin: paddle.assign(Tensor([1024, 3, 14, 14],"float32"), output=Tensor([1024, 0, 14, 14],"float32"), )

[Pass] paddle.assign(Tensor([1024, 3, 14, 14],"float32"), output=Tensor([1024, 0, 14, 14],"float32"), )
2025-03-03 17:02:29.744652 test begin: paddle.assign(Tensor([1024, 3, 14, 14],"float32"), output=Tensor([1024, 3, 0, 14],"float32"), )

[Pass] paddle.assign(Tensor([1024, 3, 14, 14],"float32"), output=Tensor([1024, 3, 0, 14],"float32"), )
2025-03-03 17:02:29.762326 test begin: paddle.assign(Tensor([1024, 3, 14, 14],"float32"), output=Tensor([1024, 3, 14, 0],"float32"), )

[Pass] paddle.assign(Tensor([1024, 3, 14, 14],"float32"), output=Tensor([1024, 3, 14, 0],"float32"), )
2025-03-03 17:02:29.779197 test begin: paddle.assign(Tensor([13, 0, 1],"float32"), )

[Pass] paddle.assign(Tensor([13, 0, 1],"float32"), )
2025-03-03 17:02:29.780864 test begin: paddle.assign(Tensor([13, 0],"int64"), Tensor([13, 0],"int64"), )

[Pass] paddle.assign(Tensor([13, 0],"int64"), Tensor([13, 0],"int64"), )
2025-03-03 17:02:29.782146 test begin: paddle.assign(Tensor([13, 0],"int64"), Tensor([13, 8],"int64"), )

[Pass] paddle.assign(Tensor([13, 0],"int64"), Tensor([13, 8],"int64"), )
2025-03-03 17:02:29.783761 test begin: paddle.assign(Tensor([13, 1, 0],"float32"), )

[Pass] paddle.assign(Tensor([13, 1, 0],"float32"), )
2025-03-03 17:02:29.785337 test begin: paddle.assign(Tensor([13, 8],"int64"), Tensor([0, 8],"int64"), )

[Pass] paddle.assign(Tensor([13, 8],"int64"), Tensor([0, 8],"int64"), )
2025-03-03 17:02:29.787164 test begin: paddle.assign(Tensor([13, 8],"int64"), Tensor([13, 0],"int64"), )

[Pass] paddle.assign(Tensor([13, 8],"int64"), Tensor([13, 0],"int64"), )
2025-03-03 17:02:29.788805 test begin: paddle.assign(Tensor([16, 0, 1],"float32"), )

[Pass] paddle.assign(Tensor([16, 0, 1],"float32"), )
2025-03-03 17:02:29.790394 test begin: paddle.assign(Tensor([16, 1, 0],"float32"), )

[Pass] paddle.assign(Tensor([16, 1, 0],"float32"), )
2025-03-03 17:02:29.791621 test begin: paddle.assign(Tensor([2, 0, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([2, 0, 3, 3],"float32"), )
2025-03-03 17:02:29.793379 test begin: paddle.assign(Tensor([2, 10, 0, 3],"float32"), )

[Pass] paddle.assign(Tensor([2, 10, 0, 3],"float32"), )
2025-03-03 17:02:29.794723 test begin: paddle.assign(Tensor([2, 10, 3, 0],"float32"), )

[Pass] paddle.assign(Tensor([2, 10, 3, 0],"float32"), )
2025-03-03 17:02:29.796054 test begin: paddle.assign(Tensor([5, 0, 3, 3, 3],"float32"), Tensor([5, 0, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 0, 3, 3, 3],"float32"), Tensor([5, 0, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.797948 test begin: paddle.assign(Tensor([5, 0, 3, 3, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 0, 3, 3, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.799223 test begin: paddle.assign(Tensor([5, 0, 4, 4, 4],"float32"), Tensor([5, 0, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 0, 4, 4, 4],"float32"), Tensor([5, 0, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.800205 test begin: paddle.assign(Tensor([5, 0, 4, 4, 4],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 0, 4, 4, 4],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.801536 test begin: paddle.assign(Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), )
2025-03-03 17:02:29.802618 test begin: paddle.assign(Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 0, 3, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.803800 test begin: paddle.assign(Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), )
2025-03-03 17:02:29.805400 test begin: paddle.assign(Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 0, 3],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.806572 test begin: paddle.assign(Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), )
2025-03-03 17:02:29.808308 test begin: paddle.assign(Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 3, 0],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.809968 test begin: paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([0, 3, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([0, 3, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.811389 test begin: paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5, 0, 3, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5, 0, 3, 3, 3],"float32"), )
2025-03-03 17:02:29.812553 test begin: paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5, 3, 0, 3, 3],"float32"), )
2025-03-03 17:02:29.813786 test begin: paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5, 3, 3, 0, 3],"float32"), )
2025-03-03 17:02:29.815715 test begin: paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), )

[Pass] paddle.assign(Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5, 3, 3, 3, 0],"float32"), )
2025-03-03 17:02:29.817165 test begin: paddle.assign(Tensor([5, 4, 0, 4, 4],"float32"), Tensor([5, 4, 0, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 0, 4, 4],"float32"), Tensor([5, 4, 0, 4, 4],"float32"), )
2025-03-03 17:02:29.818610 test begin: paddle.assign(Tensor([5, 4, 0, 4, 4],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 0, 4, 4],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.820393 test begin: paddle.assign(Tensor([5, 4, 4, 0, 4],"float32"), Tensor([5, 4, 4, 0, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 0, 4],"float32"), Tensor([5, 4, 4, 0, 4],"float32"), )
2025-03-03 17:02:29.821741 test begin: paddle.assign(Tensor([5, 4, 4, 0, 4],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 0, 4],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.823430 test begin: paddle.assign(Tensor([5, 4, 4, 4, 0],"float32"), Tensor([5, 4, 4, 4, 0],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 4, 0],"float32"), Tensor([5, 4, 4, 4, 0],"float32"), )
2025-03-03 17:02:29.827499 test begin: paddle.assign(Tensor([5, 4, 4, 4, 0],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 4, 0],"float32"), Tensor([5, 4, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.829183 test begin: paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([0, 4, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([0, 4, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.830557 test begin: paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([5, 0, 4, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([5, 0, 4, 4, 4],"float32"), )
2025-03-03 17:02:29.832287 test begin: paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([5, 4, 0, 4, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([5, 4, 0, 4, 4],"float32"), )
2025-03-03 17:02:29.833465 test begin: paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([5, 4, 4, 0, 4],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([5, 4, 4, 0, 4],"float32"), )
2025-03-03 17:02:29.834838 test begin: paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([5, 4, 4, 4, 0],"float32"), )

[Pass] paddle.assign(Tensor([5, 4, 4, 4, 4],"float32"), Tensor([5, 4, 4, 4, 0],"float32"), )
2025-03-03 17:02:29.836202 test begin: paddle.assign(Tensor([768, 0, 16, 16],"float32"), output=Tensor([768, 0, 16, 16],"float32"), )

[Pass] paddle.assign(Tensor([768, 0, 16, 16],"float32"), output=Tensor([768, 0, 16, 16],"float32"), )
2025-03-03 17:02:29.837400 test begin: paddle.assign(Tensor([768, 0, 16, 16],"float32"), output=Tensor([768, 3, 16, 16],"float32"), )

[Pass] paddle.assign(Tensor([768, 0, 16, 16],"float32"), output=Tensor([768, 3, 16, 16],"float32"), )
2025-03-03 17:02:29.845114 test begin: paddle.assign(Tensor([768, 3, 0, 16],"float32"), output=Tensor([768, 3, 0, 16],"float32"), )

[Pass] paddle.assign(Tensor([768, 3, 0, 16],"float32"), output=Tensor([768, 3, 0, 16],"float32"), )
2025-03-03 17:02:29.847024 test begin: paddle.assign(Tensor([768, 3, 0, 16],"float32"), output=Tensor([768, 3, 16, 16],"float32"), )

[Pass] paddle.assign(Tensor([768, 3, 0, 16],"float32"), output=Tensor([768, 3, 16, 16],"float32"), )
2025-03-03 17:02:29.855395 test begin: paddle.assign(Tensor([768, 3, 16, 0],"float32"), output=Tensor([768, 3, 16, 0],"float32"), )

[Pass] paddle.assign(Tensor([768, 3, 16, 0],"float32"), output=Tensor([768, 3, 16, 0],"float32"), )
2025-03-03 17:02:29.856593 test begin: paddle.assign(Tensor([768, 3, 16, 0],"float32"), output=Tensor([768, 3, 16, 16],"float32"), )

[Pass] paddle.assign(Tensor([768, 3, 16, 0],"float32"), output=Tensor([768, 3, 16, 16],"float32"), )
2025-03-03 17:02:29.864450 test begin: paddle.assign(Tensor([768, 3, 16, 16],"float32"), output=Tensor([0, 3, 16, 16],"float32"), )

[Pass] paddle.assign(Tensor([768, 3, 16, 16],"float32"), output=Tensor([0, 3, 16, 16],"float32"), )
2025-03-03 17:02:29.881886 test begin: paddle.assign(Tensor([768, 3, 16, 16],"float32"), output=Tensor([768, 0, 16, 16],"float32"), )

[Pass] paddle.assign(Tensor([768, 3, 16, 16],"float32"), output=Tensor([768, 0, 16, 16],"float32"), )
2025-03-03 17:02:29.899010 test begin: paddle.assign(Tensor([768, 3, 16, 16],"float32"), output=Tensor([768, 3, 0, 16],"float32"), )

[Pass] paddle.assign(Tensor([768, 3, 16, 16],"float32"), output=Tensor([768, 3, 0, 16],"float32"), )
2025-03-03 17:02:29.914840 test begin: paddle.assign(Tensor([768, 3, 16, 16],"float32"), output=Tensor([768, 3, 16, 0],"float32"), )

[Pass] paddle.assign(Tensor([768, 3, 16, 16],"float32"), output=Tensor([768, 3, 16, 0],"float32"), )
2025-03-03 17:02:29.932316 test begin: paddle.assign(list[Tensor([0, 10],"float64"),], )

[Pass] paddle.assign(list[Tensor([0, 10],"float64"),], )
2025-03-03 17:02:29.933806 test begin: paddle.assign(list[Tensor([100, 0],"float64"),], )

[Pass] paddle.assign(list[Tensor([100, 0],"float64"),], )
2025-03-03 17:02:29.934939 test begin: paddle.atleast_1d(Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:29.935818 test begin: paddle.atleast_1d(Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:29.936806 test begin: paddle.atleast_1d(Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:29.938570 test begin: paddle.atleast_1d(Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:29.940251 test begin: paddle.atleast_1d(Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:29.941571 test begin: paddle.atleast_1d(Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:29.943039 test begin: paddle.atleast_1d(Tensor([0, 1, 1],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1, 1],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:29.944425 test begin: paddle.atleast_1d(Tensor([0, 1, 1],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1, 1],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:29.945618 test begin: paddle.atleast_1d(Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1],"float64"), )
2025-03-03 17:02:29.946566 test begin: paddle.atleast_1d(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), )
2025-03-03 17:02:29.947715 test begin: paddle.atleast_1d(Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:29.949082 test begin: paddle.atleast_1d(Tensor([0, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:29.950256 test begin: paddle.atleast_1d(Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 2],"float64"), )
2025-03-03 17:02:29.951095 test begin: paddle.atleast_1d(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:29.952008 test begin: paddle.atleast_1d(Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:29.953235 test begin: paddle.atleast_1d(Tensor([0, 2],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 2],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:29.954420 test begin: paddle.atleast_1d(Tensor([0, 3, 1],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 3, 1],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:29.956113 test begin: paddle.atleast_1d(Tensor([0, 3, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 3, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:29.957387 test begin: paddle.atleast_1d(Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:29.958343 test begin: paddle.atleast_1d(Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:29.959416 test begin: paddle.atleast_1d(Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:29.960879 test begin: paddle.atleast_1d(Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:29.961995 test begin: paddle.atleast_1d(Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:29.963121 test begin: paddle.atleast_1d(Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:29.964536 test begin: paddle.atleast_1d(Tensor([0],"float32"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0],"float32"), Tensor([0],"float64"), )
2025-03-03 17:02:29.966007 test begin: paddle.atleast_1d(Tensor([0],"float32"), Tensor([2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0],"float32"), Tensor([2],"float64"), )
2025-03-03 17:02:29.967259 test begin: paddle.atleast_1d(Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0],"float64"), )
2025-03-03 17:02:29.968231 test begin: paddle.atleast_1d(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:29.969289 test begin: paddle.atleast_1d(Tensor([0],"float64"), Tensor([1],"float64"), Tensor([1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0],"float64"), Tensor([1],"float64"), Tensor([1],"float64"), )
2025-03-03 17:02:29.970663 test begin: paddle.atleast_1d(Tensor([0],"float64"), Tensor([5],"float64"), Tensor([5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([0],"float64"), Tensor([5],"float64"), Tensor([5],"float64"), )
2025-03-03 17:02:29.971922 test begin: paddle.atleast_1d(Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:29.972853 test begin: paddle.atleast_1d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:29.973884 test begin: paddle.atleast_1d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:29.975158 test begin: paddle.atleast_1d(Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:29.975949 test begin: paddle.atleast_1d(Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:29.977129 test begin: paddle.atleast_1d(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:29.978571 test begin: paddle.atleast_1d(Tensor([1, 0, 1],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0, 1],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:29.979748 test begin: paddle.atleast_1d(Tensor([1, 0, 1],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0, 1],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:29.981462 test begin: paddle.atleast_1d(Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0],"float64"), )
2025-03-03 17:02:29.983099 test begin: paddle.atleast_1d(Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:29.984060 test begin: paddle.atleast_1d(Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:29.985240 test begin: paddle.atleast_1d(Tensor([1, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:29.986391 test begin: paddle.atleast_1d(Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:29.987225 test begin: paddle.atleast_1d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:29.988187 test begin: paddle.atleast_1d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:29.989677 test begin: paddle.atleast_1d(Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:29.990574 test begin: paddle.atleast_1d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:29.991485 test begin: paddle.atleast_1d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:29.992669 test begin: paddle.atleast_1d(Tensor([1, 1, 0],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 0],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:29.993626 test begin: paddle.atleast_1d(Tensor([1, 1, 0],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 0],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:29.994511 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:29.995210 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:29.996263 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:29.998202 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:29.999426 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.000677 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.002088 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.003392 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.004612 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:30.005698 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:30.006899 test begin: paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:30.008048 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.009131 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.010163 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.011244 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.012302 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.013357 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.014468 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:30.015602 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:30.016718 test begin: paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:30.018160 test begin: paddle.atleast_1d(Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.019459 test begin: paddle.atleast_1d(Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.020600 test begin: paddle.atleast_1d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), )
2025-03-03 17:02:30.021633 test begin: paddle.atleast_1d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.022701 test begin: paddle.atleast_1d(Tensor([1, 2],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1, 2],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.023676 test begin: paddle.atleast_1d(Tensor([1],"float64"), Tensor([0],"float64"), Tensor([1],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1],"float64"), Tensor([0],"float64"), Tensor([1],"float64"), )
2025-03-03 17:02:30.024705 test begin: paddle.atleast_1d(Tensor([1],"float64"), Tensor([1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([1],"float64"), Tensor([1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.025855 test begin: paddle.atleast_1d(Tensor([2, 0, 1],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 0, 1],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:30.026884 test begin: paddle.atleast_1d(Tensor([2, 0, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 0, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.028253 test begin: paddle.atleast_1d(Tensor([2, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.030107 test begin: paddle.atleast_1d(Tensor([2, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.031291 test begin: paddle.atleast_1d(Tensor([2, 3, 0],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 3, 0],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:30.032390 test begin: paddle.atleast_1d(Tensor([2, 3, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 3, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.033484 test begin: paddle.atleast_1d(Tensor([2, 3, 1],"float64"), Tensor([0, 3],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 3, 1],"float64"), Tensor([0, 3],"float64"), )
2025-03-03 17:02:30.034624 test begin: paddle.atleast_1d(Tensor([2, 3, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 3, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.035705 test begin: paddle.atleast_1d(Tensor([2, 3, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2, 3, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.036707 test begin: paddle.atleast_1d(Tensor([2],"float32"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([2],"float32"), Tensor([0],"float64"), )
2025-03-03 17:02:30.037825 test begin: paddle.atleast_1d(Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.038694 test begin: paddle.atleast_1d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.039640 test begin: paddle.atleast_1d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.040873 test begin: paddle.atleast_1d(Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.041700 test begin: paddle.atleast_1d(Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.042577 test begin: paddle.atleast_1d(Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.043705 test begin: paddle.atleast_1d(Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.044483 test begin: paddle.atleast_1d(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.045355 test begin: paddle.atleast_1d(Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.047074 test begin: paddle.atleast_1d(Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.048376 test begin: paddle.atleast_1d(Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.049566 test begin: paddle.atleast_1d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.050708 test begin: paddle.atleast_1d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.051982 test begin: paddle.atleast_1d(Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.052959 test begin: paddle.atleast_1d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.053852 test begin: paddle.atleast_1d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.055006 test begin: paddle.atleast_1d(Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.055846 test begin: paddle.atleast_1d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.056778 test begin: paddle.atleast_1d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.057941 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.058705 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.059630 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.060766 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.061945 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.063142 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.064210 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.065357 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.066495 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.067623 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.068699 test begin: paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.069979 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.071548 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.072795 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.073954 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:30.075092 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.076173 test begin: paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.077291 test begin: paddle.atleast_1d(Tensor([5],"float64"), Tensor([0],"float64"), Tensor([5],"float64"), )

[Pass] paddle.atleast_1d(Tensor([5],"float64"), Tensor([0],"float64"), Tensor([5],"float64"), )
2025-03-03 17:02:30.078376 test begin: paddle.atleast_1d(Tensor([5],"float64"), Tensor([5],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_1d(Tensor([5],"float64"), Tensor([5],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.079483 test begin: paddle.atleast_2d(Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.081044 test begin: paddle.atleast_2d(Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.082472 test begin: paddle.atleast_2d(Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.084168 test begin: paddle.atleast_2d(Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:30.084971 test begin: paddle.atleast_2d(Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:30.085834 test begin: paddle.atleast_2d(Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.086849 test begin: paddle.atleast_2d(Tensor([0, 1, 1],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1, 1],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:30.087771 test begin: paddle.atleast_2d(Tensor([0, 1, 1],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1, 1],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:30.089085 test begin: paddle.atleast_2d(Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1],"float64"), )
2025-03-03 17:02:30.089860 test begin: paddle.atleast_2d(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), )
2025-03-03 17:02:30.090719 test begin: paddle.atleast_2d(Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.091736 test begin: paddle.atleast_2d(Tensor([0, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.092929 test begin: paddle.atleast_2d(Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.093712 test begin: paddle.atleast_2d(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.094682 test begin: paddle.atleast_2d(Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.095784 test begin: paddle.atleast_2d(Tensor([0, 2],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 2],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.097073 test begin: paddle.atleast_2d(Tensor([0, 3, 1],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 3, 1],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:30.098053 test begin: paddle.atleast_2d(Tensor([0, 3, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 3, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.099536 test begin: paddle.atleast_2d(Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.100566 test begin: paddle.atleast_2d(Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.101433 test begin: paddle.atleast_2d(Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.102530 test begin: paddle.atleast_2d(Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:30.103268 test begin: paddle.atleast_2d(Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:30.104083 test begin: paddle.atleast_2d(Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.105116 test begin: paddle.atleast_2d(Tensor([0],"float32"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0],"float32"), Tensor([0],"float64"), )
2025-03-03 17:02:30.105984 test begin: paddle.atleast_2d(Tensor([0],"float32"), Tensor([2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0],"float32"), Tensor([2],"float64"), )
2025-03-03 17:02:30.106919 test begin: paddle.atleast_2d(Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0],"float64"), )
2025-03-03 17:02:30.107694 test begin: paddle.atleast_2d(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.108593 test begin: paddle.atleast_2d(Tensor([0],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:30.109579 test begin: paddle.atleast_2d(Tensor([0],"float64"), Tensor([1],"float64"), Tensor([1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0],"float64"), Tensor([1],"float64"), Tensor([1],"float64"), )
2025-03-03 17:02:30.110766 test begin: paddle.atleast_2d(Tensor([0],"float64"), Tensor([5],"float64"), Tensor([5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([0],"float64"), Tensor([5],"float64"), Tensor([5],"float64"), )
2025-03-03 17:02:30.112256 test begin: paddle.atleast_2d(Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:30.113272 test begin: paddle.atleast_2d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:30.115329 test begin: paddle.atleast_2d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.116463 test begin: paddle.atleast_2d(Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:30.117289 test begin: paddle.atleast_2d(Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:30.118440 test begin: paddle.atleast_2d(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.119612 test begin: paddle.atleast_2d(Tensor([1, 0, 1],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0, 1],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:30.120564 test begin: paddle.atleast_2d(Tensor([1, 0, 1],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0, 1],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:30.121575 test begin: paddle.atleast_2d(Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.122324 test begin: paddle.atleast_2d(Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.123245 test begin: paddle.atleast_2d(Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.124389 test begin: paddle.atleast_2d(Tensor([1, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.125392 test begin: paddle.atleast_2d(Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:30.126107 test begin: paddle.atleast_2d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:30.127071 test begin: paddle.atleast_2d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.128257 test begin: paddle.atleast_2d(Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:30.129018 test begin: paddle.atleast_2d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:30.129857 test begin: paddle.atleast_2d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.130887 test begin: paddle.atleast_2d(Tensor([1, 1, 0],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 0],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:30.131837 test begin: paddle.atleast_2d(Tensor([1, 1, 0],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 0],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:30.132925 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:30.133686 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:30.134669 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.135899 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.137029 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.138326 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.139689 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.140901 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.142929 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:30.146145 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:30.147523 test begin: paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:30.148957 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.150248 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.151389 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.152509 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.153676 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.154749 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.156676 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:30.158028 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:30.159306 test begin: paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:30.160450 test begin: paddle.atleast_2d(Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.161730 test begin: paddle.atleast_2d(Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.163052 test begin: paddle.atleast_2d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), )
2025-03-03 17:02:30.164185 test begin: paddle.atleast_2d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.165238 test begin: paddle.atleast_2d(Tensor([1, 2],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1, 2],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.167034 test begin: paddle.atleast_2d(Tensor([1],"float64"), Tensor([0],"float64"), Tensor([1],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1],"float64"), Tensor([0],"float64"), Tensor([1],"float64"), )
2025-03-03 17:02:30.168212 test begin: paddle.atleast_2d(Tensor([1],"float64"), Tensor([1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([1],"float64"), Tensor([1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.169395 test begin: paddle.atleast_2d(Tensor([2, 0, 1],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 0, 1],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:30.170793 test begin: paddle.atleast_2d(Tensor([2, 0, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 0, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.172037 test begin: paddle.atleast_2d(Tensor([2, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.173006 test begin: paddle.atleast_2d(Tensor([2, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.174191 test begin: paddle.atleast_2d(Tensor([2, 3, 0],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 3, 0],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:30.175916 test begin: paddle.atleast_2d(Tensor([2, 3, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 3, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.177004 test begin: paddle.atleast_2d(Tensor([2, 3, 1],"float64"), Tensor([0, 3],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 3, 1],"float64"), Tensor([0, 3],"float64"), )
2025-03-03 17:02:30.178111 test begin: paddle.atleast_2d(Tensor([2, 3, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 3, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.180284 test begin: paddle.atleast_2d(Tensor([2, 3, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2, 3, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.181661 test begin: paddle.atleast_2d(Tensor([2],"float32"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2],"float32"), Tensor([0],"float64"), )
2025-03-03 17:02:30.183379 test begin: paddle.atleast_2d(Tensor([2],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.184632 test begin: paddle.atleast_2d(Tensor([2],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([2],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.185964 test begin: paddle.atleast_2d(Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.187027 test begin: paddle.atleast_2d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.188254 test begin: paddle.atleast_2d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.189631 test begin: paddle.atleast_2d(Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.190568 test begin: paddle.atleast_2d(Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.191791 test begin: paddle.atleast_2d(Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.193178 test begin: paddle.atleast_2d(Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.193895 test begin: paddle.atleast_2d(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.194841 test begin: paddle.atleast_2d(Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.195959 test begin: paddle.atleast_2d(Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.197341 test begin: paddle.atleast_2d(Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.198482 test begin: paddle.atleast_2d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.199999 test begin: paddle.atleast_2d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.202244 test begin: paddle.atleast_2d(Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.203614 test begin: paddle.atleast_2d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.204762 test begin: paddle.atleast_2d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.206087 test begin: paddle.atleast_2d(Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.207049 test begin: paddle.atleast_2d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.208284 test begin: paddle.atleast_2d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.209718 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.210644 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.211930 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.213419 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.214894 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.216391 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.217940 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.219116 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.220412 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.221730 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.222869 test begin: paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.224237 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.225500 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.226638 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.227804 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:30.229133 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.230252 test begin: paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.231810 test begin: paddle.atleast_2d(Tensor([5],"float64"), Tensor([0],"float64"), Tensor([5],"float64"), )

[Pass] paddle.atleast_2d(Tensor([5],"float64"), Tensor([0],"float64"), Tensor([5],"float64"), )
2025-03-03 17:02:30.233104 test begin: paddle.atleast_2d(Tensor([5],"float64"), Tensor([5],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_2d(Tensor([5],"float64"), Tensor([5],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.234329 test begin: paddle.atleast_3d(Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.235453 test begin: paddle.atleast_3d(Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.236441 test begin: paddle.atleast_3d(Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.237604 test begin: paddle.atleast_3d(Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:30.239460 test begin: paddle.atleast_3d(Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:30.240526 test begin: paddle.atleast_3d(Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.241855 test begin: paddle.atleast_3d(Tensor([0, 1, 1],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1, 1],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:30.242945 test begin: paddle.atleast_3d(Tensor([0, 1, 1],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1, 1],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:30.243991 test begin: paddle.atleast_3d(Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1],"float64"), )
2025-03-03 17:02:30.244894 test begin: paddle.atleast_3d(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), Tensor([0, 1],"float64"), )
2025-03-03 17:02:30.246008 test begin: paddle.atleast_3d(Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.247131 test begin: paddle.atleast_3d(Tensor([0, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.248384 test begin: paddle.atleast_3d(Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.249443 test begin: paddle.atleast_3d(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.250685 test begin: paddle.atleast_3d(Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.252021 test begin: paddle.atleast_3d(Tensor([0, 2],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 2],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.253098 test begin: paddle.atleast_3d(Tensor([0, 3, 1],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 3, 1],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:30.254226 test begin: paddle.atleast_3d(Tensor([0, 3, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 3, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.255393 test begin: paddle.atleast_3d(Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.256234 test begin: paddle.atleast_3d(Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.257257 test begin: paddle.atleast_3d(Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.258495 test begin: paddle.atleast_3d(Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:30.259269 test begin: paddle.atleast_3d(Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:30.260219 test begin: paddle.atleast_3d(Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.261461 test begin: paddle.atleast_3d(Tensor([0],"float32"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0],"float32"), Tensor([0],"float64"), )
2025-03-03 17:02:30.262427 test begin: paddle.atleast_3d(Tensor([0],"float32"), Tensor([2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0],"float32"), Tensor([2],"float64"), )
2025-03-03 17:02:30.263511 test begin: paddle.atleast_3d(Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0],"float64"), )
2025-03-03 17:02:30.264347 test begin: paddle.atleast_3d(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0],"float64"), Tensor([0],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.265579 test begin: paddle.atleast_3d(Tensor([0],"float64"), Tensor([1, 2, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0],"float64"), Tensor([1, 2, 1],"float64"), )
2025-03-03 17:02:30.266617 test begin: paddle.atleast_3d(Tensor([0],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:30.267565 test begin: paddle.atleast_3d(Tensor([0],"float64"), Tensor([1],"float64"), Tensor([1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0],"float64"), Tensor([1],"float64"), Tensor([1],"float64"), )
2025-03-03 17:02:30.268743 test begin: paddle.atleast_3d(Tensor([0],"float64"), Tensor([5],"float64"), Tensor([5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([0],"float64"), Tensor([5],"float64"), Tensor([5],"float64"), )
2025-03-03 17:02:30.269930 test begin: paddle.atleast_3d(Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:30.270970 test begin: paddle.atleast_3d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:30.272082 test begin: paddle.atleast_3d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.273383 test begin: paddle.atleast_3d(Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:30.274811 test begin: paddle.atleast_3d(Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:30.275752 test begin: paddle.atleast_3d(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.276922 test begin: paddle.atleast_3d(Tensor([1, 0, 1],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0, 1],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:30.278319 test begin: paddle.atleast_3d(Tensor([1, 0, 1],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0, 1],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:30.279425 test begin: paddle.atleast_3d(Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.280323 test begin: paddle.atleast_3d(Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.281952 test begin: paddle.atleast_3d(Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.283489 test begin: paddle.atleast_3d(Tensor([1, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.285000 test begin: paddle.atleast_3d(Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:30.286040 test begin: paddle.atleast_3d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:30.287235 test begin: paddle.atleast_3d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.288600 test begin: paddle.atleast_3d(Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:30.289563 test begin: paddle.atleast_3d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:30.290866 test begin: paddle.atleast_3d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.292144 test begin: paddle.atleast_3d(Tensor([1, 1, 0],"float64"), Tensor([1, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 0],"float64"), Tensor([1, 2],"float64"), )
2025-03-03 17:02:30.293286 test begin: paddle.atleast_3d(Tensor([1, 1, 0],"float64"), Tensor([2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 0],"float64"), Tensor([2],"float64"), )
2025-03-03 17:02:30.294322 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:30.295216 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:30.296195 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.297384 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.298944 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.300208 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.301427 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), Tensor([1, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.303379 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([0, 1, 1, 1],"float64"), )
2025-03-03 17:02:30.304990 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 0, 1, 1],"float64"), )
2025-03-03 17:02:30.306508 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 0, 1],"float64"), )
2025-03-03 17:02:30.307891 test begin: paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 1],"float64"), Tensor([1, 1, 1, 0],"float64"), )
2025-03-03 17:02:30.309299 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.311363 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.312807 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.314224 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.315682 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.316969 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), Tensor([1, 1, 1],"float64"), )
2025-03-03 17:02:30.318768 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([0, 1, 1],"float64"), )
2025-03-03 17:02:30.320788 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:30.321916 test begin: paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1, 1],"float64"), Tensor([1, 1, 1],"float64"), Tensor([1, 1, 0],"float64"), )
2025-03-03 17:02:30.326357 test begin: paddle.atleast_3d(Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.329097 test begin: paddle.atleast_3d(Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), Tensor([1, 1],"float64"), )
2025-03-03 17:02:30.331337 test begin: paddle.atleast_3d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([0, 1],"float64"), )
2025-03-03 17:02:30.332914 test begin: paddle.atleast_3d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 1],"float64"), Tensor([1, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.334338 test begin: paddle.atleast_3d(Tensor([1, 2],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1, 2],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.335468 test begin: paddle.atleast_3d(Tensor([1],"float64"), Tensor([0],"float64"), Tensor([1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1],"float64"), Tensor([0],"float64"), Tensor([1],"float64"), )
2025-03-03 17:02:30.336639 test begin: paddle.atleast_3d(Tensor([1],"float64"), Tensor([1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([1],"float64"), Tensor([1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.338420 test begin: paddle.atleast_3d(Tensor([2, 0, 1],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 0, 1],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:30.340111 test begin: paddle.atleast_3d(Tensor([2, 0, 1],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 0, 1],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.341136 test begin: paddle.atleast_3d(Tensor([2, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.342205 test begin: paddle.atleast_3d(Tensor([2, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.343221 test begin: paddle.atleast_3d(Tensor([2, 3, 0],"float64"), Tensor([1, 3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 3, 0],"float64"), Tensor([1, 3],"float64"), )
2025-03-03 17:02:30.344303 test begin: paddle.atleast_3d(Tensor([2, 3, 0],"float64"), Tensor([3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 3, 0],"float64"), Tensor([3],"float64"), )
2025-03-03 17:02:30.346758 test begin: paddle.atleast_3d(Tensor([2, 3, 1],"float64"), Tensor([0, 3],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 3, 1],"float64"), Tensor([0, 3],"float64"), )
2025-03-03 17:02:30.347782 test begin: paddle.atleast_3d(Tensor([2, 3, 1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 3, 1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.348807 test begin: paddle.atleast_3d(Tensor([2, 3, 1],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2, 3, 1],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.349928 test begin: paddle.atleast_3d(Tensor([2],"float32"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2],"float32"), Tensor([0],"float64"), )
2025-03-03 17:02:30.350925 test begin: paddle.atleast_3d(Tensor([2],"float64"), Tensor([0, 2, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2],"float64"), Tensor([0, 2, 1],"float64"), )
2025-03-03 17:02:30.352459 test begin: paddle.atleast_3d(Tensor([2],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.354130 test begin: paddle.atleast_3d(Tensor([2],"float64"), Tensor([1, 0, 1],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2],"float64"), Tensor([1, 0, 1],"float64"), )
2025-03-03 17:02:30.355294 test begin: paddle.atleast_3d(Tensor([2],"float64"), Tensor([1, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2],"float64"), Tensor([1, 0],"float64"), )
2025-03-03 17:02:30.356350 test begin: paddle.atleast_3d(Tensor([2],"float64"), Tensor([1, 2, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([2],"float64"), Tensor([1, 2, 0],"float64"), )
2025-03-03 17:02:30.357525 test begin: paddle.atleast_3d(Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.358477 test begin: paddle.atleast_3d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.359942 test begin: paddle.atleast_3d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.361047 test begin: paddle.atleast_3d(Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.361810 test begin: paddle.atleast_3d(Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.362822 test begin: paddle.atleast_3d(Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.364864 test begin: paddle.atleast_3d(Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.367115 test begin: paddle.atleast_3d(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.368492 test begin: paddle.atleast_3d(Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.369847 test begin: paddle.atleast_3d(Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.371300 test begin: paddle.atleast_3d(Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), Tensor([3, 2],"float64"), )
2025-03-03 17:02:30.372870 test begin: paddle.atleast_3d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([0, 2],"float64"), )
2025-03-03 17:02:30.374778 test begin: paddle.atleast_3d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 2],"float64"), Tensor([3, 2],"float64"), Tensor([3, 0],"float64"), )
2025-03-03 17:02:30.376440 test begin: paddle.atleast_3d(Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.377418 test begin: paddle.atleast_3d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.378519 test begin: paddle.atleast_3d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.379865 test begin: paddle.atleast_3d(Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.380801 test begin: paddle.atleast_3d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.382878 test begin: paddle.atleast_3d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.384256 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.385008 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.386070 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.387300 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.388430 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.389656 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.391164 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), Tensor([3, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.392770 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([0, 4, 2, 5],"float64"), )
2025-03-03 17:02:30.398880 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 0, 2, 5],"float64"), )
2025-03-03 17:02:30.400295 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 0, 5],"float64"), )
2025-03-03 17:02:30.402269 test begin: paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 5],"float64"), Tensor([3, 4, 2, 0],"float64"), )
2025-03-03 17:02:30.407391 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.409039 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.415305 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), Tensor([3, 4, 2],"float64"), )
2025-03-03 17:02:30.422051 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([0, 4, 2],"float64"), )
2025-03-03 17:02:30.423615 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 0, 2],"float64"), )
2025-03-03 17:02:30.424880 test begin: paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([3, 4, 2],"float64"), Tensor([3, 4, 2],"float64"), Tensor([3, 4, 0],"float64"), )
2025-03-03 17:02:30.426156 test begin: paddle.atleast_3d(Tensor([5],"float64"), Tensor([0],"float64"), Tensor([5],"float64"), )

[Pass] paddle.atleast_3d(Tensor([5],"float64"), Tensor([0],"float64"), Tensor([5],"float64"), )
2025-03-03 17:02:30.427489 test begin: paddle.atleast_3d(Tensor([5],"float64"), Tensor([5],"float64"), Tensor([0],"float64"), )

[Pass] paddle.atleast_3d(Tensor([5],"float64"), Tensor([5],"float64"), Tensor([0],"float64"), )
2025-03-03 17:02:30.428774 test begin: paddle.autograd.hessian(Tensor([0, 1],"float32"), Tensor([0, 2],"float32"), batch_axis=0, )

[paddle error] paddle.autograd.hessian(Tensor([0, 1],"float32"), Tensor([0, 2],"float32"), batch_axis=0, ) 
 (OutOfRange) The starting index 0 of slice is out of bounds in tensor 0-th axis, it shound be in the range of [0, 0). (at ../paddle/fluid/pybind/slice_utils.h:241)

2025-03-03 17:02:30.429603 test begin: paddle.autograd.hessian(Tensor([0, 1],"float32"), Tensor([5, 2],"float32"), batch_axis=0, )

[paddle error] paddle.autograd.hessian(Tensor([0, 1],"float32"), Tensor([5, 2],"float32"), batch_axis=0, ) 
 (OutOfRange) The starting index 0 of slice is out of bounds in tensor 0-th axis, it shound be in the range of [0, 0). (at ../paddle/fluid/pybind/slice_utils.h:241)

2025-03-03 17:02:30.430733 test begin: paddle.autograd.hessian(Tensor([5, 0],"float32"), Tensor([5, 0],"float32"), batch_axis=0, )

[paddle error] paddle.autograd.hessian(Tensor([5, 0],"float32"), Tensor([5, 0],"float32"), batch_axis=0, ) 
 (InvalidArgument) can not reshape 0, 1 to 0, -1, because the unspecified dimension 1 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:1 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 17:02:30.433151 test begin: paddle.autograd.hessian(Tensor([5, 0],"float32"), Tensor([5, 2],"float32"), batch_axis=0, )

[paddle error] paddle.autograd.hessian(Tensor([5, 0],"float32"), Tensor([5, 2],"float32"), batch_axis=0, ) 
 (InvalidArgument) can not reshape 0, 1 to 0, -1, because the unspecified dimension 1 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:1 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 17:02:30.434842 test begin: paddle.autograd.hessian(Tensor([5, 1],"float32"), Tensor([0, 2],"float32"), batch_axis=0, )

[paddle error] paddle.autograd.hessian(Tensor([5, 1],"float32"), Tensor([0, 2],"float32"), batch_axis=0, ) 
 (InvalidArgument) can not reshape 0, 2 to 0, -1, because the unspecified dimension 1 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:1 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 17:02:30.436393 test begin: paddle.autograd.hessian(Tensor([5, 1],"float32"), Tensor([5, 0],"float32"), batch_axis=0, )

[paddle error] paddle.autograd.hessian(Tensor([5, 1],"float32"), Tensor([5, 0],"float32"), batch_axis=0, ) 
 (InvalidArgument) concat(): argument 'x' (position 0) must be list of Tensors, but got empty list (at ../paddle/fluid/pybind/eager_utils.cc:1400)

2025-03-03 17:02:30.438602 test begin: paddle.autograd.jacobian(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), batch_axis=0, )

[paddle error] paddle.autograd.jacobian(Tensor([0, 3],"float64"), Tensor([0, 3],"float64"), batch_axis=0, ) 
 (InvalidArgument) can not reshape 0, 3 to 0, -1, because the unspecified dimension 1 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:1 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 17:02:30.439341 test begin: paddle.autograd.jacobian(Tensor([0, 3],"float64"), Tensor([2, 3],"float64"), batch_axis=0, )

[paddle error] paddle.autograd.jacobian(Tensor([0, 3],"float64"), Tensor([2, 3],"float64"), batch_axis=0, ) 
 (InvalidArgument) can not reshape 0, 3 to 0, -1, because the unspecified dimension 1 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:1 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 17:02:30.440432 test begin: paddle.autograd.jacobian(Tensor([0],"float64"), Tensor([0],"float64"), batch_axis=None, )

[Pass] paddle.autograd.jacobian(Tensor([0],"float64"), Tensor([0],"float64"), batch_axis=None, )
2025-03-03 17:02:30.441636 test begin: paddle.autograd.jacobian(Tensor([0],"float64"), Tensor([2],"float64"), batch_axis=None, )

[Pass] paddle.autograd.jacobian(Tensor([0],"float64"), Tensor([2],"float64"), batch_axis=None, )
2025-03-03 17:02:30.442739 test begin: paddle.autograd.jacobian(Tensor([0],"float64"), Tensor([6],"float64"), batch_axis=None, )

[Pass] paddle.autograd.jacobian(Tensor([0],"float64"), Tensor([6],"float64"), batch_axis=None, )
2025-03-03 17:02:30.444073 test begin: paddle.autograd.jacobian(Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), batch_axis=0, )

[Pass] paddle.autograd.jacobian(Tensor([2, 0],"float64"), Tensor([2, 0],"float64"), batch_axis=0, )
2025-03-03 17:02:30.447270 test begin: paddle.autograd.jacobian(Tensor([2, 0],"float64"), Tensor([2, 3],"float64"), batch_axis=0, )

[Pass] paddle.autograd.jacobian(Tensor([2, 0],"float64"), Tensor([2, 3],"float64"), batch_axis=0, )
2025-03-03 17:02:30.448601 test begin: paddle.autograd.jacobian(Tensor([2, 3],"float64"), Tensor([0, 3],"float64"), batch_axis=0, )

[paddle error] paddle.autograd.jacobian(Tensor([2, 3],"float64"), Tensor([0, 3],"float64"), batch_axis=0, ) 
 (InvalidArgument) can not reshape 0, 3 to 0, -1, because the unspecified dimension 1 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:1 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 17:02:30.449839 test begin: paddle.autograd.jacobian(Tensor([2, 3],"float64"), Tensor([2, 0],"float64"), batch_axis=0, )

[Pass] paddle.autograd.jacobian(Tensor([2, 3],"float64"), Tensor([2, 0],"float64"), batch_axis=0, )
2025-03-03 17:02:30.450985 test begin: paddle.autograd.jacobian(Tensor([2],"float64"), Tensor([0],"float64"), batch_axis=None, )

[Pass] paddle.autograd.jacobian(Tensor([2],"float64"), Tensor([0],"float64"), batch_axis=None, )
2025-03-03 17:02:30.452205 test begin: paddle.autograd.jacobian(Tensor([6],"float64"), Tensor([0],"float64"), batch_axis=None, )

[Pass] paddle.autograd.jacobian(Tensor([6],"float64"), Tensor([0],"float64"), batch_axis=None, )
2025-03-03 17:02:30.453610 test begin: paddle.bincount(x=Tensor([0],"int32"), )

[Pass] paddle.bincount(x=Tensor([0],"int32"), )
2025-03-03 17:02:30.454513 test begin: paddle.bitwise_and(Tensor([0, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_and(Tensor([0, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4, 1] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.455931 test begin: paddle.bitwise_and(Tensor([3, 0, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 0, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.457160 test begin: paddle.bitwise_and(Tensor([3, 4, 0],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_and(Tensor([3, 4, 0],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 0] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.459173 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([0, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([0, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.461633 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 0, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 0, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 1] and the shape of Y = [2, 0, 1, 5]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.463449 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 0, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 0, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 1] and the shape of Y = [2, 3, 0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.465895 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 0],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 0],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.467482 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([0, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([0, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.469011 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 0, 4, 5],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 0, 4, 5],"int64"), )
2025-03-03 17:02:30.470343 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 0, 5],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 0, 5],"int64"), )
2025-03-03 17:02:30.471835 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 0],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 0],"int64"), )
2025-03-03 17:02:30.473592 test begin: paddle.bitwise_invert(Tensor([0, 3, 4, 5],"int32"), )

[cuda error] paddle.bitwise_invert(Tensor([0, 3, 4, 5],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.474706 test begin: paddle.bitwise_invert(Tensor([0, 4, 1],"int32"), )

[cuda error] paddle.bitwise_invert(Tensor([0, 4, 1],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.476151 test begin: paddle.bitwise_invert(Tensor([2, 0, 4, 5],"int32"), )

[cuda error] paddle.bitwise_invert(Tensor([2, 0, 4, 5],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.477206 test begin: paddle.bitwise_invert(Tensor([2, 3, 0, 5],"int32"), )

[cuda error] paddle.bitwise_invert(Tensor([2, 3, 0, 5],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.477921 test begin: paddle.bitwise_invert(Tensor([2, 3, 4, 0],"int32"), )

[cuda error] paddle.bitwise_invert(Tensor([2, 3, 4, 0],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.478528 test begin: paddle.bitwise_invert(Tensor([3, 0, 1],"int32"), )

[cuda error] paddle.bitwise_invert(Tensor([3, 0, 1],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.479242 test begin: paddle.bitwise_invert(Tensor([3, 4, 0],"int32"), )

[cuda error] paddle.bitwise_invert(Tensor([3, 4, 0],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.479916 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([0, 300],"int16"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([0, 300],"int16"), False, )
2025-03-03 17:02:30.483208 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([200, 300],"int16"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([200, 300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 300] and the shape of Y = [200, 300]. Received [0] in X is not equal to [200] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.487707 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([300],"int16"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([300],"int16"), False, )
2025-03-03 17:02:30.500440 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([0, 300],"int32"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([0, 300],"int32"), False, )
2025-03-03 17:02:30.508323 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([200, 300],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([200, 300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 300] and the shape of Y = [200, 300]. Received [0] in X is not equal to [200] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.514573 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([300],"int32"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([300],"int32"), False, )
2025-03-03 17:02:30.516202 test begin: paddle.bitwise_left_shift(Tensor([0, 4, 5],"int32"), Tensor([0, 4, 5],"int32"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0, 4, 5],"int32"), Tensor([0, 4, 5],"int32"), False, )
2025-03-03 17:02:30.518270 test begin: paddle.bitwise_left_shift(Tensor([0, 4, 5],"int32"), Tensor([0, 4, 5],"int32"), True, )

[Pass] paddle.bitwise_left_shift(Tensor([0, 4, 5],"int32"), Tensor([0, 4, 5],"int32"), True, )
2025-03-03 17:02:30.520971 test begin: paddle.bitwise_left_shift(Tensor([0, 4, 5],"int32"), Tensor([3, 4, 5],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([0, 4, 5],"int32"), Tensor([3, 4, 5],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4, 5] and the shape of Y = [3, 4, 5]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.523323 test begin: paddle.bitwise_left_shift(Tensor([0, 4, 5],"int32"), Tensor([3, 4, 5],"int32"), True, )

[paddle error] paddle.bitwise_left_shift(Tensor([0, 4, 5],"int32"), Tensor([3, 4, 5],"int32"), True, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4, 5] and the shape of Y = [3, 4, 5]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.524396 test begin: paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([0],"int16"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([0],"int16"), False, )
2025-03-03 17:02:30.525745 test begin: paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([1],"int16"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([1],"int16"), False, )
2025-03-03 17:02:30.527254 test begin: paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([200, 300],"int16"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([200, 300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [200, 300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.528709 test begin: paddle.bitwise_left_shift(Tensor([0],"int32"), Tensor([200, 300],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([0],"int32"), Tensor([200, 300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [200, 300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.530734 test begin: paddle.bitwise_left_shift(Tensor([0],"uint8"), Tensor([0],"uint8"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0],"uint8"), Tensor([0],"uint8"), False, )
2025-03-03 17:02:30.532755 test begin: paddle.bitwise_left_shift(Tensor([0],"uint8"), Tensor([1],"uint8"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([0],"uint8"), Tensor([1],"uint8"), False, )
2025-03-03 17:02:30.534009 test begin: paddle.bitwise_left_shift(Tensor([1],"int16"), Tensor([0],"int16"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([1],"int16"), Tensor([0],"int16"), False, )
2025-03-03 17:02:30.535025 test begin: paddle.bitwise_left_shift(Tensor([1],"uint8"), Tensor([0],"uint8"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([1],"uint8"), Tensor([0],"uint8"), False, )
2025-03-03 17:02:30.537340 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([200, 0],"int16"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([200, 0],"int16"), False, )
2025-03-03 17:02:30.538787 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([200, 300],"int16"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([200, 300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 0] and the shape of Y = [200, 300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.540199 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([300],"int16"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 0] and the shape of Y = [300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.541858 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([200, 0],"int32"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([200, 0],"int32"), False, )
2025-03-03 17:02:30.543048 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([200, 300],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([200, 300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 0] and the shape of Y = [200, 300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.544595 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([300],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 0] and the shape of Y = [300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.545713 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([0, 300],"int16"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([0, 300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [0, 300]. Received [200] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.547339 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([0],"int16"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([0],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.548682 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([200, 0],"int16"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([200, 0],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [200, 0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.550316 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([0, 300],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([0, 300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [0, 300]. Received [200] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.552005 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([0],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([0],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.553653 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([200, 0],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([200, 0],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [200, 0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.555210 test begin: paddle.bitwise_left_shift(Tensor([3, 0, 5],"int32"), Tensor([3, 0, 5],"int32"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([3, 0, 5],"int32"), Tensor([3, 0, 5],"int32"), False, )
2025-03-03 17:02:30.556998 test begin: paddle.bitwise_left_shift(Tensor([3, 0, 5],"int32"), Tensor([3, 0, 5],"int32"), True, )

[Pass] paddle.bitwise_left_shift(Tensor([3, 0, 5],"int32"), Tensor([3, 0, 5],"int32"), True, )
2025-03-03 17:02:30.557943 test begin: paddle.bitwise_left_shift(Tensor([3, 0, 5],"int32"), Tensor([3, 4, 5],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 0, 5],"int32"), Tensor([3, 4, 5],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 5] and the shape of Y = [3, 4, 5]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.558941 test begin: paddle.bitwise_left_shift(Tensor([3, 0, 5],"int32"), Tensor([3, 4, 5],"int32"), True, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 0, 5],"int32"), Tensor([3, 4, 5],"int32"), True, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 5] and the shape of Y = [3, 4, 5]. Received [0] in X is not equal to [4] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.560132 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 0],"int32"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 0],"int32"), False, )
2025-03-03 17:02:30.562111 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 0],"int32"), True, )

[Pass] paddle.bitwise_left_shift(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 0],"int32"), True, )
2025-03-03 17:02:30.563198 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 5],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 5],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 0] and the shape of Y = [3, 4, 5]. Received [0] in X is not equal to [5] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.564275 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 5],"int32"), True, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 5],"int32"), True, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 0] and the shape of Y = [3, 4, 5]. Received [0] in X is not equal to [5] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.565588 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([0, 4, 5],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([0, 4, 5],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 5] and the shape of Y = [0, 4, 5]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.566861 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([0, 4, 5],"int32"), True, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([0, 4, 5],"int32"), True, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 5] and the shape of Y = [0, 4, 5]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.568091 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([3, 0, 5],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([3, 0, 5],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 5] and the shape of Y = [3, 0, 5]. Received [4] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.569204 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([3, 0, 5],"int32"), True, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([3, 0, 5],"int32"), True, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 5] and the shape of Y = [3, 0, 5]. Received [4] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.570212 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([3, 4, 0],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([3, 4, 0],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 5] and the shape of Y = [3, 4, 0]. Received [5] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.571467 test begin: paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([3, 4, 0],"int32"), True, )

[paddle error] paddle.bitwise_left_shift(Tensor([3, 4, 5],"int32"), Tensor([3, 4, 0],"int32"), True, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 5] and the shape of Y = [3, 4, 0]. Received [5] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.572519 test begin: paddle.bitwise_left_shift(Tensor([300],"int16"), Tensor([0, 300],"int16"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([300],"int16"), Tensor([0, 300],"int16"), False, )
2025-03-03 17:02:30.573729 test begin: paddle.bitwise_left_shift(Tensor([300],"int16"), Tensor([200, 0],"int16"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([300],"int16"), Tensor([200, 0],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [300] and the shape of Y = [200, 0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.574794 test begin: paddle.bitwise_left_shift(Tensor([300],"int32"), Tensor([0, 300],"int32"), False, )

[Pass] paddle.bitwise_left_shift(Tensor([300],"int32"), Tensor([0, 300],"int32"), False, )
2025-03-03 17:02:30.575858 test begin: paddle.bitwise_left_shift(Tensor([300],"int32"), Tensor([200, 0],"int32"), False, )

[paddle error] paddle.bitwise_left_shift(Tensor([300],"int32"), Tensor([200, 0],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [300] and the shape of Y = [200, 0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.577478 test begin: paddle.bitwise_not(Tensor([0, 4, 1],"int64"), out=Tensor([0, 4, 1],"int64"), )

[cuda error] paddle.bitwise_not(Tensor([0, 4, 1],"int64"), out=Tensor([0, 4, 1],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.578812 test begin: paddle.bitwise_not(Tensor([0, 4, 1],"int64"), out=Tensor([3, 4, 1],"int64"), )

[cuda error] paddle.bitwise_not(Tensor([0, 4, 1],"int64"), out=Tensor([3, 4, 1],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.580313 test begin: paddle.bitwise_not(Tensor([3, 0, 1],"int64"), out=Tensor([3, 0, 1],"int64"), )

[cuda error] paddle.bitwise_not(Tensor([3, 0, 1],"int64"), out=Tensor([3, 0, 1],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.581656 test begin: paddle.bitwise_not(Tensor([3, 0, 1],"int64"), out=Tensor([3, 4, 1],"int64"), )

[cuda error] paddle.bitwise_not(Tensor([3, 0, 1],"int64"), out=Tensor([3, 4, 1],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.582743 test begin: paddle.bitwise_not(Tensor([3, 4, 0],"int64"), out=Tensor([3, 4, 0],"int64"), )

[cuda error] paddle.bitwise_not(Tensor([3, 4, 0],"int64"), out=Tensor([3, 4, 0],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.583411 test begin: paddle.bitwise_not(Tensor([3, 4, 0],"int64"), out=Tensor([3, 4, 1],"int64"), )

[cuda error] paddle.bitwise_not(Tensor([3, 4, 0],"int64"), out=Tensor([3, 4, 1],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:02:30.584439 test begin: paddle.bitwise_not(Tensor([3, 4, 1],"int64"), out=Tensor([0, 4, 1],"int64"), )

[Pass] paddle.bitwise_not(Tensor([3, 4, 1],"int64"), out=Tensor([0, 4, 1],"int64"), )
2025-03-03 17:02:30.586141 test begin: paddle.bitwise_not(Tensor([3, 4, 1],"int64"), out=Tensor([3, 0, 1],"int64"), )

[Pass] paddle.bitwise_not(Tensor([3, 4, 1],"int64"), out=Tensor([3, 0, 1],"int64"), )
2025-03-03 17:02:30.587670 test begin: paddle.bitwise_not(Tensor([3, 4, 1],"int64"), out=Tensor([3, 4, 0],"int64"), )

[Pass] paddle.bitwise_not(Tensor([3, 4, 1],"int64"), out=Tensor([3, 4, 0],"int64"), )
2025-03-03 17:02:30.588972 test begin: paddle.bitwise_or(Tensor([0, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_or(Tensor([0, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4, 1] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.591023 test begin: paddle.bitwise_or(Tensor([3, 0, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_or(Tensor([3, 0, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.592682 test begin: paddle.bitwise_or(Tensor([3, 4, 0],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_or(Tensor([3, 4, 0],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 0] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.594445 test begin: paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([0, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([0, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.596107 test begin: paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 0, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 0, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 1] and the shape of Y = [2, 0, 1, 5]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.597579 test begin: paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 0, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 0, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 1] and the shape of Y = [2, 3, 0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.599132 test begin: paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 0],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 0],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.600750 test begin: paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([0, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([0, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.602435 test begin: paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 0, 4, 5],"int64"), )

[Pass] paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 0, 4, 5],"int64"), )
2025-03-03 17:02:30.604019 test begin: paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 0, 5],"int64"), )

[Pass] paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 0, 5],"int64"), )
2025-03-03 17:02:30.605818 test begin: paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 0],"int64"), )

[Pass] paddle.bitwise_or(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 0],"int64"), )
2025-03-03 17:02:30.608631 test begin: paddle.bitwise_right_shift(Tensor([0, 300],"int16"), Tensor([0, 300],"int16"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([0, 300],"int16"), Tensor([0, 300],"int16"), False, )
2025-03-03 17:02:30.611277 test begin: paddle.bitwise_right_shift(Tensor([0, 300],"int16"), Tensor([200, 300],"int16"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([0, 300],"int16"), Tensor([200, 300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 300] and the shape of Y = [200, 300]. Received [0] in X is not equal to [200] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.612743 test begin: paddle.bitwise_right_shift(Tensor([0, 300],"int16"), Tensor([300],"int16"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([0, 300],"int16"), Tensor([300],"int16"), False, )
2025-03-03 17:02:30.613778 test begin: paddle.bitwise_right_shift(Tensor([0, 300],"int32"), Tensor([0, 300],"int32"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([0, 300],"int32"), Tensor([0, 300],"int32"), False, )
2025-03-03 17:02:30.614800 test begin: paddle.bitwise_right_shift(Tensor([0, 300],"int32"), Tensor([200, 300],"int32"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([0, 300],"int32"), Tensor([200, 300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 300] and the shape of Y = [200, 300]. Received [0] in X is not equal to [200] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.618885 test begin: paddle.bitwise_right_shift(Tensor([0, 300],"int32"), Tensor([300],"int32"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([0, 300],"int32"), Tensor([300],"int32"), False, )
2025-03-03 17:02:30.619835 test begin: paddle.bitwise_right_shift(Tensor([0],"int16"), Tensor([200, 300],"int16"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([0],"int16"), Tensor([200, 300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [200, 300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.621065 test begin: paddle.bitwise_right_shift(Tensor([0],"int32"), Tensor([200, 300],"int32"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([0],"int32"), Tensor([200, 300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [200, 300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.622335 test begin: paddle.bitwise_right_shift(Tensor([0],"int8"), Tensor([0],"int8"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([0],"int8"), Tensor([0],"int8"), False, )
2025-03-03 17:02:30.624365 test begin: paddle.bitwise_right_shift(Tensor([0],"int8"), Tensor([1],"int8"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([0],"int8"), Tensor([1],"int8"), False, )
2025-03-03 17:02:30.625729 test begin: paddle.bitwise_right_shift(Tensor([0],"uint8"), Tensor([0],"uint8"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([0],"uint8"), Tensor([0],"uint8"), False, )
2025-03-03 17:02:30.626836 test begin: paddle.bitwise_right_shift(Tensor([0],"uint8"), Tensor([1],"uint8"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([0],"uint8"), Tensor([1],"uint8"), False, )
2025-03-03 17:02:30.628181 test begin: paddle.bitwise_right_shift(Tensor([1],"int8"), Tensor([0],"int8"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([1],"int8"), Tensor([0],"int8"), False, )
2025-03-03 17:02:30.629419 test begin: paddle.bitwise_right_shift(Tensor([1],"uint8"), Tensor([0],"uint8"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([1],"uint8"), Tensor([0],"uint8"), False, )
2025-03-03 17:02:30.630512 test begin: paddle.bitwise_right_shift(Tensor([200, 0],"int16"), Tensor([200, 0],"int16"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([200, 0],"int16"), Tensor([200, 0],"int16"), False, )
2025-03-03 17:02:30.631528 test begin: paddle.bitwise_right_shift(Tensor([200, 0],"int16"), Tensor([200, 300],"int16"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 0],"int16"), Tensor([200, 300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 0] and the shape of Y = [200, 300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.633779 test begin: paddle.bitwise_right_shift(Tensor([200, 0],"int16"), Tensor([300],"int16"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 0],"int16"), Tensor([300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 0] and the shape of Y = [300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.635012 test begin: paddle.bitwise_right_shift(Tensor([200, 0],"int32"), Tensor([200, 0],"int32"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([200, 0],"int32"), Tensor([200, 0],"int32"), False, )
2025-03-03 17:02:30.636374 test begin: paddle.bitwise_right_shift(Tensor([200, 0],"int32"), Tensor([200, 300],"int32"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 0],"int32"), Tensor([200, 300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 0] and the shape of Y = [200, 300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.637853 test begin: paddle.bitwise_right_shift(Tensor([200, 0],"int32"), Tensor([300],"int32"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 0],"int32"), Tensor([300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 0] and the shape of Y = [300]. Received [0] in X is not equal to [300] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.638984 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int16"), Tensor([0, 300],"int16"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 300],"int16"), Tensor([0, 300],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [0, 300]. Received [200] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.640393 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int16"), Tensor([0],"int16"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 300],"int16"), Tensor([0],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.641779 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int16"), Tensor([200, 0],"int16"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 300],"int16"), Tensor([200, 0],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [200, 0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.643068 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int32"), Tensor([0, 300],"int32"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 300],"int32"), Tensor([0, 300],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [0, 300]. Received [200] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.644382 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int32"), Tensor([0],"int32"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 300],"int32"), Tensor([0],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.646074 test begin: paddle.bitwise_right_shift(Tensor([200, 300],"int32"), Tensor([200, 0],"int32"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([200, 300],"int32"), Tensor([200, 0],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [200, 300] and the shape of Y = [200, 0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.647651 test begin: paddle.bitwise_right_shift(Tensor([300],"int16"), Tensor([0, 300],"int16"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([300],"int16"), Tensor([0, 300],"int16"), False, )
2025-03-03 17:02:30.648902 test begin: paddle.bitwise_right_shift(Tensor([300],"int16"), Tensor([200, 0],"int16"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([300],"int16"), Tensor([200, 0],"int16"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [300] and the shape of Y = [200, 0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.650718 test begin: paddle.bitwise_right_shift(Tensor([300],"int32"), Tensor([0, 300],"int32"), False, )

[Pass] paddle.bitwise_right_shift(Tensor([300],"int32"), Tensor([0, 300],"int32"), False, )
2025-03-03 17:02:30.651786 test begin: paddle.bitwise_right_shift(Tensor([300],"int32"), Tensor([200, 0],"int32"), False, )

[paddle error] paddle.bitwise_right_shift(Tensor([300],"int32"), Tensor([200, 0],"int32"), False, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [300] and the shape of Y = [200, 0]. Received [300] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.652816 test begin: paddle.bitwise_xor(Tensor([0, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_xor(Tensor([0, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 4, 1] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.654024 test begin: paddle.bitwise_xor(Tensor([3, 0, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_xor(Tensor([3, 0, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.655778 test begin: paddle.bitwise_xor(Tensor([3, 4, 0],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_xor(Tensor([3, 4, 0],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 0] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.657045 test begin: paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([0, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([0, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.658986 test begin: paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 0, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 0, 1, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 1] and the shape of Y = [2, 0, 1, 5]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.660372 test begin: paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 0, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[paddle error] paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 0, 5],"int64"), out=Tensor([2, 3, 4, 5],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 4, 1] and the shape of Y = [2, 3, 0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:02:30.661742 test begin: paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 0],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 0],"int64"), out=Tensor([2, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.663752 test begin: paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([0, 3, 4, 5],"int64"), )

[Pass] paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([0, 3, 4, 5],"int64"), )
2025-03-03 17:02:30.665446 test begin: paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 0, 4, 5],"int64"), )

[Pass] paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 0, 4, 5],"int64"), )
2025-03-03 17:02:30.666797 test begin: paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 0, 5],"int64"), )

[Pass] paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 0, 5],"int64"), )
2025-03-03 17:02:30.668080 test begin: paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 0],"int64"), )

[Pass] paddle.bitwise_xor(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), out=Tensor([2, 3, 4, 0],"int64"), )
2025-03-03 17:02:30.670323 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 2, 6, 1],"bool"),Tensor([0, 6, 1, 1, 3],"bool"),Tensor([0, 1, 1, 6, 3],"bool"),Tensor([0, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 2, 6, 1],"bool"),Tensor([0, 6, 1, 1, 3],"bool"),Tensor([0, 1, 1, 6, 3],"bool"),Tensor([0, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.672915 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.676191 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 2, 6, 1],"float32"),Tensor([0, 6, 1, 1, 3],"float32"),Tensor([0, 1, 1, 6, 3],"float32"),Tensor([0, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 2, 6, 1],"float32"),Tensor([0, 6, 1, 1, 3],"float32"),Tensor([0, 1, 1, 6, 3],"float32"),Tensor([0, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.678505 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.681124 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 1],"bool"),Tensor([0, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 1],"bool"),Tensor([0, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.682867 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 1],"bool"),Tensor([6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 1],"bool"),Tensor([6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.684125 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 1],"float32"),Tensor([0, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 1],"float32"),Tensor([0, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.685452 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 1],"float32"),Tensor([6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 1],"float32"),Tensor([6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.686749 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 6, 1],"bool"),Tensor([0, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 6, 1],"bool"),Tensor([0, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.688020 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 6, 1],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 6, 1],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.689465 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 6, 1],"float32"),Tensor([0, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 6, 1],"float32"),Tensor([0, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.690708 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 6, 1],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6, 6, 1],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.692209 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6],"bool"),Tensor([0, 1, 6],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6],"bool"),Tensor([0, 1, 6],"bool"),], )
2025-03-03 17:02:30.693551 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6],"bool"),Tensor([6, 1, 6],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6],"bool"),Tensor([6, 1, 6],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.694753 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6],"float32"),Tensor([0, 1, 6],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6],"float32"),Tensor([0, 1, 6],"float32"),], )
2025-03-03 17:02:30.696320 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6, 6],"float32"),Tensor([6, 1, 6],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([0, 6, 6],"float32"),Tensor([6, 1, 6],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.697608 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6],"bool"),Tensor([0, 1],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6],"bool"),Tensor([0, 1],"bool"),], )
2025-03-03 17:02:30.699292 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6],"bool"),Tensor([6, 1],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([0, 6],"bool"),Tensor([6, 1],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.700315 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6],"float32"),Tensor([0, 1],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([0, 6],"float32"),Tensor([0, 1],"float32"),], )
2025-03-03 17:02:30.701682 test begin: paddle.broadcast_tensors(input=list[Tensor([0, 6],"float32"),Tensor([6, 1],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([0, 6],"float32"),Tensor([6, 1],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.702754 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 0, 2, 6, 1],"bool"),Tensor([1, 0, 1, 1, 3],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 0, 2, 6, 1],"bool"),Tensor([1, 0, 1, 1, 3],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.704416 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 0, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 0, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.706977 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 0, 2, 6, 1],"float32"),Tensor([1, 0, 1, 1, 3],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 0, 2, 6, 1],"float32"),Tensor([1, 0, 1, 1, 3],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.709179 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 0, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 0, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.710631 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 0, 6, 6, 1],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 0, 6, 6, 1],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.711907 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 0, 6, 6, 1],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 0, 6, 6, 1],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.712956 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 0, 6, 6, 1],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 0, 6, 6, 1],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.714163 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 0, 6, 6, 1],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 0, 6, 6, 1],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.715242 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"bool"),Tensor([1, 6, 0, 1, 3],"bool"),Tensor([1, 1, 0, 6, 3],"bool"),Tensor([1, 6, 0, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"bool"),Tensor([1, 6, 0, 1, 3],"bool"),Tensor([1, 1, 0, 6, 3],"bool"),Tensor([1, 6, 0, 6, 3],"bool"),], )
2025-03-03 17:02:30.716765 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"bool"),Tensor([1, 6, 0, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"bool"),Tensor([1, 6, 0, 6, 3],"bool"),], )
2025-03-03 17:02:30.717999 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.719819 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.722412 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"float32"),Tensor([1, 6, 0, 1, 3],"float32"),Tensor([1, 1, 0, 6, 3],"float32"),Tensor([1, 6, 0, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"float32"),Tensor([1, 6, 0, 1, 3],"float32"),Tensor([1, 1, 0, 6, 3],"float32"),Tensor([1, 6, 0, 6, 3],"float32"),], )
2025-03-03 17:02:30.724282 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"float32"),Tensor([1, 6, 0, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"float32"),Tensor([1, 6, 0, 6, 3],"float32"),], )
2025-03-03 17:02:30.725950 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.727887 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 0, 6, 1],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.729729 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 0, 1],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),Tensor([1, 1, 1, 0, 3],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 0, 1],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),Tensor([1, 1, 1, 0, 3],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),], )
2025-03-03 17:02:30.731234 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 0, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 0, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.732876 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 0, 1],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),Tensor([1, 1, 1, 0, 3],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 0, 1],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),Tensor([1, 1, 1, 0, 3],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),], )
2025-03-03 17:02:30.734333 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 0, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 0, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.749427 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 0],"bool"),Tensor([1, 6, 1, 1, 0],"bool"),Tensor([1, 1, 1, 6, 0],"bool"),Tensor([1, 6, 1, 6, 0],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 0],"bool"),Tensor([1, 6, 1, 1, 0],"bool"),Tensor([1, 1, 1, 6, 0],"bool"),Tensor([1, 6, 1, 6, 0],"bool"),], )
2025-03-03 17:02:30.752615 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 0],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 0],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.812278 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 0],"float32"),Tensor([1, 6, 1, 1, 0],"float32"),Tensor([1, 1, 1, 6, 0],"float32"),Tensor([1, 6, 1, 6, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 0],"float32"),Tensor([1, 6, 1, 1, 0],"float32"),Tensor([1, 1, 1, 6, 0],"float32"),Tensor([1, 6, 1, 6, 0],"float32"),], )
2025-03-03 17:02:30.860872 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 0],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 0],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.865240 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([0, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([0, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.869660 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 0, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 0, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.872295 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 0, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 0, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.887704 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.909011 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 0],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 0],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.911869 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([0, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([0, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.916274 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.919263 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 0, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 0, 6, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.921693 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 0, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 0, 3],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.925358 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 0],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 0],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.929350 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([0, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([0, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:30.933666 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.936537 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 0, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 0, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.939013 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.941392 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 0],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"bool"),Tensor([1, 6, 1, 1, 3],"bool"),Tensor([1, 1, 1, 6, 3],"bool"),Tensor([1, 6, 1, 6, 0],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.943931 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([0, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([0, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.947173 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 0, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 0, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.950116 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 0, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 0, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.952564 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.955114 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 0],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 0],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.957606 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([0, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([0, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.962040 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.965098 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 0, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 0, 6, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.967463 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 0, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 0, 3],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.969968 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 0],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 0],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.972367 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([0, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([0, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:30.977202 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.981637 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 0, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 0, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.985035 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.988719 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 0],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 2, 6, 1],"float32"),Tensor([1, 6, 1, 1, 3],"float32"),Tensor([1, 1, 1, 6, 3],"float32"),Tensor([1, 6, 1, 6, 0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.992442 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 0, 1],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 0, 1],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),], )
2025-03-03 17:02:30.995641 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 0, 1],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 0, 1],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:30.998239 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 0, 1],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 0, 1],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),], )
2025-03-03 17:02:31.001891 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 0, 1],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 0, 1],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.004442 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 0],"bool"),Tensor([1, 6, 1, 6, 0],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 0],"bool"),Tensor([1, 6, 1, 6, 0],"bool"),], )
2025-03-03 17:02:31.008028 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 0],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 0],"bool"),Tensor([1, 6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.010534 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 0],"float32"),Tensor([1, 6, 1, 6, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 0],"float32"),Tensor([1, 6, 1, 6, 0],"float32"),], )
2025-03-03 17:02:31.012965 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 0],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 0],"float32"),Tensor([1, 6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.015460 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([0, 6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([0, 6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:31.018512 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([1, 0, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.022078 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([1, 6, 0, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([1, 6, 0, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.025746 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([1, 6, 1, 0, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.029216 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([1, 6, 1, 6, 0],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"bool"),Tensor([1, 6, 1, 6, 0],"bool"),], )
2025-03-03 17:02:31.032261 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([0, 6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([0, 6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:31.035990 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([1, 0, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.039325 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([1, 6, 0, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([1, 6, 0, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.041534 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([1, 6, 1, 0, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.044061 test begin: paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([1, 6, 1, 6, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([1, 6, 6, 6, 1],"float32"),Tensor([1, 6, 1, 6, 0],"float32"),], )
2025-03-03 17:02:31.046979 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0, 6, 1],"bool"),Tensor([6, 0, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0, 6, 1],"bool"),Tensor([6, 0, 6, 3],"bool"),], )
2025-03-03 17:02:31.049990 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0, 6, 1],"bool"),Tensor([6, 1, 6, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0, 6, 1],"bool"),Tensor([6, 1, 6, 3],"bool"),], )
2025-03-03 17:02:31.053127 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0, 6, 1],"float32"),Tensor([6, 0, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0, 6, 1],"float32"),Tensor([6, 0, 6, 3],"float32"),], )
2025-03-03 17:02:31.056299 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0, 6, 1],"float32"),Tensor([6, 1, 6, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0, 6, 1],"float32"),Tensor([6, 1, 6, 3],"float32"),], )
2025-03-03 17:02:31.059302 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0, 6],"bool"),Tensor([6, 0, 6],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0, 6],"bool"),Tensor([6, 0, 6],"bool"),], )
2025-03-03 17:02:31.062510 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0, 6],"bool"),Tensor([6, 1, 6],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0, 6],"bool"),Tensor([6, 1, 6],"bool"),], )
2025-03-03 17:02:31.065417 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0, 6],"float32"),Tensor([6, 0, 6],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0, 6],"float32"),Tensor([6, 0, 6],"float32"),], )
2025-03-03 17:02:31.068577 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0, 6],"float32"),Tensor([6, 1, 6],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0, 6],"float32"),Tensor([6, 1, 6],"float32"),], )
2025-03-03 17:02:31.071569 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0],"bool"),Tensor([6, 0],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0],"bool"),Tensor([6, 0],"bool"),], )
2025-03-03 17:02:31.074781 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0],"bool"),Tensor([6, 1],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0],"bool"),Tensor([6, 1],"bool"),], )
2025-03-03 17:02:31.077856 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0],"float32"),Tensor([6, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0],"float32"),Tensor([6, 0],"float32"),], )
2025-03-03 17:02:31.081066 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 0],"float32"),Tensor([6, 1],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 0],"float32"),Tensor([6, 1],"float32"),], )
2025-03-03 17:02:31.084503 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 0, 1],"bool"),Tensor([6, 1, 0, 3],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 6, 0, 1],"bool"),Tensor([6, 1, 0, 3],"bool"),], )
2025-03-03 17:02:31.087221 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 0, 1],"bool"),Tensor([6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 0, 1],"bool"),Tensor([6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.089833 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 0, 1],"float32"),Tensor([6, 1, 0, 3],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 6, 0, 1],"float32"),Tensor([6, 1, 0, 3],"float32"),], )
2025-03-03 17:02:31.092240 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 0, 1],"float32"),Tensor([6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 0, 1],"float32"),Tensor([6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.094768 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 0],"bool"),Tensor([6, 1, 0],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 6, 0],"bool"),Tensor([6, 1, 0],"bool"),], )
2025-03-03 17:02:31.097107 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 0],"bool"),Tensor([6, 1, 6],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 0],"bool"),Tensor([6, 1, 6],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.100004 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 0],"float32"),Tensor([6, 1, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 6, 0],"float32"),Tensor([6, 1, 0],"float32"),], )
2025-03-03 17:02:31.103472 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 0],"float32"),Tensor([6, 1, 6],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 0],"float32"),Tensor([6, 1, 6],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.107304 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 0],"bool"),Tensor([6, 1, 6, 0],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 0],"bool"),Tensor([6, 1, 6, 0],"bool"),], )
2025-03-03 17:02:31.110950 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 0],"bool"),Tensor([6, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 0],"bool"),Tensor([6, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.113531 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 0],"float32"),Tensor([6, 1, 6, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 0],"float32"),Tensor([6, 1, 6, 0],"float32"),], )
2025-03-03 17:02:31.117082 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 0],"float32"),Tensor([6, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 0],"float32"),Tensor([6, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.121036 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"bool"),Tensor([0, 1, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"bool"),Tensor([0, 1, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.123697 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"bool"),Tensor([6, 0, 6, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"bool"),Tensor([6, 0, 6, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.127300 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"bool"),Tensor([6, 1, 0, 3],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"bool"),Tensor([6, 1, 0, 3],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.129706 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"bool"),Tensor([6, 1, 6, 0],"bool"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"bool"),Tensor([6, 1, 6, 0],"bool"),], )
2025-03-03 17:02:31.132704 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"float32"),Tensor([0, 1, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"float32"),Tensor([0, 1, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.135997 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"float32"),Tensor([6, 0, 6, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"float32"),Tensor([6, 0, 6, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.138364 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"float32"),Tensor([6, 1, 0, 3],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"float32"),Tensor([6, 1, 0, 3],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.140982 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"float32"),Tensor([6, 1, 6, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6, 1],"float32"),Tensor([6, 1, 6, 0],"float32"),], )
2025-03-03 17:02:31.145310 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"bool"),Tensor([0, 1, 6],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"bool"),Tensor([0, 1, 6],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.148546 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"bool"),Tensor([6, 0, 6],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"bool"),Tensor([6, 0, 6],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.152091 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"bool"),Tensor([6, 1, 0],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"bool"),Tensor([6, 1, 0],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.154459 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"float32"),Tensor([0, 1, 6],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"float32"),Tensor([0, 1, 6],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.158509 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"float32"),Tensor([6, 0, 6],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"float32"),Tensor([6, 0, 6],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.162063 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"float32"),Tensor([6, 1, 0],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6, 6],"float32"),Tensor([6, 1, 0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.164677 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6],"bool"),Tensor([0, 1],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6],"bool"),Tensor([0, 1],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.168375 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6],"bool"),Tensor([6, 0],"bool"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6],"bool"),Tensor([6, 0],"bool"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.170769 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6],"float32"),Tensor([0, 1],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6],"float32"),Tensor([0, 1],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.173281 test begin: paddle.broadcast_tensors(input=list[Tensor([6, 6],"float32"),Tensor([6, 0],"float32"),], )

[paddle error] paddle.broadcast_tensors(input=list[Tensor([6, 6],"float32"),Tensor([6, 0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.175718 test begin: paddle.broadcast_tensors(list[Tensor([0, 1, 10, 1],"float64"),Tensor([0, 1, 10, 1],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 1, 10, 1],"float64"),Tensor([0, 1, 10, 1],"float64"),], )
2025-03-03 17:02:31.179219 test begin: paddle.broadcast_tensors(list[Tensor([0, 1, 10, 1],"float64"),Tensor([12, 1, 10, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0, 1, 10, 1],"float64"),Tensor([12, 1, 10, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.181812 test begin: paddle.broadcast_tensors(list[Tensor([0, 1, 2],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 1, 2],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:31.184900 test begin: paddle.broadcast_tensors(list[Tensor([0, 1, 4, 1],"complex128"),Tensor([0, 4, 1, 4],"complex128"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 1, 4, 1],"complex128"),Tensor([0, 4, 1, 4],"complex128"),], )
2025-03-03 17:02:31.187823 test begin: paddle.broadcast_tensors(list[Tensor([0, 1, 4, 1],"complex128"),Tensor([1, 4, 1, 4],"complex128"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 1, 4, 1],"complex128"),Tensor([1, 4, 1, 4],"complex128"),], )
2025-03-03 17:02:31.190896 test begin: paddle.broadcast_tensors(list[Tensor([0, 10, 10],"float64"),Tensor([0, 10, 10],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 10, 10],"float64"),Tensor([0, 10, 10],"float64"),], )
2025-03-03 17:02:31.194018 test begin: paddle.broadcast_tensors(list[Tensor([0, 10, 10],"float64"),Tensor([10, 10, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0, 10, 10],"float64"),Tensor([10, 10, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.196740 test begin: paddle.broadcast_tensors(list[Tensor([0, 1],"float32"),Tensor([0, 1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 1],"float32"),Tensor([0, 1],"float32"),], )
2025-03-03 17:02:31.200346 test begin: paddle.broadcast_tensors(list[Tensor([0, 1],"float32"),Tensor([1, 1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 1],"float32"),Tensor([1, 1],"float32"),], )
2025-03-03 17:02:31.203432 test begin: paddle.broadcast_tensors(list[Tensor([0, 2, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 4],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 2, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 4],"float32"),], )
2025-03-03 17:02:31.207391 test begin: paddle.broadcast_tensors(list[Tensor([0, 2],"float32"),Tensor([0, 2],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 2],"float32"),Tensor([0, 2],"float32"),], )
2025-03-03 17:02:31.210146 test begin: paddle.broadcast_tensors(list[Tensor([0, 2],"float32"),Tensor([1, 2],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 2],"float32"),Tensor([1, 2],"float32"),], )
2025-03-03 17:02:31.214867 test begin: paddle.broadcast_tensors(list[Tensor([0, 2],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 2],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:31.219729 test begin: paddle.broadcast_tensors(list[Tensor([0, 4],"float32"),Tensor([0, 1],"float32"),Tensor([0, 4],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0, 4],"float32"),Tensor([0, 1],"float32"),Tensor([0, 4],"float32"),], )
2025-03-03 17:02:31.223910 test begin: paddle.broadcast_tensors(list[Tensor([0, 4],"float32"),Tensor([3, 1],"float32"),Tensor([3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0, 4],"float32"),Tensor([3, 1],"float32"),Tensor([3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.228098 test begin: paddle.broadcast_tensors(list[Tensor([0, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.231723 test begin: paddle.broadcast_tensors(list[Tensor([0, 60, 1],"float64"),Tensor([6, 2, 1, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0, 60, 1],"float64"),Tensor([6, 2, 1, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.235266 test begin: paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:31.240124 test begin: paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:31.243766 test begin: paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([1, 2],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([1, 2],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.247754 test begin: paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:31.252162 test begin: paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:31.257998 test begin: paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([2, 2],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0],"float32"),Tensor([2, 2],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.261252 test begin: paddle.broadcast_tensors(list[Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([0],"float64"),Tensor([0],"float64"),Tensor([0],"float64"),], )
2025-03-03 17:02:31.278647 test begin: paddle.broadcast_tensors(list[Tensor([0],"float64"),Tensor([2],"float64"),Tensor([2, 2],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0],"float64"),Tensor([2],"float64"),Tensor([2, 2],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.285885 test begin: paddle.broadcast_tensors(list[Tensor([0],"float64"),Tensor([2],"float64"),Tensor([2],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([0],"float64"),Tensor([2],"float64"),Tensor([2],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.292186 test begin: paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([1, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([1, 0],"float32"),], )
2025-03-03 17:02:31.297570 test begin: paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([1, 1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([1, 1],"float32"),], )
2025-03-03 17:02:31.299150 test begin: paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([1, 2],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([1, 2],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.300296 test begin: paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([3, 0],"float32"),Tensor([3, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([3, 0],"float32"),Tensor([3, 0],"float32"),], )
2025-03-03 17:02:31.301665 test begin: paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([3, 1],"float32"),Tensor([3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([1, 0],"float32"),Tensor([3, 1],"float32"),Tensor([3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.302995 test begin: paddle.broadcast_tensors(list[Tensor([1, 1],"float32"),Tensor([0, 1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1, 1],"float32"),Tensor([0, 1],"float32"),], )
2025-03-03 17:02:31.304640 test begin: paddle.broadcast_tensors(list[Tensor([1, 1],"float32"),Tensor([1, 0],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1, 1],"float32"),Tensor([1, 0],"float32"),], )
2025-03-03 17:02:31.306818 test begin: paddle.broadcast_tensors(list[Tensor([1, 2],"float32"),Tensor([0, 2],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1, 2],"float32"),Tensor([0, 2],"float32"),], )
2025-03-03 17:02:31.308460 test begin: paddle.broadcast_tensors(list[Tensor([1, 2],"float32"),Tensor([1, 0],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([1, 2],"float32"),Tensor([1, 0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.309559 test begin: paddle.broadcast_tensors(list[Tensor([1, 4],"float32"),Tensor([0, 1],"float32"),Tensor([3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([1, 4],"float32"),Tensor([0, 1],"float32"),Tensor([3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.310904 test begin: paddle.broadcast_tensors(list[Tensor([1, 4],"float32"),Tensor([3, 0],"float32"),Tensor([3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([1, 4],"float32"),Tensor([3, 0],"float32"),Tensor([3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.313505 test begin: paddle.broadcast_tensors(list[Tensor([1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([0, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([0, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.314790 test begin: paddle.broadcast_tensors(list[Tensor([1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([3, 0],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([3, 0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.316356 test begin: paddle.broadcast_tensors(list[Tensor([10, 0, 10],"float64"),Tensor([10, 0, 10],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([10, 0, 10],"float64"),Tensor([10, 0, 10],"float64"),], )
2025-03-03 17:02:31.317848 test begin: paddle.broadcast_tensors(list[Tensor([10, 0, 10],"float64"),Tensor([10, 10, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([10, 0, 10],"float64"),Tensor([10, 10, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.319035 test begin: paddle.broadcast_tensors(list[Tensor([10, 10, 0],"float64"),Tensor([10, 10, 0],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([10, 10, 0],"float64"),Tensor([10, 10, 0],"float64"),], )
2025-03-03 17:02:31.320305 test begin: paddle.broadcast_tensors(list[Tensor([10, 10, 0],"float64"),Tensor([10, 10, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([10, 10, 0],"float64"),Tensor([10, 10, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.321379 test begin: paddle.broadcast_tensors(list[Tensor([10, 10, 10],"float64"),Tensor([0, 10, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([10, 10, 10],"float64"),Tensor([0, 10, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.322811 test begin: paddle.broadcast_tensors(list[Tensor([10, 10, 10],"float64"),Tensor([10, 0, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([10, 10, 10],"float64"),Tensor([10, 0, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.323867 test begin: paddle.broadcast_tensors(list[Tensor([10, 10, 10],"float64"),Tensor([10, 10, 0],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([10, 10, 10],"float64"),Tensor([10, 10, 0],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.324940 test begin: paddle.broadcast_tensors(list[Tensor([12, 0, 10, 1],"float64"),Tensor([12, 0, 10, 1],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([12, 0, 10, 1],"float64"),Tensor([12, 0, 10, 1],"float64"),], )
2025-03-03 17:02:31.326196 test begin: paddle.broadcast_tensors(list[Tensor([12, 0, 10, 1],"float64"),Tensor([12, 1, 10, 1],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([12, 0, 10, 1],"float64"),Tensor([12, 1, 10, 1],"float64"),], )
2025-03-03 17:02:31.327828 test begin: paddle.broadcast_tensors(list[Tensor([12, 1, 0, 1],"float64"),Tensor([12, 1, 0, 1],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([12, 1, 0, 1],"float64"),Tensor([12, 1, 0, 1],"float64"),], )
2025-03-03 17:02:31.329074 test begin: paddle.broadcast_tensors(list[Tensor([12, 1, 0, 1],"float64"),Tensor([12, 1, 10, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([12, 1, 0, 1],"float64"),Tensor([12, 1, 10, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.330186 test begin: paddle.broadcast_tensors(list[Tensor([12, 1, 10, 0],"float64"),Tensor([12, 1, 10, 0],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([12, 1, 10, 0],"float64"),Tensor([12, 1, 10, 0],"float64"),], )
2025-03-03 17:02:31.331399 test begin: paddle.broadcast_tensors(list[Tensor([12, 1, 10, 0],"float64"),Tensor([12, 1, 10, 1],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([12, 1, 10, 0],"float64"),Tensor([12, 1, 10, 1],"float64"),], )
2025-03-03 17:02:31.332881 test begin: paddle.broadcast_tensors(list[Tensor([12, 1, 10, 1],"float64"),Tensor([0, 1, 10, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([12, 1, 10, 1],"float64"),Tensor([0, 1, 10, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.334115 test begin: paddle.broadcast_tensors(list[Tensor([12, 1, 10, 1],"float64"),Tensor([12, 0, 10, 1],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([12, 1, 10, 1],"float64"),Tensor([12, 0, 10, 1],"float64"),], )
2025-03-03 17:02:31.335686 test begin: paddle.broadcast_tensors(list[Tensor([12, 1, 10, 1],"float64"),Tensor([12, 1, 0, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([12, 1, 10, 1],"float64"),Tensor([12, 1, 0, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.336826 test begin: paddle.broadcast_tensors(list[Tensor([12, 1, 10, 1],"float64"),Tensor([12, 1, 10, 0],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([12, 1, 10, 1],"float64"),Tensor([12, 1, 10, 0],"float64"),], )
2025-03-03 17:02:31.338202 test begin: paddle.broadcast_tensors(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:31.340004 test begin: paddle.broadcast_tensors(list[Tensor([1],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:31.341393 test begin: paddle.broadcast_tensors(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )
2025-03-03 17:02:31.343086 test begin: paddle.broadcast_tensors(list[Tensor([2, 0, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 0, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.344322 test begin: paddle.broadcast_tensors(list[Tensor([2, 0, 1],"float64"),Tensor([6, 2, 1, 10],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2, 0, 1],"float64"),Tensor([6, 2, 1, 10],"float64"),], )
2025-03-03 17:02:31.345846 test begin: paddle.broadcast_tensors(list[Tensor([2, 0, 2],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2, 0, 2],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:31.347255 test begin: paddle.broadcast_tensors(list[Tensor([2, 0],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2, 0],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:31.348639 test begin: paddle.broadcast_tensors(list[Tensor([2, 1, 0],"float32"),Tensor([1],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2, 1, 0],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:31.350099 test begin: paddle.broadcast_tensors(list[Tensor([2, 1, 2],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 1, 2],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.351212 test begin: paddle.broadcast_tensors(list[Tensor([2, 2],"float32"),Tensor([0],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 2],"float32"),Tensor([0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.352245 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 0],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 0],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.353403 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 0],"float64"),Tensor([6, 2, 1, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 0],"float64"),Tensor([6, 2, 1, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.354444 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([0, 2, 1, 10],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([0, 2, 1, 10],"float64"),], )
2025-03-03 17:02:31.355926 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([0, 2, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([0, 2, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], )
2025-03-03 17:02:31.358096 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 0, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 0, 1, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.359682 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 0, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 0, 30],"float64"),Tensor([1, 2, 60, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.361374 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 0],"float64"),Tensor([1, 2, 60, 1],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 0],"float64"),Tensor([1, 2, 60, 1],"float64"),], )
2025-03-03 17:02:31.363786 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([0, 2, 60, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([0, 2, 60, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.365498 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 0, 60, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 0, 60, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.367008 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 0, 1],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 0, 1],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.368687 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 60, 0],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([2, 2, 1, 30],"float64"),Tensor([1, 2, 60, 0],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.370166 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([6, 0, 1, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([6, 0, 1, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.371220 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([6, 2, 0, 10],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([6, 2, 0, 10],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.372317 test begin: paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([6, 2, 1, 0],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2, 60, 1],"float64"),Tensor([6, 2, 1, 0],"float64"),], )
2025-03-03 17:02:31.373876 test begin: paddle.broadcast_tensors(list[Tensor([2],"float32"),Tensor([0, 2],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2],"float32"),Tensor([0, 2],"float32"),], )
2025-03-03 17:02:31.375379 test begin: paddle.broadcast_tensors(list[Tensor([2],"float32"),Tensor([1, 0],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2],"float32"),Tensor([1, 0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.376592 test begin: paddle.broadcast_tensors(list[Tensor([2],"float32"),Tensor([2, 0],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2],"float32"),Tensor([2, 0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.377614 test begin: paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([0],"float64"),Tensor([2, 2],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([0],"float64"),Tensor([2, 2],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.378846 test begin: paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([0],"float64"),Tensor([2],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([0],"float64"),Tensor([2],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.393779 test begin: paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([2],"float64"),Tensor([0, 2],"float64"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([2],"float64"),Tensor([0, 2],"float64"),], )
2025-03-03 17:02:31.408578 test begin: paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([2],"float64"),Tensor([0],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([2],"float64"),Tensor([0],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.415156 test begin: paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([2],"float64"),Tensor([2, 0],"float64"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([2],"float64"),Tensor([2],"float64"),Tensor([2, 0],"float64"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.417034 test begin: paddle.broadcast_tensors(list[Tensor([4, 0, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 4],"float32"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([4, 0, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 4],"float32"),], )
2025-03-03 17:02:31.418999 test begin: paddle.broadcast_tensors(list[Tensor([4, 0, 4, 1],"complex128"),Tensor([1, 0, 1, 4],"complex128"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([4, 0, 4, 1],"complex128"),Tensor([1, 0, 1, 4],"complex128"),], )
2025-03-03 17:02:31.420605 test begin: paddle.broadcast_tensors(list[Tensor([4, 0, 4, 1],"complex128"),Tensor([1, 4, 1, 4],"complex128"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 0, 4, 1],"complex128"),Tensor([1, 4, 1, 4],"complex128"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.422217 test begin: paddle.broadcast_tensors(list[Tensor([4, 1, 0, 1],"complex128"),Tensor([1, 4, 0, 4],"complex128"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([4, 1, 0, 1],"complex128"),Tensor([1, 4, 0, 4],"complex128"),], )
2025-03-03 17:02:31.424611 test begin: paddle.broadcast_tensors(list[Tensor([4, 1, 0, 1],"complex128"),Tensor([1, 4, 1, 4],"complex128"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([4, 1, 0, 1],"complex128"),Tensor([1, 4, 1, 4],"complex128"),], )
2025-03-03 17:02:31.426286 test begin: paddle.broadcast_tensors(list[Tensor([4, 1, 4, 0],"complex128"),Tensor([1, 4, 1, 0],"complex128"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([4, 1, 4, 0],"complex128"),Tensor([1, 4, 1, 0],"complex128"),], )
2025-03-03 17:02:31.428092 test begin: paddle.broadcast_tensors(list[Tensor([4, 1, 4, 0],"complex128"),Tensor([1, 4, 1, 4],"complex128"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 1, 4, 0],"complex128"),Tensor([1, 4, 1, 4],"complex128"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.429287 test begin: paddle.broadcast_tensors(list[Tensor([4, 1, 4, 1],"complex128"),Tensor([0, 4, 1, 4],"complex128"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 1, 4, 1],"complex128"),Tensor([0, 4, 1, 4],"complex128"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 3 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.430395 test begin: paddle.broadcast_tensors(list[Tensor([4, 1, 4, 1],"complex128"),Tensor([1, 0, 1, 4],"complex128"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([4, 1, 4, 1],"complex128"),Tensor([1, 0, 1, 4],"complex128"),], )
2025-03-03 17:02:31.432679 test begin: paddle.broadcast_tensors(list[Tensor([4, 1, 4, 1],"complex128"),Tensor([1, 4, 0, 4],"complex128"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 1, 4, 1],"complex128"),Tensor([1, 4, 0, 4],"complex128"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.434008 test begin: paddle.broadcast_tensors(list[Tensor([4, 1, 4, 1],"complex128"),Tensor([1, 4, 1, 0],"complex128"),], )

[Pass] paddle.broadcast_tensors(list[Tensor([4, 1, 4, 1],"complex128"),Tensor([1, 4, 1, 0],"complex128"),], )
2025-03-03 17:02:31.436651 test begin: paddle.broadcast_tensors(list[Tensor([4, 2, 0, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 2, 0, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.439166 test begin: paddle.broadcast_tensors(list[Tensor([4, 2, 1, 0],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 2, 1, 0],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.442654 test begin: paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([0, 1],"float32"),Tensor([1, 3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([0, 1],"float32"),Tensor([1, 3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.445610 test begin: paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([3, 0],"float32"),Tensor([1, 3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([3, 0],"float32"),Tensor([1, 3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.448542 test begin: paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([0, 3, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([0, 3, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 2 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.450735 test begin: paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 0, 4],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 0, 4],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.452639 test begin: paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 0],"float32"),], )

[paddle error] paddle.broadcast_tensors(list[Tensor([4, 2, 1, 4],"float32"),Tensor([3, 1],"float32"),Tensor([1, 3, 0],"float32"),], ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.454593 test begin: paddle.broadcast_tensors(tuple(Tensor([0, 1],"float32"),Tensor([0, 3],"float32"),Tensor([0, 3],"float32"),), )

[Pass] paddle.broadcast_tensors(tuple(Tensor([0, 1],"float32"),Tensor([0, 3],"float32"),Tensor([0, 3],"float32"),), )
2025-03-03 17:02:31.456802 test begin: paddle.broadcast_tensors(tuple(Tensor([0, 1],"float32"),Tensor([2, 3],"float32"),Tensor([2, 3],"float32"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([0, 1],"float32"),Tensor([2, 3],"float32"),Tensor([2, 3],"float32"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.458639 test begin: paddle.broadcast_tensors(tuple(Tensor([0, 1],"float64"),Tensor([0, 1],"float64"),Tensor([0, 1],"float64"),), )

[Pass] paddle.broadcast_tensors(tuple(Tensor([0, 1],"float64"),Tensor([0, 1],"float64"),Tensor([0, 1],"float64"),), )
2025-03-03 17:02:31.460418 test begin: paddle.broadcast_tensors(tuple(Tensor([0, 1],"float64"),Tensor([2, 1],"float64"),Tensor([2, 1],"float64"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([0, 1],"float64"),Tensor([2, 1],"float64"),Tensor([2, 1],"float64"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.462178 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 0],"float32"),Tensor([2, 0],"float32"),Tensor([2, 0],"float32"),), )

[Pass] paddle.broadcast_tensors(tuple(Tensor([2, 0],"float32"),Tensor([2, 0],"float32"),Tensor([2, 0],"float32"),), )
2025-03-03 17:02:31.465158 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 0],"float32"),Tensor([2, 3],"float32"),Tensor([2, 3],"float32"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([2, 0],"float32"),Tensor([2, 3],"float32"),Tensor([2, 3],"float32"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.467314 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 0],"float64"),Tensor([2, 0],"float64"),Tensor([2, 0],"float64"),), )

[Pass] paddle.broadcast_tensors(tuple(Tensor([2, 0],"float64"),Tensor([2, 0],"float64"),Tensor([2, 0],"float64"),), )
2025-03-03 17:02:31.469248 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 0],"float64"),Tensor([2, 1],"float64"),Tensor([2, 1],"float64"),), )

[Pass] paddle.broadcast_tensors(tuple(Tensor([2, 0],"float64"),Tensor([2, 1],"float64"),Tensor([2, 1],"float64"),), )
2025-03-03 17:02:31.470918 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 1],"float32"),Tensor([0, 3],"float32"),Tensor([2, 3],"float32"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([2, 1],"float32"),Tensor([0, 3],"float32"),Tensor([2, 3],"float32"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.472898 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 1],"float32"),Tensor([2, 0],"float32"),Tensor([2, 3],"float32"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([2, 1],"float32"),Tensor([2, 0],"float32"),Tensor([2, 3],"float32"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.474879 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 1],"float32"),Tensor([2, 3],"float32"),Tensor([0, 3],"float32"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([2, 1],"float32"),Tensor([2, 3],"float32"),Tensor([0, 3],"float32"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.477145 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 1],"float32"),Tensor([2, 3],"float32"),Tensor([2, 0],"float32"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([2, 1],"float32"),Tensor([2, 3],"float32"),Tensor([2, 0],"float32"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 0 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.478522 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 1],"float64"),Tensor([0, 1],"float64"),Tensor([2, 1],"float64"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([2, 1],"float64"),Tensor([0, 1],"float64"),Tensor([2, 1],"float64"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.479809 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 1],"float64"),Tensor([2, 0],"float64"),Tensor([2, 1],"float64"),), )

[Pass] paddle.broadcast_tensors(tuple(Tensor([2, 1],"float64"),Tensor([2, 0],"float64"),Tensor([2, 1],"float64"),), )
2025-03-03 17:02:31.482133 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 1],"float64"),Tensor([2, 1],"float64"),Tensor([0, 1],"float64"),), )

[paddle error] paddle.broadcast_tensors(tuple(Tensor([2, 1],"float64"),Tensor([2, 1],"float64"),Tensor([0, 1],"float64"),), ) 
 (InvalidArgument) BroadcastTensorsOp inputs does not satisfy bcast semantics, please check axis = 1 in reverse order (at ../paddle/phi/infermeta/multiary.cc:1123)

2025-03-03 17:02:31.483407 test begin: paddle.broadcast_tensors(tuple(Tensor([2, 1],"float64"),Tensor([2, 1],"float64"),Tensor([2, 0],"float64"),), )

[Pass] paddle.broadcast_tensors(tuple(Tensor([2, 1],"float64"),Tensor([2, 1],"float64"),Tensor([2, 0],"float64"),), )
2025-03-03 17:02:31.485331 test begin: paddle.cartesian_prod(list[Tensor([0],"complex128"),Tensor([0],"complex128"),Tensor([0],"complex128"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   MeshgridGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::meshgrid_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::MeshgridGradKernel<phi::dtype::complex<double>, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::MeshgridGradKernel<phi::dtype::complex<double>, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
6   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, phi::dtype::complex<double>, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<phi::dtype::complex<double>, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<phi::dtype::complex<double> const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992551 (unix time) try "date -d @1740992551" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f93810590ef) received by PID 162115 (TID 0x7f9321dc2700) from PID 18446744071579209967 ***]

2025-03-03 17:02:43.196453 test begin: paddle.cartesian_prod(list[Tensor([0],"complex128"),Tensor([1],"complex128"),Tensor([3],"complex128"),], )

W0303 17:02:46.629031 25223 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:02:46.630134 25223 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.cartesian_prod(list[Tensor([0],"complex128"),Tensor([1],"complex128"),Tensor([3],"complex128"),], )
2025-03-03 17:02:46.813349 test begin: paddle.cartesian_prod(list[Tensor([0],"complex128"),], )

[Pass] paddle.cartesian_prod(list[Tensor([0],"complex128"),], )
2025-03-03 17:02:46.816244 test begin: paddle.cartesian_prod(list[Tensor([0],"float32"),Tensor([0],"float32"),Tensor([0],"float32"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   MeshgridGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::meshgrid_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::MeshgridGradKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::MeshgridGradKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
6   void phi::MeshgridBackward<float, phi::GPUContext, 3>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
7   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)
8   Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 1> const, Eigen::TensorReductionOp<Eigen::internal::SumReducer<float>, Eigen::DSizes<long, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 6> const, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer> const> const, Eigen::MakePointer> const> const> const, Eigen::GpuDevice, false, (Eigen::internal::TiledEvaluation)0>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 1> const, Eigen::TensorReductionOp<Eigen::internal::SumReducer<float>, Eigen::DSizes<long, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 6> const, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer> const> const, Eigen::MakePointer> const> const> const&, Eigen::GpuDevice const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992566 (unix time) try "date -d @1740992566" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f62e755c0ef) received by PID 16720 (TID 0x7f6275f48700) from PID 18446744073295741167 ***]

2025-03-03 17:02:51.425274 test begin: paddle.cartesian_prod(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )

W0303 17:02:54.833161 50023 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:02:54.834239 50023 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.cartesian_prod(list[Tensor([0],"float32"),Tensor([1],"float32"),Tensor([1],"float32"),], )
2025-03-03 17:02:54.883957 test begin: paddle.cartesian_prod(list[Tensor([0],"float32"),], )

[Pass] paddle.cartesian_prod(list[Tensor([0],"float32"),], )
2025-03-03 17:02:54.930755 test begin: paddle.cartesian_prod(list[Tensor([1],"float32"),Tensor([0],"float32"),Tensor([1],"float32"),], )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   MeshgridGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::meshgrid_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::MeshgridGradKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::MeshgridGradKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
6   void phi::MeshgridBackward<float, phi::GPUContext, 3>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
7   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)
8   Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 1> const, Eigen::TensorReductionOp<Eigen::internal::SumReducer<float>, Eigen::DSizes<long, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 6> const, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer> const> const, Eigen::MakePointer> const> const> const, Eigen::GpuDevice, false, (Eigen::internal::TiledEvaluation)0>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 1> const, Eigen::TensorReductionOp<Eigen::internal::SumReducer<float>, Eigen::DSizes<long, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 6> const, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer> const> const, Eigen::MakePointer> const> const> const&, Eigen::GpuDevice const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992574 (unix time) try "date -d @1740992574" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f61c79a80ef) received by PID 36815 (TID 0x7f6138949700) from PID 18446744072763375855 ***]

2025-03-03 17:03:00.653017 test begin: paddle.cartesian_prod(list[Tensor([1],"float32"),Tensor([1],"float32"),Tensor([0],"float32"),], )

W0303 17:03:03.399305 78406 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:03:03.400944 78406 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   MeshgridGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::meshgrid_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::MeshgridGradKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::MeshgridGradKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
6   void phi::MeshgridBackward<float, phi::GPUContext, 3>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
7   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, float, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)
8   Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 1> const, Eigen::TensorReductionOp<Eigen::internal::SumReducer<float>, Eigen::DSizes<long, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 6> const, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer> const> const, Eigen::MakePointer> const> const> const, Eigen::GpuDevice, false, (Eigen::internal::TiledEvaluation)0>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 1> const, Eigen::TensorReductionOp<Eigen::internal::SumReducer<float>, Eigen::DSizes<long, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 6> const, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 0, Eigen::MakePointer> const> const, Eigen::MakePointer> const> const> const&, Eigen::GpuDevice const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992583 (unix time) try "date -d @1740992583" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb88d1460ef) received by PID 66112 (TID 0x7fb7f9dc2700) from PID 18446744071781507311 ***]

2025-03-03 17:03:07.990939 test begin: paddle.cartesian_prod(list[Tensor([2],"complex128"),Tensor([0],"complex128"),Tensor([3],"complex128"),], )

W0303 17:03:11.286893 95416 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:03:11.287868 95416 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   MeshgridGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::meshgrid_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::MeshgridGradKernel<phi::dtype::complex<double>, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::MeshgridGradKernel<phi::dtype::complex<double>, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
6   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, phi::dtype::complex<double>, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<phi::dtype::complex<double>, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<phi::dtype::complex<double> const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992591 (unix time) try "date -d @1740992591" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7a13e410ef) received by PID 88114 (TID 0x7f7958949700) from PID 333713647 ***]

2025-03-03 17:03:22.437323 test begin: paddle.cartesian_prod(list[Tensor([2],"complex128"),Tensor([1],"complex128"),Tensor([0],"complex128"),], )

W0303 17:03:25.807881 115991 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:03:25.808939 115991 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   MeshgridGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::meshgrid_grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor*, std::allocator<paddle::Tensor*> >)
4   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::MeshgridGradKernel<phi::dtype::complex<double>, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::MeshgridGradKernel<phi::dtype::complex<double>, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
6   phi::funcs::EigenBroadcastGrad<Eigen::GpuDevice, phi::dtype::complex<double>, 3>::Eval(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<phi::dtype::complex<double>, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<phi::dtype::complex<double> const, 1, 1, long>, 0, Eigen::MakePointer>, Eigen::DSizes<long, 3> const&, Eigen::DSizes<long, 6> const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992605 (unix time) try "date -d @1740992605" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f11742910ef) received by PID 111599 (TID 0x7f10c734a700) from PID 1948848367 ***]

2025-03-03 17:03:30.474304 test begin: paddle.cast(Tensor([0, 1, 1, 1, 1],"bool"), Dtype(float32), )

W0303 17:03:33.936662 129744 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:03:33.938045 129744 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.cast(Tensor([0, 1, 1, 1, 1],"bool"), Dtype(float32), )
2025-03-03 17:03:33.946148 test begin: paddle.cast(Tensor([0, 1, 1, 1, 1],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 1, 1, 1, 1],"bool"), Dtype(float64), )
2025-03-03 17:03:33.950388 test begin: paddle.cast(Tensor([0, 1, 1, 114],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([0, 1, 1, 114],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:33.964826 test begin: paddle.cast(Tensor([0, 1, 1, 11],"bool"), "bool", )

[Pass] paddle.cast(Tensor([0, 1, 1, 11],"bool"), "bool", )
2025-03-03 17:03:33.970015 test begin: paddle.cast(Tensor([0, 1, 1, 11],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([0, 1, 1, 11],"bool"), dtype="float32", )
2025-03-03 17:03:33.973878 test begin: paddle.cast(Tensor([0, 1, 1, 13],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1, 1, 13],"float32"), Dtype(float16), )
2025-03-03 17:03:33.978665 test begin: paddle.cast(Tensor([0, 1, 1, 1],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 1, 1, 1],"bool"), Dtype(float64), )
2025-03-03 17:03:33.981855 test begin: paddle.cast(Tensor([0, 1, 1, 1],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([0, 1, 1, 1],"bool"), dtype="float32", )
2025-03-03 17:03:33.985921 test begin: paddle.cast(Tensor([0, 1, 1, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([0, 1, 1, 1],"float16"), "float32", )
2025-03-03 17:03:33.990554 test begin: paddle.cast(Tensor([0, 1, 1, 1],"int64"), "bool", )

[Pass] paddle.cast(Tensor([0, 1, 1, 1],"int64"), "bool", )
2025-03-03 17:03:33.993420 test begin: paddle.cast(Tensor([0, 1, 1, 2, 3],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 1, 1, 2, 3],"float32"), Dtype(float32), )
2025-03-03 17:03:33.998513 test begin: paddle.cast(Tensor([0, 1, 1, 2, 3],"float64"), Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 1, 1, 2, 3],"float64"), Dtype(float64), )
2025-03-03 17:03:34.001030 test begin: paddle.cast(Tensor([0, 1, 1, 2048],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1, 1, 2048],"float32"), Dtype(float16), )
2025-03-03 17:03:34.003347 test begin: paddle.cast(Tensor([0, 1, 1, 50],"float16"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1, 1, 50],"float16"), dtype=Dtype(float16), )
2025-03-03 17:03:34.005302 test begin: paddle.cast(Tensor([0, 1, 100],"float32"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1, 100],"float32"), dtype=Dtype(float16), )
2025-03-03 17:03:34.007658 test begin: paddle.cast(Tensor([0, 1, 101],"float32"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1, 101],"float32"), dtype=Dtype(float16), )
2025-03-03 17:03:34.011845 test begin: paddle.cast(Tensor([0, 1, 1024],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([0, 1, 1024],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.014741 test begin: paddle.cast(Tensor([0, 1, 1408],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([0, 1, 1408],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.016536 test begin: paddle.cast(Tensor([0, 1, 192],"float16"), "float32", )

[Pass] paddle.cast(Tensor([0, 1, 192],"float16"), "float32", )
2025-03-03 17:03:34.021487 test begin: paddle.cast(Tensor([0, 1, 192],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([0, 1, 192],"float32"), dtype="float32", )
2025-03-03 17:03:34.024841 test begin: paddle.cast(Tensor([0, 1, 2, 2],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 1, 2, 2],"bool"), Dtype(float64), )
2025-03-03 17:03:34.027030 test begin: paddle.cast(Tensor([0, 1, 2, 2],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([0, 1, 2, 2],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.029395 test begin: paddle.cast(Tensor([0, 1, 24276],"bool"), "float32", )

[Pass] paddle.cast(Tensor([0, 1, 24276],"bool"), "float32", )
2025-03-03 17:03:34.032446 test begin: paddle.cast(Tensor([0, 1, 28, 28],"float32"), "float32", )

[Pass] paddle.cast(Tensor([0, 1, 28, 28],"float32"), "float32", )
2025-03-03 17:03:34.036743 test begin: paddle.cast(Tensor([0, 1, 2],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([0, 1, 2],"float64"), dtype="float64", )
2025-03-03 17:03:34.040864 test begin: paddle.cast(Tensor([0, 1, 50, 50],"float16"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1, 50, 50],"float16"), dtype=Dtype(float16), )
2025-03-03 17:03:34.043114 test begin: paddle.cast(Tensor([0, 1, 64],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1, 64],"float32"), Dtype(float16), )
2025-03-03 17:03:34.045143 test begin: paddle.cast(Tensor([0, 1, 768],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1, 768],"float32"), Dtype(float16), )
2025-03-03 17:03:34.047595 test begin: paddle.cast(Tensor([0, 1024, 1024, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([0, 1024, 1024, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.055331 test begin: paddle.cast(Tensor([0, 1024, 1024],"bool"), "int64", )

[Pass] paddle.cast(Tensor([0, 1024, 1024],"bool"), "int64", )
2025-03-03 17:03:34.063452 test begin: paddle.cast(Tensor([0, 1024, 128],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([0, 1024, 128],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:34.070652 test begin: paddle.cast(Tensor([0, 1024, 1],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([0, 1024, 1],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.078702 test begin: paddle.cast(Tensor([0, 1024, 768],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([0, 1024, 768],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:34.081352 test begin: paddle.cast(Tensor([0, 10800],"bool"), dtype=VarType(float32), )

[Pass] paddle.cast(Tensor([0, 10800],"bool"), dtype=VarType(float32), )
2025-03-03 17:03:34.089038 test begin: paddle.cast(Tensor([0, 10],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([0, 10],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.091993 test begin: paddle.cast(Tensor([0, 10],"bool"), "bool", )

[Pass] paddle.cast(Tensor([0, 10],"bool"), "bool", )
2025-03-03 17:03:34.105513 test begin: paddle.cast(Tensor([0, 10],"bool"), "float", )

[Pass] paddle.cast(Tensor([0, 10],"bool"), "float", )
2025-03-03 17:03:34.108487 test begin: paddle.cast(Tensor([0, 10],"bool"), dtype="int32", )

[Pass] paddle.cast(Tensor([0, 10],"bool"), dtype="int32", )
2025-03-03 17:03:34.110979 test begin: paddle.cast(Tensor([0, 10],"float16"), VarType(float32), )

[Pass] paddle.cast(Tensor([0, 10],"float16"), VarType(float32), )
2025-03-03 17:03:34.113092 test begin: paddle.cast(Tensor([0, 10],"float32"), "int", )

[Pass] paddle.cast(Tensor([0, 10],"float32"), "int", )
2025-03-03 17:03:34.115177 test begin: paddle.cast(Tensor([0, 10],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 10],"float32"), Dtype(float16), )
2025-03-03 17:03:34.116825 test begin: paddle.cast(Tensor([0, 10],"float32"), VarType(bfloat16), )

[Pass] paddle.cast(Tensor([0, 10],"float32"), VarType(bfloat16), )
2025-03-03 17:03:34.119201 test begin: paddle.cast(Tensor([0, 10],"float32"), VarType(float16), )

[Pass] paddle.cast(Tensor([0, 10],"float32"), VarType(float16), )
2025-03-03 17:03:34.122182 test begin: paddle.cast(Tensor([0, 10],"float32"), VarType(float64), )

[Pass] paddle.cast(Tensor([0, 10],"float32"), VarType(float64), )
2025-03-03 17:03:34.123931 test begin: paddle.cast(Tensor([0, 10],"float32"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([0, 10],"float32"), type(numpy.float32), )
2025-03-03 17:03:34.126087 test begin: paddle.cast(Tensor([0, 10],"float32"), type(numpy.float64), )

[Pass] paddle.cast(Tensor([0, 10],"float32"), type(numpy.float64), )
2025-03-03 17:03:34.127563 test begin: paddle.cast(Tensor([0, 10],"float64"), "int", )

[Pass] paddle.cast(Tensor([0, 10],"float64"), "int", )
2025-03-03 17:03:34.134071 test begin: paddle.cast(Tensor([0, 10],"float64"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([0, 10],"float64"), type(numpy.float32), )
2025-03-03 17:03:34.138861 test begin: paddle.cast(Tensor([0, 11],"bool"), "int64", )

[Pass] paddle.cast(Tensor([0, 11],"bool"), "int64", )
2025-03-03 17:03:34.142969 test begin: paddle.cast(Tensor([0, 1200],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 1200],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:34.145601 test begin: paddle.cast(Tensor([0, 120],"int64"), "int32", )

[Pass] paddle.cast(Tensor([0, 120],"int64"), "int32", )
2025-03-03 17:03:34.152396 test begin: paddle.cast(Tensor([0, 128, 128],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([0, 128, 128],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.158125 test begin: paddle.cast(Tensor([0, 128, 128],"int32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 128, 128],"int32"), dtype=Dtype(float32), )
2025-03-03 17:03:34.164229 test begin: paddle.cast(Tensor([0, 128, 128],"int32"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([0, 128, 128],"int32"), dtype=Dtype(int32), )
2025-03-03 17:03:34.166325 test begin: paddle.cast(Tensor([0, 128, 256],"int32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 128, 256],"int32"), dtype=Dtype(float32), )
2025-03-03 17:03:34.169560 test begin: paddle.cast(Tensor([0, 128, 2],"int32"), dtype=VarType(float32), )

[Pass] paddle.cast(Tensor([0, 128, 2],"int32"), dtype=VarType(float32), )
2025-03-03 17:03:34.172015 test begin: paddle.cast(Tensor([0, 128, 96],"float32"), dtype="int32", )

[Pass] paddle.cast(Tensor([0, 128, 96],"float32"), dtype="int32", )
2025-03-03 17:03:34.173722 test begin: paddle.cast(Tensor([0, 12],"int64"), Dtype(bool), )

[Pass] paddle.cast(Tensor([0, 12],"int64"), Dtype(bool), )
2025-03-03 17:03:34.178492 test begin: paddle.cast(Tensor([0, 12],"int64"), Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 12],"int64"), Dtype(float16), )
2025-03-03 17:03:34.180869 test begin: paddle.cast(Tensor([0, 12],"int64"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 12],"int64"), Dtype(float32), )
2025-03-03 17:03:34.182887 test begin: paddle.cast(Tensor([0, 12],"int64"), Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 12],"int64"), Dtype(float64), )
2025-03-03 17:03:34.185345 test begin: paddle.cast(Tensor([0, 12],"int64"), Dtype(int32), )

[Pass] paddle.cast(Tensor([0, 12],"int64"), Dtype(int32), )
2025-03-03 17:03:34.187605 test begin: paddle.cast(Tensor([0, 12],"int64"), Dtype(int64), )

[Pass] paddle.cast(Tensor([0, 12],"int64"), Dtype(int64), )
2025-03-03 17:03:34.192553 test begin: paddle.cast(Tensor([0, 14, 5, 14],"bool"), "int64", )

[Pass] paddle.cast(Tensor([0, 14, 5, 14],"bool"), "int64", )
2025-03-03 17:03:34.196464 test begin: paddle.cast(Tensor([0, 151936],"float16"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 151936],"float16"), Dtype(float32), )
2025-03-03 17:03:34.198283 test begin: paddle.cast(Tensor([0, 16, 64],"float16"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 16, 64],"float16"), Dtype(float32), )
2025-03-03 17:03:34.199900 test begin: paddle.cast(Tensor([0, 16],"bfloat16"), VarType(bfloat16), )

[Pass] paddle.cast(Tensor([0, 16],"bfloat16"), VarType(bfloat16), )
2025-03-03 17:03:34.201432 test begin: paddle.cast(Tensor([0, 17, 10],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([0, 17, 10],"bool"), dtype="bool", )
2025-03-03 17:03:34.202922 test begin: paddle.cast(Tensor([0, 17, 5, 6, 7],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([0, 17, 5, 6, 7],"bool"), dtype="bool", )
2025-03-03 17:03:34.204354 test begin: paddle.cast(Tensor([0, 17],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([0, 17],"bool"), dtype="bool", )
2025-03-03 17:03:34.206175 test begin: paddle.cast(Tensor([0, 1],"bool"), "int32", )

[Pass] paddle.cast(Tensor([0, 1],"bool"), "int32", )
2025-03-03 17:03:34.208353 test begin: paddle.cast(Tensor([0, 1],"bool"), "int64", )

[Pass] paddle.cast(Tensor([0, 1],"bool"), "int64", )
2025-03-03 17:03:34.210241 test begin: paddle.cast(Tensor([0, 1],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 1],"bool"), Dtype(float64), )
2025-03-03 17:03:34.212019 test begin: paddle.cast(Tensor([0, 1],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([0, 1],"bool"), dtype="int", )
2025-03-03 17:03:34.215044 test begin: paddle.cast(Tensor([0, 1],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 1],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:34.217266 test begin: paddle.cast(Tensor([0, 1],"bool"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 1],"bool"), dtype=Dtype(float64), )
2025-03-03 17:03:34.218796 test begin: paddle.cast(Tensor([0, 1],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([0, 1],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.221167 test begin: paddle.cast(Tensor([0, 1],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([0, 1],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.222688 test begin: paddle.cast(Tensor([0, 1],"bool"), dtype=VarType(float32), )

[Pass] paddle.cast(Tensor([0, 1],"bool"), dtype=VarType(float32), )
2025-03-03 17:03:34.224095 test begin: paddle.cast(Tensor([0, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([0, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.225626 test begin: paddle.cast(Tensor([0, 1],"bool"), dtype=type(numpy.int32), )

[Pass] paddle.cast(Tensor([0, 1],"bool"), dtype=type(numpy.int32), )
2025-03-03 17:03:34.227452 test begin: paddle.cast(Tensor([0, 1],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([0, 1],"float32"), dtype="float32", )
2025-03-03 17:03:34.231509 test begin: paddle.cast(Tensor([0, 1],"float32"), dtype="int64", )

[Pass] paddle.cast(Tensor([0, 1],"float32"), dtype="int64", )
2025-03-03 17:03:34.233811 test begin: paddle.cast(Tensor([0, 1],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([0, 1],"float64"), dtype="float64", )
2025-03-03 17:03:34.235807 test begin: paddle.cast(Tensor([0, 1],"int32"), Dtype(int64), )

[Pass] paddle.cast(Tensor([0, 1],"int32"), Dtype(int64), )
2025-03-03 17:03:34.237790 test begin: paddle.cast(Tensor([0, 2, 1, 58],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 2, 1, 58],"float32"), Dtype(float32), )
2025-03-03 17:03:34.239659 test begin: paddle.cast(Tensor([0, 2, 2, 1],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([0, 2, 2, 1],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.241281 test begin: paddle.cast(Tensor([0, 2, 2, 2],"float64"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 2, 2, 2],"float64"), Dtype(float32), )
2025-03-03 17:03:34.242821 test begin: paddle.cast(Tensor([0, 20, 1],"float32"), "float64", )

[Pass] paddle.cast(Tensor([0, 20, 1],"float32"), "float64", )
2025-03-03 17:03:34.244267 test begin: paddle.cast(Tensor([0, 20, 1],"float64"), "float64", )

[Pass] paddle.cast(Tensor([0, 20, 1],"float64"), "float64", )
2025-03-03 17:03:34.245655 test begin: paddle.cast(Tensor([0, 20, 20],"int32"), "bool", )

[Pass] paddle.cast(Tensor([0, 20, 20],"int32"), "bool", )
2025-03-03 17:03:34.247106 test begin: paddle.cast(Tensor([0, 20, 2],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([0, 20, 2],"bool"), dtype="bool", )
2025-03-03 17:03:34.248561 test begin: paddle.cast(Tensor([0, 2048, 1],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([0, 2048, 1],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.250051 test begin: paddle.cast(Tensor([0, 20],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([0, 20],"bool"), dtype="float32", )
2025-03-03 17:03:34.251426 test begin: paddle.cast(Tensor([0, 20],"float32"), "float32", )

[Pass] paddle.cast(Tensor([0, 20],"float32"), "float32", )
2025-03-03 17:03:34.252885 test begin: paddle.cast(Tensor([0, 20],"float32"), "float64", )

[Pass] paddle.cast(Tensor([0, 20],"float32"), "float64", )
2025-03-03 17:03:34.254374 test begin: paddle.cast(Tensor([0, 224, 3],"uint8"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([0, 224, 3],"uint8"), type(numpy.float32), )
2025-03-03 17:03:34.261391 test begin: paddle.cast(Tensor([0, 22],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([0, 22],"bool"), dtype="int64", )
2025-03-03 17:03:34.263156 test begin: paddle.cast(Tensor([0, 256, 1, 1, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([0, 256, 1, 1, 1],"float16"), "float32", )
2025-03-03 17:03:34.264782 test begin: paddle.cast(Tensor([0, 256],"float16"), Dtype(int8), )

[Pass] paddle.cast(Tensor([0, 256],"float16"), Dtype(int8), )
2025-03-03 17:03:34.266634 test begin: paddle.cast(Tensor([0, 256],"float64"), Dtype(int8), )

[Pass] paddle.cast(Tensor([0, 256],"float64"), Dtype(int8), )
2025-03-03 17:03:34.268137 test begin: paddle.cast(Tensor([0, 28, 28],"uint8"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([0, 28, 28],"uint8"), type(numpy.float32), )
2025-03-03 17:03:34.269683 test begin: paddle.cast(Tensor([0, 2],"float32"), dtype=Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([0, 2],"float32"), dtype=Dtype(bfloat16), )
2025-03-03 17:03:34.271162 test begin: paddle.cast(Tensor([0, 3, 14, 14],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([0, 3, 14, 14],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.272855 test begin: paddle.cast(Tensor([0, 3, 4, 5],"bool"), "int64", )

[Pass] paddle.cast(Tensor([0, 3, 4, 5],"bool"), "int64", )
2025-03-03 17:03:34.274414 test begin: paddle.cast(Tensor([0, 3, 4, 5],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([0, 3, 4, 5],"bool"), dtype="bool", )
2025-03-03 17:03:34.275904 test begin: paddle.cast(Tensor([0, 300],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 300],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:34.277394 test begin: paddle.cast(Tensor([0, 31, 64],"float16"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 31, 64],"float16"), Dtype(float32), )
2025-03-03 17:03:34.278907 test begin: paddle.cast(Tensor([0, 32, 32, 1],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 32, 32, 1],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:34.280392 test begin: paddle.cast(Tensor([0, 32, 32, 1],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([0, 32, 32, 1],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.281881 test begin: paddle.cast(Tensor([0, 32],"float16"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([0, 32],"float16"), Dtype(bfloat16), )
2025-03-03 17:03:34.283291 test begin: paddle.cast(Tensor([0, 3],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([0, 3],"bool"), dtype="int", )
2025-03-03 17:03:34.284782 test begin: paddle.cast(Tensor([0, 3],"float32"), "float16", )

[Pass] paddle.cast(Tensor([0, 3],"float32"), "float16", )
2025-03-03 17:03:34.286134 test begin: paddle.cast(Tensor([0, 3],"float32"), "float32", )

[Pass] paddle.cast(Tensor([0, 3],"float32"), "float32", )
2025-03-03 17:03:34.287508 test begin: paddle.cast(Tensor([0, 3],"float32"), "float64", )

[Pass] paddle.cast(Tensor([0, 3],"float32"), "float64", )
2025-03-03 17:03:34.288879 test begin: paddle.cast(Tensor([0, 3],"float32"), "uint8", )

[Pass] paddle.cast(Tensor([0, 3],"float32"), "uint8", )
2025-03-03 17:03:34.290349 test begin: paddle.cast(Tensor([0, 3],"float32"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 3],"float32"), dtype=Dtype(float64), )
2025-03-03 17:03:34.292141 test begin: paddle.cast(Tensor([0, 4, 19, 34],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([0, 4, 19, 34],"bool"), dtype="int64", )
2025-03-03 17:03:34.294012 test begin: paddle.cast(Tensor([0, 4, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([0, 4, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.295594 test begin: paddle.cast(Tensor([0, 4, 2],"bool"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 4, 2],"bool"), dtype=Dtype(float64), )
2025-03-03 17:03:34.297084 test begin: paddle.cast(Tensor([0, 4, 2],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([0, 4, 2],"float32"), dtype="float32", )
2025-03-03 17:03:34.298488 test begin: paddle.cast(Tensor([0, 4, 2],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([0, 4, 2],"float64"), dtype="float64", )
2025-03-03 17:03:34.300036 test begin: paddle.cast(Tensor([0, 4, 38, 68],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([0, 4, 38, 68],"bool"), dtype="int64", )
2025-03-03 17:03:34.301547 test begin: paddle.cast(Tensor([0, 496, 512],"bool"), "int64", )

[Pass] paddle.cast(Tensor([0, 496, 512],"bool"), "int64", )
2025-03-03 17:03:34.303142 test begin: paddle.cast(Tensor([0, 4],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([0, 4],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:34.304602 test begin: paddle.cast(Tensor([0, 4],"float32"), "complex64", )

[Pass] paddle.cast(Tensor([0, 4],"float32"), "complex64", )
2025-03-03 17:03:34.307250 test begin: paddle.cast(Tensor([0, 4],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([0, 4],"float64"), dtype="float64", )
2025-03-03 17:03:34.308993 test begin: paddle.cast(Tensor([0, 512, 1024, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([0, 512, 1024, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.310625 test begin: paddle.cast(Tensor([0, 52, 7, 14],"float32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([0, 52, 7, 14],"float32"), dtype=Dtype(float32), )
2025-03-03 17:03:34.312165 test begin: paddle.cast(Tensor([0, 58],"int64"), "bool", )

[Pass] paddle.cast(Tensor([0, 58],"int64"), "bool", )
2025-03-03 17:03:34.313578 test begin: paddle.cast(Tensor([0, 6, 6],"float64"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([0, 6, 6],"float64"), dtype=Dtype(float64), )
2025-03-03 17:03:34.315114 test begin: paddle.cast(Tensor([0, 640, 1, 1, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([0, 640, 1, 1, 1],"float16"), "float32", )
2025-03-03 17:03:34.316522 test begin: paddle.cast(Tensor([0, 64],"float16"), "int8", )

[Pass] paddle.cast(Tensor([0, 64],"float16"), "int8", )
2025-03-03 17:03:34.317972 test begin: paddle.cast(Tensor([0, 7, 14],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([0, 7, 14],"bool"), dtype="int64", )
2025-03-03 17:03:34.319536 test begin: paddle.cast(Tensor([0, 768, 14, 14],"float32"), VarType(float32), )

[Pass] paddle.cast(Tensor([0, 768, 14, 14],"float32"), VarType(float32), )
2025-03-03 17:03:34.320937 test begin: paddle.cast(Tensor([0, 768],"float16"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([0, 768],"float16"), Dtype(bfloat16), )
2025-03-03 17:03:34.322537 test begin: paddle.cast(Tensor([0, 8, 9, 18],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([0, 8, 9, 18],"bool"), dtype="int", )
2025-03-03 17:03:34.323930 test begin: paddle.cast(Tensor([0],"bool"), "int64", )

[Pass] paddle.cast(Tensor([0],"bool"), "int64", )
2025-03-03 17:03:34.325377 test begin: paddle.cast(Tensor([0],"bool"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0],"bool"), Dtype(float32), )
2025-03-03 17:03:34.326852 test begin: paddle.cast(Tensor([0],"bool"), Dtype(int64), )

[Pass] paddle.cast(Tensor([0],"bool"), Dtype(int64), )
2025-03-03 17:03:34.328198 test begin: paddle.cast(Tensor([0],"bool"), VarType(float32), )

[Pass] paddle.cast(Tensor([0],"bool"), VarType(float32), )
2025-03-03 17:03:34.329518 test begin: paddle.cast(Tensor([0],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([0],"bool"), dtype="bool", )
2025-03-03 17:03:34.330813 test begin: paddle.cast(Tensor([0],"bool"), dtype="float64", )

[Pass] paddle.cast(Tensor([0],"bool"), dtype="float64", )
2025-03-03 17:03:34.333102 test begin: paddle.cast(Tensor([0],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([0],"bool"), dtype="int", )
2025-03-03 17:03:34.335384 test begin: paddle.cast(Tensor([0],"bool"), dtype="int32", )

[Pass] paddle.cast(Tensor([0],"bool"), dtype="int32", )
2025-03-03 17:03:34.337145 test begin: paddle.cast(Tensor([0],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([0],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:34.338571 test begin: paddle.cast(Tensor([0],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([0],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:34.340127 test begin: paddle.cast(Tensor([0],"bool"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([0],"bool"), dtype=Dtype(float64), )
2025-03-03 17:03:34.341465 test begin: paddle.cast(Tensor([0],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([0],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.343075 test begin: paddle.cast(Tensor([0],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([0],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.344501 test begin: paddle.cast(Tensor([0],"bool"), type(numpy.float16), )

[Pass] paddle.cast(Tensor([0],"bool"), type(numpy.float16), )
2025-03-03 17:03:34.346013 test begin: paddle.cast(Tensor([0],"bool"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([0],"bool"), type(numpy.float32), )
2025-03-03 17:03:34.349170 test begin: paddle.cast(Tensor([0],"bool"), type(numpy.float64), )

[Pass] paddle.cast(Tensor([0],"bool"), type(numpy.float64), )
2025-03-03 17:03:34.351477 test begin: paddle.cast(Tensor([0],"bool"), type(numpy.int32), )

[Pass] paddle.cast(Tensor([0],"bool"), type(numpy.int32), )
2025-03-03 17:03:34.353707 test begin: paddle.cast(Tensor([0],"bool"), type(numpy.int64), )

[Pass] paddle.cast(Tensor([0],"bool"), type(numpy.int64), )
2025-03-03 17:03:34.355887 test begin: paddle.cast(Tensor([0],"float16"), "float32", )

[Pass] paddle.cast(Tensor([0],"float16"), "float32", )
2025-03-03 17:03:34.357859 test begin: paddle.cast(Tensor([0],"float16"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([0],"float16"), Dtype(bfloat16), )
2025-03-03 17:03:34.359746 test begin: paddle.cast(Tensor([0],"float16"), Dtype(float32), )

[Pass] paddle.cast(Tensor([0],"float16"), Dtype(float32), )
2025-03-03 17:03:34.361641 test begin: paddle.cast(Tensor([0],"float16"), VarType(float32), )

[Pass] paddle.cast(Tensor([0],"float16"), VarType(float32), )
2025-03-03 17:03:34.363423 test begin: paddle.cast(Tensor([0],"float16"), type(numpy.float16), )

[Pass] paddle.cast(Tensor([0],"float16"), type(numpy.float16), )
2025-03-03 17:03:34.365180 test begin: paddle.cast(Tensor([0],"float16"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([0],"float16"), type(numpy.float32), )
2025-03-03 17:03:34.367035 test begin: paddle.cast(Tensor([0],"float16"), type(numpy.float64), )

[Pass] paddle.cast(Tensor([0],"float16"), type(numpy.float64), )
2025-03-03 17:03:34.369107 test begin: paddle.cast(Tensor([0],"float16"), type(numpy.int32), )

[Pass] paddle.cast(Tensor([0],"float16"), type(numpy.int32), )
2025-03-03 17:03:34.371196 test begin: paddle.cast(Tensor([0],"float16"), type(numpy.int64), )

[Pass] paddle.cast(Tensor([0],"float16"), type(numpy.int64), )
2025-03-03 17:03:34.373053 test begin: paddle.cast(Tensor([0],"float32"), "float16", )

[Pass] paddle.cast(Tensor([0],"float32"), "float16", )
2025-03-03 17:03:34.374939 test begin: paddle.cast(Tensor([0],"float32"), "float32", )

[Pass] paddle.cast(Tensor([0],"float32"), "float32", )
2025-03-03 17:03:34.376965 test begin: paddle.cast(Tensor([0],"float32"), "float64", )

[Pass] paddle.cast(Tensor([0],"float32"), "float64", )
2025-03-03 17:03:34.378866 test begin: paddle.cast(Tensor([0],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([0],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:34.380652 test begin: paddle.cast(Tensor([0],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([0],"float32"), Dtype(float16), )
2025-03-03 17:03:34.382549 test begin: paddle.cast(Tensor([0],"float32"), Dtype(float64), )

[Pass] paddle.cast(Tensor([0],"float32"), Dtype(float64), )
2025-03-03 17:03:34.384386 test begin: paddle.cast(Tensor([0],"float32"), Dtype(int64), )

[Pass] paddle.cast(Tensor([0],"float32"), Dtype(int64), )
2025-03-03 17:03:34.386289 test begin: paddle.cast(Tensor([0],"float32"), dtype="float16", )

[Pass] paddle.cast(Tensor([0],"float32"), dtype="float16", )
2025-03-03 17:03:34.388235 test begin: paddle.cast(Tensor([0],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([0],"float32"), dtype="float32", )
2025-03-03 17:03:34.389976 test begin: paddle.cast(Tensor([0],"float32"), dtype="int64", )

[Pass] paddle.cast(Tensor([0],"float32"), dtype="int64", )
2025-03-03 17:03:34.391926 test begin: paddle.cast(Tensor([0],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([0],"float64"), dtype="float64", )
2025-03-03 17:03:34.393871 test begin: paddle.cast(Tensor([0],"float64"), dtype="int32", )

[Pass] paddle.cast(Tensor([0],"float64"), dtype="int32", )
2025-03-03 17:03:34.395920 test begin: paddle.cast(Tensor([0],"int64"), "bool", )

[Pass] paddle.cast(Tensor([0],"int64"), "bool", )
2025-03-03 17:03:34.397935 test begin: paddle.cast(Tensor([0],"int64"), "int32", )

[Pass] paddle.cast(Tensor([0],"int64"), "int32", )
2025-03-03 17:03:34.399850 test begin: paddle.cast(Tensor([0],"int64"), Dtype(int32), )

[Pass] paddle.cast(Tensor([0],"int64"), Dtype(int32), )
2025-03-03 17:03:34.401685 test begin: paddle.cast(Tensor([0],"int64"), VarType(int32), )

[Pass] paddle.cast(Tensor([0],"int64"), VarType(int32), )
2025-03-03 17:03:34.403897 test begin: paddle.cast(Tensor([0],"int64"), dtype="int64", )

[Pass] paddle.cast(Tensor([0],"int64"), dtype="int64", )
2025-03-03 17:03:34.405918 test begin: paddle.cast(Tensor([0],"int64"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([0],"int64"), dtype=Dtype(float16), )
2025-03-03 17:03:34.407829 test begin: paddle.cast(Tensor([0],"int64"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([0],"int64"), dtype=Dtype(float32), )
2025-03-03 17:03:34.409977 test begin: paddle.cast(Tensor([1, 0, 1, 11],"bool"), "bool", )

[Pass] paddle.cast(Tensor([1, 0, 1, 11],"bool"), "bool", )
2025-03-03 17:03:34.411974 test begin: paddle.cast(Tensor([1, 0, 1, 11],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 0, 1, 11],"bool"), dtype="float32", )
2025-03-03 17:03:34.413950 test begin: paddle.cast(Tensor([1, 0, 1, 1],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 0, 1, 1],"bool"), dtype="float32", )
2025-03-03 17:03:34.415885 test begin: paddle.cast(Tensor([1, 0, 1, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1, 0, 1, 1],"float16"), "float32", )
2025-03-03 17:03:34.417882 test begin: paddle.cast(Tensor([1, 0, 1, 1],"int64"), "bool", )

[Pass] paddle.cast(Tensor([1, 0, 1, 1],"int64"), "bool", )
2025-03-03 17:03:34.419804 test begin: paddle.cast(Tensor([1, 0, 1, 2048],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 0, 1, 2048],"float32"), Dtype(float16), )
2025-03-03 17:03:34.421657 test begin: paddle.cast(Tensor([1, 0, 1, 50],"float16"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 0, 1, 50],"float16"), dtype=Dtype(float16), )
2025-03-03 17:03:34.436440 test begin: paddle.cast(Tensor([1, 0, 1, 58],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 0, 1, 58],"float32"), Dtype(float32), )
2025-03-03 17:03:34.440869 test begin: paddle.cast(Tensor([1, 0, 1024],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([1, 0, 1024],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.443345 test begin: paddle.cast(Tensor([1, 0, 1024],"bool"), "int64", )

[Pass] paddle.cast(Tensor([1, 0, 1024],"bool"), "int64", )
2025-03-03 17:03:34.445047 test begin: paddle.cast(Tensor([1, 0, 128],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([1, 0, 128],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.447519 test begin: paddle.cast(Tensor([1, 0, 128],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([1, 0, 128],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:34.449546 test begin: paddle.cast(Tensor([1, 0, 128],"int32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 0, 128],"int32"), dtype=Dtype(float32), )
2025-03-03 17:03:34.451816 test begin: paddle.cast(Tensor([1, 0, 128],"int32"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([1, 0, 128],"int32"), dtype=Dtype(int32), )
2025-03-03 17:03:34.453559 test begin: paddle.cast(Tensor([1, 0, 14, 14],"float32"), VarType(float32), )

[Pass] paddle.cast(Tensor([1, 0, 14, 14],"float32"), VarType(float32), )
2025-03-03 17:03:34.455575 test begin: paddle.cast(Tensor([1, 0, 1408],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([1, 0, 1408],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.457783 test begin: paddle.cast(Tensor([1, 0, 192],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1, 0, 192],"float16"), "float32", )
2025-03-03 17:03:34.459946 test begin: paddle.cast(Tensor([1, 0, 192],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 0, 192],"float32"), dtype="float32", )
2025-03-03 17:03:34.462091 test begin: paddle.cast(Tensor([1, 0, 1],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([1, 0, 1],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.465225 test begin: paddle.cast(Tensor([1, 0, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([1, 0, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.469126 test begin: paddle.cast(Tensor([1, 0, 2, 2],"float64"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 0, 2, 2],"float64"), Dtype(float32), )
2025-03-03 17:03:34.470927 test begin: paddle.cast(Tensor([1, 0, 24276],"bool"), "float32", )

[Pass] paddle.cast(Tensor([1, 0, 24276],"bool"), "float32", )
2025-03-03 17:03:34.472317 test begin: paddle.cast(Tensor([1, 0, 256],"int32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 0, 256],"int32"), dtype=Dtype(float32), )
2025-03-03 17:03:34.473692 test begin: paddle.cast(Tensor([1, 0, 28, 28],"float32"), "float32", )

[Pass] paddle.cast(Tensor([1, 0, 28, 28],"float32"), "float32", )
2025-03-03 17:03:34.475068 test begin: paddle.cast(Tensor([1, 0, 28],"uint8"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([1, 0, 28],"uint8"), type(numpy.float32), )
2025-03-03 17:03:34.476452 test begin: paddle.cast(Tensor([1, 0, 2],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 0, 2],"float32"), dtype="float32", )
2025-03-03 17:03:34.477731 test begin: paddle.cast(Tensor([1, 0, 2],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([1, 0, 2],"float64"), dtype="float64", )
2025-03-03 17:03:34.479014 test begin: paddle.cast(Tensor([1, 0, 5, 14],"bool"), "int64", )

[Pass] paddle.cast(Tensor([1, 0, 5, 14],"bool"), "int64", )
2025-03-03 17:03:34.481299 test begin: paddle.cast(Tensor([1, 0, 50, 50],"float16"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 0, 50, 50],"float16"), dtype=Dtype(float16), )
2025-03-03 17:03:34.482899 test begin: paddle.cast(Tensor([1, 0, 512],"bool"), "int64", )

[Pass] paddle.cast(Tensor([1, 0, 512],"bool"), "int64", )
2025-03-03 17:03:34.484281 test begin: paddle.cast(Tensor([1, 0, 64],"float16"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 0, 64],"float16"), Dtype(float32), )
2025-03-03 17:03:34.485736 test begin: paddle.cast(Tensor([1, 0, 64],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 0, 64],"float32"), Dtype(float16), )
2025-03-03 17:03:34.487059 test begin: paddle.cast(Tensor([1, 0, 768],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([1, 0, 768],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:34.488373 test begin: paddle.cast(Tensor([1, 0, 768],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 0, 768],"float32"), Dtype(float16), )
2025-03-03 17:03:34.489758 test begin: paddle.cast(Tensor([1, 0],"bool"), "int32", )

[Pass] paddle.cast(Tensor([1, 0],"bool"), "int32", )
2025-03-03 17:03:34.491148 test begin: paddle.cast(Tensor([1, 0],"bool"), "int64", )

[Pass] paddle.cast(Tensor([1, 0],"bool"), "int64", )
2025-03-03 17:03:34.492597 test begin: paddle.cast(Tensor([1, 0],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 0],"bool"), dtype="float32", )
2025-03-03 17:03:34.493967 test begin: paddle.cast(Tensor([1, 0],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([1, 0],"bool"), dtype="int", )
2025-03-03 17:03:34.495449 test begin: paddle.cast(Tensor([1, 0],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([1, 0],"bool"), dtype="int64", )
2025-03-03 17:03:34.496851 test begin: paddle.cast(Tensor([1, 0],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([1, 0],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.498335 test begin: paddle.cast(Tensor([1, 0],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([1, 0],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.499697 test begin: paddle.cast(Tensor([1, 0],"float16"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 0],"float16"), Dtype(float32), )
2025-03-03 17:03:34.501205 test begin: paddle.cast(Tensor([1, 0],"float32"), "float32", )

[Pass] paddle.cast(Tensor([1, 0],"float32"), "float32", )
2025-03-03 17:03:34.502491 test begin: paddle.cast(Tensor([1, 0],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 0],"float32"), dtype="float32", )
2025-03-03 17:03:34.503872 test begin: paddle.cast(Tensor([1, 0],"float32"), dtype="int64", )

[Pass] paddle.cast(Tensor([1, 0],"float32"), dtype="int64", )
2025-03-03 17:03:34.505284 test begin: paddle.cast(Tensor([1, 0],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([1, 0],"float64"), dtype="float64", )
2025-03-03 17:03:34.507242 test begin: paddle.cast(Tensor([1, 0],"int32"), Dtype(int64), )

[Pass] paddle.cast(Tensor([1, 0],"int32"), Dtype(int64), )
2025-03-03 17:03:34.509173 test begin: paddle.cast(Tensor([1, 0],"int64"), "bool", )

[Pass] paddle.cast(Tensor([1, 0],"int64"), "bool", )
2025-03-03 17:03:34.510496 test begin: paddle.cast(Tensor([1, 1, 0, 11],"bool"), "bool", )

[Pass] paddle.cast(Tensor([1, 1, 0, 11],"bool"), "bool", )
2025-03-03 17:03:34.511851 test begin: paddle.cast(Tensor([1, 1, 0, 11],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 1, 0, 11],"bool"), dtype="float32", )
2025-03-03 17:03:34.513170 test begin: paddle.cast(Tensor([1, 1, 0, 1],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 1, 0, 1],"bool"), dtype="float32", )
2025-03-03 17:03:34.514497 test begin: paddle.cast(Tensor([1, 1, 0, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1, 1, 0, 1],"float16"), "float32", )
2025-03-03 17:03:34.517804 test begin: paddle.cast(Tensor([1, 1, 0, 1],"int64"), "bool", )

[Pass] paddle.cast(Tensor([1, 1, 0, 1],"int64"), "bool", )
2025-03-03 17:03:34.520694 test begin: paddle.cast(Tensor([1, 1, 0, 2048],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 1, 0, 2048],"float32"), Dtype(float16), )
2025-03-03 17:03:34.522843 test begin: paddle.cast(Tensor([1, 1, 0, 28],"float32"), "float32", )

[Pass] paddle.cast(Tensor([1, 1, 0, 28],"float32"), "float32", )
2025-03-03 17:03:34.525463 test begin: paddle.cast(Tensor([1, 1, 0, 50],"float16"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 1, 0, 50],"float16"), dtype=Dtype(float16), )
2025-03-03 17:03:34.529222 test begin: paddle.cast(Tensor([1, 1, 0],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([1, 1, 0],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.533303 test begin: paddle.cast(Tensor([1, 1, 0],"bool"), "float32", )

[Pass] paddle.cast(Tensor([1, 1, 0],"bool"), "float32", )
2025-03-03 17:03:34.536045 test begin: paddle.cast(Tensor([1, 1, 0],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1, 1, 0],"float16"), "float32", )
2025-03-03 17:03:34.539501 test begin: paddle.cast(Tensor([1, 1, 0],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 1, 0],"float32"), Dtype(float16), )
2025-03-03 17:03:34.545142 test begin: paddle.cast(Tensor([1, 1, 0],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 1, 0],"float32"), dtype="float32", )
2025-03-03 17:03:34.547839 test begin: paddle.cast(Tensor([1, 1, 1, 0],"bool"), "bool", )

[Pass] paddle.cast(Tensor([1, 1, 1, 0],"bool"), "bool", )
2025-03-03 17:03:34.551129 test begin: paddle.cast(Tensor([1, 1, 1, 0],"bool"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 1, 1, 0],"bool"), dtype="float32", )
2025-03-03 17:03:34.573491 test begin: paddle.cast(Tensor([1, 1, 1, 0],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1, 1, 1, 0],"float16"), "float32", )
2025-03-03 17:03:34.577361 test begin: paddle.cast(Tensor([1, 1, 1, 0],"float16"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 1, 1, 0],"float16"), dtype=Dtype(float16), )
2025-03-03 17:03:34.582910 test begin: paddle.cast(Tensor([1, 1, 1, 0],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 1, 1, 0],"float32"), Dtype(float16), )
2025-03-03 17:03:34.588991 test begin: paddle.cast(Tensor([1, 1, 1, 0],"int64"), "bool", )

[Pass] paddle.cast(Tensor([1, 1, 1, 0],"int64"), "bool", )
2025-03-03 17:03:34.594241 test begin: paddle.cast(Tensor([1, 1, 28, 0],"float32"), "float32", )

[Pass] paddle.cast(Tensor([1, 1, 28, 0],"float32"), "float32", )
2025-03-03 17:03:34.598555 test begin: paddle.cast(Tensor([1, 1, 50, 0],"float16"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([1, 1, 50, 0],"float16"), dtype=Dtype(float16), )
2025-03-03 17:03:34.601572 test begin: paddle.cast(Tensor([1, 1024, 0],"bool"), "int64", )

[Pass] paddle.cast(Tensor([1, 1024, 0],"bool"), "int64", )
2025-03-03 17:03:34.605130 test begin: paddle.cast(Tensor([1, 1024, 0],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([1, 1024, 0],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.609096 test begin: paddle.cast(Tensor([1, 1024, 0],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([1, 1024, 0],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:34.612040 test begin: paddle.cast(Tensor([1, 128, 0],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([1, 128, 0],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.616007 test begin: paddle.cast(Tensor([1, 128, 0],"int32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 128, 0],"int32"), dtype=Dtype(float32), )
2025-03-03 17:03:34.619741 test begin: paddle.cast(Tensor([1, 128, 0],"int32"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([1, 128, 0],"int32"), dtype=Dtype(int32), )
2025-03-03 17:03:34.626223 test begin: paddle.cast(Tensor([1, 14, 0, 14],"bool"), "int64", )

[Pass] paddle.cast(Tensor([1, 14, 0, 14],"bool"), "int64", )
2025-03-03 17:03:34.628950 test begin: paddle.cast(Tensor([1, 14, 5, 0],"bool"), "int64", )

[Pass] paddle.cast(Tensor([1, 14, 5, 0],"bool"), "int64", )
2025-03-03 17:03:34.673795 test begin: paddle.cast(Tensor([1, 16, 0],"float16"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 16, 0],"float16"), Dtype(float32), )
2025-03-03 17:03:34.678753 test begin: paddle.cast(Tensor([1, 2, 0, 2],"float64"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 2, 0, 2],"float64"), Dtype(float32), )
2025-03-03 17:03:34.697698 test begin: paddle.cast(Tensor([1, 2, 0, 58],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 2, 0, 58],"float32"), Dtype(float32), )
2025-03-03 17:03:34.702681 test begin: paddle.cast(Tensor([1, 2, 1, 0],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 2, 1, 0],"float32"), Dtype(float32), )
2025-03-03 17:03:34.707701 test begin: paddle.cast(Tensor([1, 2, 2, 0],"float64"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 2, 2, 0],"float64"), Dtype(float32), )
2025-03-03 17:03:34.712321 test begin: paddle.cast(Tensor([1, 2048, 0],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([1, 2048, 0],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.716936 test begin: paddle.cast(Tensor([1, 28, 0],"uint8"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([1, 28, 0],"uint8"), type(numpy.float32), )
2025-03-03 17:03:34.721517 test begin: paddle.cast(Tensor([1, 31, 0],"float16"), Dtype(float32), )

[Pass] paddle.cast(Tensor([1, 31, 0],"float16"), Dtype(float32), )
2025-03-03 17:03:34.729414 test begin: paddle.cast(Tensor([1, 4, 0],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([1, 4, 0],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.733446 test begin: paddle.cast(Tensor([1, 4, 0],"float32"), dtype="float32", )

[Pass] paddle.cast(Tensor([1, 4, 0],"float32"), dtype="float32", )
2025-03-03 17:03:34.739457 test begin: paddle.cast(Tensor([1, 4, 0],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([1, 4, 0],"float64"), dtype="float64", )
2025-03-03 17:03:34.752209 test begin: paddle.cast(Tensor([1, 496, 0],"bool"), "int64", )

[Pass] paddle.cast(Tensor([1, 496, 0],"bool"), "int64", )
2025-03-03 17:03:34.763058 test begin: paddle.cast(Tensor([1, 768, 0, 14],"float32"), VarType(float32), )

[Pass] paddle.cast(Tensor([1, 768, 0, 14],"float32"), VarType(float32), )
2025-03-03 17:03:34.770100 test begin: paddle.cast(Tensor([1, 768, 14, 0],"float32"), VarType(float32), )

[Pass] paddle.cast(Tensor([1, 768, 14, 0],"float32"), VarType(float32), )
2025-03-03 17:03:34.776164 test begin: paddle.cast(Tensor([10, 0, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([10, 0, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.780542 test begin: paddle.cast(Tensor([10, 0, 1],"float32"), "float64", )

[Pass] paddle.cast(Tensor([10, 0, 1],"float32"), "float64", )
2025-03-03 17:03:34.790557 test begin: paddle.cast(Tensor([10, 0, 1],"float64"), "float64", )

[Pass] paddle.cast(Tensor([10, 0, 1],"float64"), "float64", )
2025-03-03 17:03:34.796630 test begin: paddle.cast(Tensor([10, 0],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([10, 0],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.801547 test begin: paddle.cast(Tensor([10, 0],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([10, 0],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.805779 test begin: paddle.cast(Tensor([10, 0],"float16"), VarType(float32), )

[Pass] paddle.cast(Tensor([10, 0],"float16"), VarType(float32), )
2025-03-03 17:03:34.816779 test begin: paddle.cast(Tensor([10, 0],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([10, 0],"float32"), Dtype(float16), )
2025-03-03 17:03:34.823087 test begin: paddle.cast(Tensor([10, 0],"float32"), VarType(bfloat16), )

[Pass] paddle.cast(Tensor([10, 0],"float32"), VarType(bfloat16), )
2025-03-03 17:03:34.829269 test begin: paddle.cast(Tensor([10, 0],"float32"), VarType(float16), )

[Pass] paddle.cast(Tensor([10, 0],"float32"), VarType(float16), )
2025-03-03 17:03:34.840426 test begin: paddle.cast(Tensor([10, 0],"float32"), VarType(float64), )

[Pass] paddle.cast(Tensor([10, 0],"float32"), VarType(float64), )
2025-03-03 17:03:34.843153 test begin: paddle.cast(Tensor([10, 0],"float32"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([10, 0],"float32"), type(numpy.float32), )
2025-03-03 17:03:34.845247 test begin: paddle.cast(Tensor([10, 0],"float32"), type(numpy.float64), )

[Pass] paddle.cast(Tensor([10, 0],"float32"), type(numpy.float64), )
2025-03-03 17:03:34.846954 test begin: paddle.cast(Tensor([10, 0],"float64"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([10, 0],"float64"), type(numpy.float32), )
2025-03-03 17:03:34.850255 test begin: paddle.cast(Tensor([10, 0],"int64"), Dtype(bool), )

[Pass] paddle.cast(Tensor([10, 0],"int64"), Dtype(bool), )
2025-03-03 17:03:34.854639 test begin: paddle.cast(Tensor([10, 0],"int64"), Dtype(float16), )

[Pass] paddle.cast(Tensor([10, 0],"int64"), Dtype(float16), )
2025-03-03 17:03:34.857029 test begin: paddle.cast(Tensor([10, 0],"int64"), Dtype(float32), )

[Pass] paddle.cast(Tensor([10, 0],"int64"), Dtype(float32), )
2025-03-03 17:03:34.859199 test begin: paddle.cast(Tensor([10, 0],"int64"), Dtype(float64), )

[Pass] paddle.cast(Tensor([10, 0],"int64"), Dtype(float64), )
2025-03-03 17:03:34.862410 test begin: paddle.cast(Tensor([10, 0],"int64"), Dtype(int32), )

[Pass] paddle.cast(Tensor([10, 0],"int64"), Dtype(int32), )
2025-03-03 17:03:34.864919 test begin: paddle.cast(Tensor([10, 0],"int64"), Dtype(int64), )

[Pass] paddle.cast(Tensor([10, 0],"int64"), Dtype(int64), )
2025-03-03 17:03:34.866988 test begin: paddle.cast(Tensor([10, 20, 0],"float32"), "float64", )

[Pass] paddle.cast(Tensor([10, 20, 0],"float32"), "float64", )
2025-03-03 17:03:34.869237 test begin: paddle.cast(Tensor([10, 20, 0],"float64"), "float64", )

[Pass] paddle.cast(Tensor([10, 20, 0],"float64"), "float64", )
2025-03-03 17:03:34.871187 test begin: paddle.cast(Tensor([10, 4, 0],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([10, 4, 0],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.874354 test begin: paddle.cast(Tensor([100, 0],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([100, 0],"bool"), Dtype(float64), )
2025-03-03 17:03:34.876389 test begin: paddle.cast(Tensor([100, 0],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([100, 0],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.879668 test begin: paddle.cast(Tensor([101, 0],"bool"), dtype=type(numpy.int32), )

[Pass] paddle.cast(Tensor([101, 0],"bool"), dtype=type(numpy.int32), )
2025-03-03 17:03:34.881878 test begin: paddle.cast(Tensor([102, 0],"bool"), dtype=type(numpy.int32), )

[Pass] paddle.cast(Tensor([102, 0],"bool"), dtype=type(numpy.int32), )
2025-03-03 17:03:34.883613 test begin: paddle.cast(Tensor([1024, 0, 1, 1, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1024, 0, 1, 1, 1],"float16"), "float32", )
2025-03-03 17:03:34.885546 test begin: paddle.cast(Tensor([1024, 0, 14, 14],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([1024, 0, 14, 14],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.888693 test begin: paddle.cast(Tensor([1024, 256, 0, 1, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1024, 256, 0, 1, 1],"float16"), "float32", )
2025-03-03 17:03:34.891041 test begin: paddle.cast(Tensor([1024, 256, 1, 0, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1024, 256, 1, 0, 1],"float16"), "float32", )
2025-03-03 17:03:34.892757 test begin: paddle.cast(Tensor([1024, 256, 1, 1, 0],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1024, 256, 1, 1, 0],"float16"), "float32", )
2025-03-03 17:03:34.895462 test begin: paddle.cast(Tensor([1024, 3, 0, 14],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([1024, 3, 0, 14],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.898541 test begin: paddle.cast(Tensor([1024, 3, 14, 0],"bfloat16"), VarType(float32), )

[Pass] paddle.cast(Tensor([1024, 3, 14, 0],"bfloat16"), VarType(float32), )
2025-03-03 17:03:34.901127 test begin: paddle.cast(Tensor([1024, 640, 0, 1, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1024, 640, 0, 1, 1],"float16"), "float32", )
2025-03-03 17:03:34.903728 test begin: paddle.cast(Tensor([1024, 640, 1, 0, 1],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1024, 640, 1, 0, 1],"float16"), "float32", )
2025-03-03 17:03:34.905907 test begin: paddle.cast(Tensor([1024, 640, 1, 1, 0],"float16"), "float32", )

[Pass] paddle.cast(Tensor([1024, 640, 1, 1, 0],"float16"), "float32", )
2025-03-03 17:03:34.908235 test begin: paddle.cast(Tensor([104, 0, 1, 13],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([104, 0, 1, 13],"float32"), Dtype(float16), )
2025-03-03 17:03:34.910593 test begin: paddle.cast(Tensor([104, 0],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([104, 0],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.913028 test begin: paddle.cast(Tensor([104, 1, 0, 13],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([104, 1, 0, 13],"float32"), Dtype(float16), )
2025-03-03 17:03:34.915526 test begin: paddle.cast(Tensor([104, 1, 1, 0],"float32"), Dtype(float16), )

[Pass] paddle.cast(Tensor([104, 1, 1, 0],"float32"), Dtype(float16), )
2025-03-03 17:03:34.918049 test begin: paddle.cast(Tensor([11, 0, 10],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([11, 0, 10],"bool"), dtype="bool", )
2025-03-03 17:03:34.920489 test begin: paddle.cast(Tensor([11, 0],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([11, 0],"bool"), dtype="bool", )
2025-03-03 17:03:34.922906 test begin: paddle.cast(Tensor([11, 17, 0],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([11, 17, 0],"bool"), dtype="bool", )
2025-03-03 17:03:34.925534 test begin: paddle.cast(Tensor([11008, 0],"float16"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([11008, 0],"float16"), Dtype(bfloat16), )
2025-03-03 17:03:34.927799 test begin: paddle.cast(Tensor([1124, 0],"float16"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([1124, 0],"float16"), Dtype(bfloat16), )
2025-03-03 17:03:34.929965 test begin: paddle.cast(Tensor([114, 0],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([114, 0],"float64"), dtype="float64", )
2025-03-03 17:03:34.932142 test begin: paddle.cast(Tensor([117, 0],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([117, 0],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.934193 test begin: paddle.cast(Tensor([12, 0, 19, 34],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([12, 0, 19, 34],"bool"), dtype="int64", )
2025-03-03 17:03:34.936391 test begin: paddle.cast(Tensor([12, 0, 2],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([12, 0, 2],"bool"), dtype="bool", )
2025-03-03 17:03:34.938401 test begin: paddle.cast(Tensor([12, 0, 38, 68],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([12, 0, 38, 68],"bool"), dtype="int64", )
2025-03-03 17:03:34.939973 test begin: paddle.cast(Tensor([12, 0],"bool"), "bool", )

[Pass] paddle.cast(Tensor([12, 0],"bool"), "bool", )
2025-03-03 17:03:34.941596 test begin: paddle.cast(Tensor([12, 0],"bool"), "float", )

[Pass] paddle.cast(Tensor([12, 0],"bool"), "float", )
2025-03-03 17:03:34.943934 test begin: paddle.cast(Tensor([12, 0],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([12, 0],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:34.945673 test begin: paddle.cast(Tensor([12, 0],"float32"), "int", )

[Pass] paddle.cast(Tensor([12, 0],"float32"), "int", )
2025-03-03 17:03:34.947251 test begin: paddle.cast(Tensor([12, 0],"float64"), "int", )

[Pass] paddle.cast(Tensor([12, 0],"float64"), "int", )
2025-03-03 17:03:34.948682 test begin: paddle.cast(Tensor([12, 20, 0],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([12, 20, 0],"bool"), dtype="bool", )
2025-03-03 17:03:34.950113 test begin: paddle.cast(Tensor([12, 4, 0, 34],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([12, 4, 0, 34],"bool"), dtype="int64", )
2025-03-03 17:03:34.951640 test begin: paddle.cast(Tensor([12, 4, 0, 68],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([12, 4, 0, 68],"bool"), dtype="int64", )
2025-03-03 17:03:34.953003 test begin: paddle.cast(Tensor([12, 4, 19, 0],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([12, 4, 19, 0],"bool"), dtype="int64", )
2025-03-03 17:03:34.954414 test begin: paddle.cast(Tensor([12, 4, 38, 0],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([12, 4, 38, 0],"bool"), dtype="int64", )
2025-03-03 17:03:34.955870 test begin: paddle.cast(Tensor([16, 0],"bfloat16"), VarType(bfloat16), )

[Pass] paddle.cast(Tensor([16, 0],"bfloat16"), VarType(bfloat16), )
2025-03-03 17:03:34.957318 test begin: paddle.cast(Tensor([16, 0],"float16"), Dtype(int8), )

[Pass] paddle.cast(Tensor([16, 0],"float16"), Dtype(int8), )
2025-03-03 17:03:34.958806 test begin: paddle.cast(Tensor([16, 0],"float64"), Dtype(int8), )

[Pass] paddle.cast(Tensor([16, 0],"float64"), Dtype(int8), )
2025-03-03 17:03:34.961334 test begin: paddle.cast(Tensor([192, 0],"bool"), dtype=VarType(float32), )

[Pass] paddle.cast(Tensor([192, 0],"bool"), dtype=VarType(float32), )
2025-03-03 17:03:34.963251 test begin: paddle.cast(Tensor([192, 0],"float16"), "int8", )

[Pass] paddle.cast(Tensor([192, 0],"float16"), "int8", )
2025-03-03 17:03:34.965359 test begin: paddle.cast(Tensor([2, 0, 1, 1, 1],"bool"), Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 0, 1, 1, 1],"bool"), Dtype(float32), )
2025-03-03 17:03:34.967003 test begin: paddle.cast(Tensor([2, 0, 1, 1, 1],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 0, 1, 1, 1],"bool"), Dtype(float64), )
2025-03-03 17:03:34.968597 test begin: paddle.cast(Tensor([2, 0, 1, 114],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([2, 0, 1, 114],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:34.970426 test begin: paddle.cast(Tensor([2, 0, 1, 1],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 0, 1, 1],"bool"), Dtype(float64), )
2025-03-03 17:03:34.972082 test begin: paddle.cast(Tensor([2, 0, 1, 2, 3],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 0, 1, 2, 3],"float32"), Dtype(float32), )
2025-03-03 17:03:34.973655 test begin: paddle.cast(Tensor([2, 0, 1, 2, 3],"float64"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 0, 1, 2, 3],"float64"), Dtype(float64), )
2025-03-03 17:03:34.975318 test begin: paddle.cast(Tensor([2, 0, 100],"float32"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([2, 0, 100],"float32"), dtype=Dtype(float16), )
2025-03-03 17:03:34.977621 test begin: paddle.cast(Tensor([2, 0, 101],"float32"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([2, 0, 101],"float32"), dtype=Dtype(float16), )
2025-03-03 17:03:34.979259 test begin: paddle.cast(Tensor([2, 0, 1024, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([2, 0, 1024, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:34.980993 test begin: paddle.cast(Tensor([2, 0, 2, 1],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([2, 0, 2, 1],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.982842 test begin: paddle.cast(Tensor([2, 0, 2, 2],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 0, 2, 2],"bool"), Dtype(float64), )
2025-03-03 17:03:34.984401 test begin: paddle.cast(Tensor([2, 0, 2, 2],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([2, 0, 2, 2],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:34.985962 test begin: paddle.cast(Tensor([2, 0, 20],"int32"), "bool", )

[Pass] paddle.cast(Tensor([2, 0, 20],"int32"), "bool", )
2025-03-03 17:03:34.987560 test begin: paddle.cast(Tensor([2, 0, 32, 1],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 0, 32, 1],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:34.989212 test begin: paddle.cast(Tensor([2, 0, 32, 1],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([2, 0, 32, 1],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:34.990882 test begin: paddle.cast(Tensor([2, 0, 4, 5],"bool"), "int64", )

[Pass] paddle.cast(Tensor([2, 0, 4, 5],"bool"), "int64", )
2025-03-03 17:03:34.992452 test begin: paddle.cast(Tensor([2, 0, 4, 5],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([2, 0, 4, 5],"bool"), dtype="bool", )
2025-03-03 17:03:34.994191 test begin: paddle.cast(Tensor([2, 0, 7, 14],"float32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 0, 7, 14],"float32"), dtype=Dtype(float32), )
2025-03-03 17:03:34.995792 test begin: paddle.cast(Tensor([2, 0],"bool"), dtype="int32", )

[Pass] paddle.cast(Tensor([2, 0],"bool"), dtype="int32", )
2025-03-03 17:03:34.997427 test begin: paddle.cast(Tensor([2, 0],"bool"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 0],"bool"), dtype=Dtype(float64), )
2025-03-03 17:03:34.998984 test begin: paddle.cast(Tensor([2, 0],"bool"), dtype=VarType(float32), )

[Pass] paddle.cast(Tensor([2, 0],"bool"), dtype=VarType(float32), )
2025-03-03 17:03:35.000515 test begin: paddle.cast(Tensor([2, 0],"float32"), "float16", )

[Pass] paddle.cast(Tensor([2, 0],"float32"), "float16", )
2025-03-03 17:03:35.002078 test begin: paddle.cast(Tensor([2, 0],"float32"), "float64", )

[Pass] paddle.cast(Tensor([2, 0],"float32"), "float64", )
2025-03-03 17:03:35.003617 test begin: paddle.cast(Tensor([2, 0],"float32"), "uint8", )

[Pass] paddle.cast(Tensor([2, 0],"float32"), "uint8", )
2025-03-03 17:03:35.005314 test begin: paddle.cast(Tensor([2, 0],"float32"), dtype=Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([2, 0],"float32"), dtype=Dtype(bfloat16), )
2025-03-03 17:03:35.006898 test begin: paddle.cast(Tensor([2, 0],"float32"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 0],"float32"), dtype=Dtype(float64), )
2025-03-03 17:03:35.008492 test begin: paddle.cast(Tensor([2, 0],"int64"), "int32", )

[Pass] paddle.cast(Tensor([2, 0],"int64"), "int32", )
2025-03-03 17:03:35.010182 test begin: paddle.cast(Tensor([2, 1, 0, 1, 1],"bool"), Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 1, 0, 1, 1],"bool"), Dtype(float32), )
2025-03-03 17:03:35.011748 test begin: paddle.cast(Tensor([2, 1, 0, 1, 1],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 0, 1, 1],"bool"), Dtype(float64), )
2025-03-03 17:03:35.013318 test begin: paddle.cast(Tensor([2, 1, 0, 114],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([2, 1, 0, 114],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:35.014855 test begin: paddle.cast(Tensor([2, 1, 0, 1],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 0, 1],"bool"), Dtype(float64), )
2025-03-03 17:03:35.016389 test begin: paddle.cast(Tensor([2, 1, 0, 2, 3],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 1, 0, 2, 3],"float32"), Dtype(float32), )
2025-03-03 17:03:35.017906 test begin: paddle.cast(Tensor([2, 1, 0, 2, 3],"float64"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 0, 2, 3],"float64"), Dtype(float64), )
2025-03-03 17:03:35.019505 test begin: paddle.cast(Tensor([2, 1, 0, 2],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 0, 2],"bool"), Dtype(float64), )
2025-03-03 17:03:35.021008 test begin: paddle.cast(Tensor([2, 1, 0, 2],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([2, 1, 0, 2],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:35.022802 test begin: paddle.cast(Tensor([2, 1, 0],"float32"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([2, 1, 0],"float32"), dtype=Dtype(float16), )
2025-03-03 17:03:35.024339 test begin: paddle.cast(Tensor([2, 1, 1, 0, 1],"bool"), Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 1, 1, 0, 1],"bool"), Dtype(float32), )
2025-03-03 17:03:35.025945 test begin: paddle.cast(Tensor([2, 1, 1, 0, 1],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 1, 0, 1],"bool"), Dtype(float64), )
2025-03-03 17:03:35.027952 test begin: paddle.cast(Tensor([2, 1, 1, 0, 3],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 1, 1, 0, 3],"float32"), Dtype(float32), )
2025-03-03 17:03:35.030294 test begin: paddle.cast(Tensor([2, 1, 1, 0, 3],"float64"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 1, 0, 3],"float64"), Dtype(float64), )
2025-03-03 17:03:35.031900 test begin: paddle.cast(Tensor([2, 1, 1, 0],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 1, 0],"bool"), Dtype(float64), )
2025-03-03 17:03:35.033571 test begin: paddle.cast(Tensor([2, 1, 1, 0],"float32"), Dtype(bfloat16), )

[Pass] paddle.cast(Tensor([2, 1, 1, 0],"float32"), Dtype(bfloat16), )
2025-03-03 17:03:35.035177 test begin: paddle.cast(Tensor([2, 1, 1, 1, 0],"bool"), Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 1, 1, 1, 0],"bool"), Dtype(float32), )
2025-03-03 17:03:35.036696 test begin: paddle.cast(Tensor([2, 1, 1, 1, 0],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 1, 1, 0],"bool"), Dtype(float64), )
2025-03-03 17:03:35.038203 test begin: paddle.cast(Tensor([2, 1, 1, 2, 0],"float32"), Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 1, 1, 2, 0],"float32"), Dtype(float32), )
2025-03-03 17:03:35.039728 test begin: paddle.cast(Tensor([2, 1, 1, 2, 0],"float64"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 1, 2, 0],"float64"), Dtype(float64), )
2025-03-03 17:03:35.041284 test begin: paddle.cast(Tensor([2, 1, 2, 0],"bool"), Dtype(float64), )

[Pass] paddle.cast(Tensor([2, 1, 2, 0],"bool"), Dtype(float64), )
2025-03-03 17:03:35.042835 test begin: paddle.cast(Tensor([2, 1, 2, 0],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([2, 1, 2, 0],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:35.044469 test begin: paddle.cast(Tensor([2, 1024, 0, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([2, 1024, 0, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:35.046043 test begin: paddle.cast(Tensor([2, 1024, 1024, 0],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([2, 1024, 1024, 0],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:35.048532 test begin: paddle.cast(Tensor([2, 2, 0, 1],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([2, 2, 0, 1],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:35.050874 test begin: paddle.cast(Tensor([2, 2, 2, 0],"bool"), dtype=Dtype(int64), )

[Pass] paddle.cast(Tensor([2, 2, 2, 0],"bool"), dtype=Dtype(int64), )
2025-03-03 17:03:35.053213 test begin: paddle.cast(Tensor([2, 20, 0],"int32"), "bool", )

[Pass] paddle.cast(Tensor([2, 20, 0],"int32"), "bool", )
2025-03-03 17:03:35.054826 test begin: paddle.cast(Tensor([2, 3, 0, 5],"bool"), "int64", )

[Pass] paddle.cast(Tensor([2, 3, 0, 5],"bool"), "int64", )
2025-03-03 17:03:35.056346 test begin: paddle.cast(Tensor([2, 3, 0, 5],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([2, 3, 0, 5],"bool"), dtype="bool", )
2025-03-03 17:03:35.057869 test begin: paddle.cast(Tensor([2, 3, 4, 0],"bool"), "int64", )

[Pass] paddle.cast(Tensor([2, 3, 4, 0],"bool"), "int64", )
2025-03-03 17:03:35.059389 test begin: paddle.cast(Tensor([2, 3, 4, 0],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([2, 3, 4, 0],"bool"), dtype="bool", )
2025-03-03 17:03:35.060984 test begin: paddle.cast(Tensor([2, 32, 0, 1],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 32, 0, 1],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:35.062920 test begin: paddle.cast(Tensor([2, 32, 0, 1],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([2, 32, 0, 1],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:35.064692 test begin: paddle.cast(Tensor([2, 32, 32, 0],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 32, 32, 0],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:35.066221 test begin: paddle.cast(Tensor([2, 32, 32, 0],"bool"), dtype=Dtype(int32), )

[Pass] paddle.cast(Tensor([2, 32, 32, 0],"bool"), dtype=Dtype(int32), )
2025-03-03 17:03:35.067806 test begin: paddle.cast(Tensor([2, 512, 0, 1],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([2, 512, 0, 1],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:35.069513 test begin: paddle.cast(Tensor([2, 512, 1024, 0],"bool"), dtype=VarType(int64), )

[Pass] paddle.cast(Tensor([2, 512, 1024, 0],"bool"), dtype=VarType(int64), )
2025-03-03 17:03:35.071435 test begin: paddle.cast(Tensor([2, 52, 0, 14],"float32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 52, 0, 14],"float32"), dtype=Dtype(float32), )
2025-03-03 17:03:35.073629 test begin: paddle.cast(Tensor([2, 52, 7, 0],"float32"), dtype=Dtype(float32), )

[Pass] paddle.cast(Tensor([2, 52, 7, 0],"float32"), dtype=Dtype(float32), )
2025-03-03 17:03:35.075932 test begin: paddle.cast(Tensor([22016, 0],"float16"), "int8", )

[Pass] paddle.cast(Tensor([22016, 0],"float16"), "int8", )
2025-03-03 17:03:35.078276 test begin: paddle.cast(Tensor([224, 0, 3],"uint8"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([224, 0, 3],"uint8"), type(numpy.float32), )
2025-03-03 17:03:35.080883 test begin: paddle.cast(Tensor([224, 224, 0],"uint8"), type(numpy.float32), )

[Pass] paddle.cast(Tensor([224, 224, 0],"uint8"), type(numpy.float32), )
2025-03-03 17:03:35.091994 test begin: paddle.cast(Tensor([3, 0, 2],"bool"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([3, 0, 2],"bool"), dtype=Dtype(float64), )
2025-03-03 17:03:35.094572 test begin: paddle.cast(Tensor([3, 0, 2],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([3, 0, 2],"float64"), dtype="float64", )
2025-03-03 17:03:35.097093 test begin: paddle.cast(Tensor([3, 0, 96],"float32"), dtype="int32", )

[Pass] paddle.cast(Tensor([3, 0, 96],"float32"), dtype="int32", )
2025-03-03 17:03:35.100269 test begin: paddle.cast(Tensor([3, 0],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([3, 0],"bool"), dtype="int", )
2025-03-03 17:03:35.104006 test begin: paddle.cast(Tensor([3, 0],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([3, 0],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:35.107718 test begin: paddle.cast(Tensor([3, 1, 0],"float64"), dtype="float64", )

[Pass] paddle.cast(Tensor([3, 1, 0],"float64"), dtype="float64", )
2025-03-03 17:03:35.110007 test begin: paddle.cast(Tensor([3, 128, 0],"float32"), dtype="int32", )

[Pass] paddle.cast(Tensor([3, 128, 0],"float32"), dtype="int32", )
2025-03-03 17:03:35.112282 test begin: paddle.cast(Tensor([3, 4, 0],"bool"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([3, 4, 0],"bool"), dtype=Dtype(float64), )
2025-03-03 17:03:35.115698 test begin: paddle.cast(Tensor([32, 0],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(Tensor([32, 0],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:35.117480 test begin: paddle.cast(Tensor([4, 0, 2],"int32"), dtype=VarType(float32), )

[Pass] paddle.cast(Tensor([4, 0, 2],"int32"), dtype=VarType(float32), )
2025-03-03 17:03:35.119418 test begin: paddle.cast(Tensor([4, 0],"float32"), "complex64", )

[Pass] paddle.cast(Tensor([4, 0],"float32"), "complex64", )
2025-03-03 17:03:35.121256 test begin: paddle.cast(Tensor([4, 128, 0],"int32"), dtype=VarType(float32), )

[Pass] paddle.cast(Tensor([4, 128, 0],"int32"), dtype=VarType(float32), )
2025-03-03 17:03:35.123172 test begin: paddle.cast(Tensor([5, 0, 6],"float64"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([5, 0, 6],"float64"), dtype=Dtype(float64), )
2025-03-03 17:03:35.124930 test begin: paddle.cast(Tensor([5, 0],"float32"), "float64", )

[Pass] paddle.cast(Tensor([5, 0],"float32"), "float64", )
2025-03-03 17:03:35.126497 test begin: paddle.cast(Tensor([5, 6, 0],"float64"), dtype=Dtype(float64), )

[Pass] paddle.cast(Tensor([5, 6, 0],"float64"), dtype=Dtype(float64), )
2025-03-03 17:03:35.128150 test begin: paddle.cast(Tensor([6, 0, 9, 18],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([6, 0, 9, 18],"bool"), dtype="int", )
2025-03-03 17:03:35.130216 test begin: paddle.cast(Tensor([6, 8, 0, 18],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([6, 8, 0, 18],"bool"), dtype="int", )
2025-03-03 17:03:35.133203 test begin: paddle.cast(Tensor([6, 8, 9, 0],"bool"), dtype="int", )

[Pass] paddle.cast(Tensor([6, 8, 9, 0],"bool"), dtype="int", )
2025-03-03 17:03:35.135160 test begin: paddle.cast(Tensor([7, 0, 14],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([7, 0, 14],"bool"), dtype="int64", )
2025-03-03 17:03:35.138335 test begin: paddle.cast(Tensor([7, 7, 0],"bool"), dtype="int64", )

[Pass] paddle.cast(Tensor([7, 7, 0],"bool"), dtype="int64", )
2025-03-03 17:03:35.140204 test begin: paddle.cast(Tensor([8, 0, 5, 6, 7],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([8, 0, 5, 6, 7],"bool"), dtype="bool", )
2025-03-03 17:03:35.142407 test begin: paddle.cast(Tensor([8, 17, 0, 6, 7],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([8, 17, 0, 6, 7],"bool"), dtype="bool", )
2025-03-03 17:03:35.144860 test begin: paddle.cast(Tensor([8, 17, 5, 0, 7],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([8, 17, 5, 0, 7],"bool"), dtype="bool", )
2025-03-03 17:03:35.146464 test begin: paddle.cast(Tensor([8, 17, 5, 6, 0],"bool"), dtype="bool", )

[Pass] paddle.cast(Tensor([8, 17, 5, 6, 0],"bool"), dtype="bool", )
2025-03-03 17:03:35.147935 test begin: paddle.cast(x=Tensor([0, 100, 100],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(x=Tensor([0, 100, 100],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:35.149319 test begin: paddle.cast(x=Tensor([0, 100, 100],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(x=Tensor([0, 100, 100],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:35.150860 test begin: paddle.cast(x=Tensor([0, 1],"int64"), dtype="int32", )

[Pass] paddle.cast(x=Tensor([0, 1],"int64"), dtype="int32", )
2025-03-03 17:03:35.152674 test begin: paddle.cast(x=Tensor([0, 3, 3],"float32"), dtype="float64", )

[Pass] paddle.cast(x=Tensor([0, 3, 3],"float32"), dtype="float64", )
2025-03-03 17:03:35.153990 test begin: paddle.cast(x=Tensor([0, 3, 3],"float64"), dtype="float64", )

[Pass] paddle.cast(x=Tensor([0, 3, 3],"float64"), dtype="float64", )
2025-03-03 17:03:35.155548 test begin: paddle.cast(x=Tensor([0, 93431],"bool"), dtype="float32", )

[Pass] paddle.cast(x=Tensor([0, 93431],"bool"), dtype="float32", )
2025-03-03 17:03:35.156920 test begin: paddle.cast(x=Tensor([0],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(x=Tensor([0],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:35.158280 test begin: paddle.cast(x=Tensor([0],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(x=Tensor([0],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:35.160330 test begin: paddle.cast(x=Tensor([104, 0],"bool"), dtype="float32", )

[Pass] paddle.cast(x=Tensor([104, 0],"bool"), dtype="float32", )
2025-03-03 17:03:35.161669 test begin: paddle.cast(x=Tensor([1242, 0],"int64"), dtype="int32", )

[Pass] paddle.cast(x=Tensor([1242, 0],"int64"), dtype="int32", )
2025-03-03 17:03:35.163078 test begin: paddle.cast(x=Tensor([128, 0],"bool"), dtype="float32", )

[Pass] paddle.cast(x=Tensor([128, 0],"bool"), dtype="float32", )
2025-03-03 17:03:35.164666 test begin: paddle.cast(x=Tensor([3, 0, 3],"float32"), dtype="float64", )

[Pass] paddle.cast(x=Tensor([3, 0, 3],"float32"), dtype="float64", )
2025-03-03 17:03:35.170875 test begin: paddle.cast(x=Tensor([3, 0, 3],"float64"), dtype="float64", )

[Pass] paddle.cast(x=Tensor([3, 0, 3],"float64"), dtype="float64", )
2025-03-03 17:03:35.184837 test begin: paddle.cast(x=Tensor([3, 3, 0],"float32"), dtype="float64", )

[Pass] paddle.cast(x=Tensor([3, 3, 0],"float32"), dtype="float64", )
2025-03-03 17:03:35.197282 test begin: paddle.cast(x=Tensor([3, 3, 0],"float64"), dtype="float64", )

[Pass] paddle.cast(x=Tensor([3, 3, 0],"float64"), dtype="float64", )
2025-03-03 17:03:35.213772 test begin: paddle.cast(x=Tensor([8, 0, 100],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(x=Tensor([8, 0, 100],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:35.244289 test begin: paddle.cast(x=Tensor([8, 0, 100],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(x=Tensor([8, 0, 100],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:35.253670 test begin: paddle.cast(x=Tensor([8, 100, 0],"bool"), dtype=Dtype(float16), )

[Pass] paddle.cast(x=Tensor([8, 100, 0],"bool"), dtype=Dtype(float16), )
2025-03-03 17:03:35.257629 test begin: paddle.cast(x=Tensor([8, 100, 0],"bool"), dtype=Dtype(float32), )

[Pass] paddle.cast(x=Tensor([8, 100, 0],"bool"), dtype=Dtype(float32), )
2025-03-03 17:03:35.260572 test begin: paddle.clip(x=Tensor([0],"float64"), )

[cuda error] paddle.clip(x=Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:03:35.267179 test begin: paddle.count_nonzero(Tensor([0, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )

[Pass] paddle.count_nonzero(Tensor([0, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
2025-03-03 17:03:35.279523 test begin: paddle.count_nonzero(Tensor([0, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )

[Pass] paddle.count_nonzero(Tensor([0, 14, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
2025-03-03 17:03:35.282804 test begin: paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=-1, keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=-1, keepdim=False, )
2025-03-03 17:03:35.288194 test begin: paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=2, keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=2, keepdim=True, )
2025-03-03 17:03:35.290951 test begin: paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=None, keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=None, keepdim=False, )
2025-03-03 17:03:35.293815 test begin: paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=None, keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=None, keepdim=True, )
2025-03-03 17:03:35.299205 test begin: paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, )
2025-03-03 17:03:35.301477 test begin: paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=list[0,2,], keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=list[0,2,], keepdim=False, )
2025-03-03 17:03:35.303635 test begin: paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=tuple(0,1,3,), keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=tuple(0,1,3,), keepdim=False, )
2025-03-03 17:03:35.305797 test begin: paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=tuple(0,2,), keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([0, 3, 4, 5],"float32"), axis=tuple(0,2,), keepdim=False, )
2025-03-03 17:03:35.307935 test begin: paddle.count_nonzero(Tensor([0, 5],"float32"), keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([0, 5],"float32"), keepdim=True, )
2025-03-03 17:03:35.310152 test begin: paddle.count_nonzero(Tensor([1, 0, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )

[Pass] paddle.count_nonzero(Tensor([1, 0, 5, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
2025-03-03 17:03:35.312226 test begin: paddle.count_nonzero(Tensor([1, 0, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )

[Pass] paddle.count_nonzero(Tensor([1, 0, 5, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
2025-03-03 17:03:35.314213 test begin: paddle.count_nonzero(Tensor([1, 14, 0, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )

[Pass] paddle.count_nonzero(Tensor([1, 14, 0, 14],"float64"), axis=list[1,3,], keepdim=False, name=None, )
2025-03-03 17:03:35.317102 test begin: paddle.count_nonzero(Tensor([1, 14, 0, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )

[Pass] paddle.count_nonzero(Tensor([1, 14, 0, 14],"float64"), axis=list[1,3,], keepdim=True, name=None, )
2025-03-03 17:03:35.320484 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 0],"float64"), axis=list[1,3,], keepdim=False, name=None, )

[Pass] paddle.count_nonzero(Tensor([1, 14, 5, 0],"float64"), axis=list[1,3,], keepdim=False, name=None, )
2025-03-03 17:03:35.324085 test begin: paddle.count_nonzero(Tensor([1, 14, 5, 0],"float64"), axis=list[1,3,], keepdim=True, name=None, )

[Pass] paddle.count_nonzero(Tensor([1, 14, 5, 0],"float64"), axis=list[1,3,], keepdim=True, name=None, )
2025-03-03 17:03:35.328068 test begin: paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=-1, keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=-1, keepdim=False, )
2025-03-03 17:03:35.330517 test begin: paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=2, keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=2, keepdim=True, )
2025-03-03 17:03:35.332646 test begin: paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=None, keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=None, keepdim=False, )
2025-03-03 17:03:35.335761 test begin: paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=None, keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=None, keepdim=True, )
2025-03-03 17:03:35.339182 test begin: paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, )
2025-03-03 17:03:35.341498 test begin: paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=list[0,2,], keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=list[0,2,], keepdim=False, )
2025-03-03 17:03:35.344486 test begin: paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=tuple(0,1,3,), keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=tuple(0,1,3,), keepdim=False, )
2025-03-03 17:03:35.346547 test begin: paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=tuple(0,2,), keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 0, 4, 5],"float32"), axis=tuple(0,2,), keepdim=False, )
2025-03-03 17:03:35.348229 test begin: paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=-1, keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=-1, keepdim=False, )
2025-03-03 17:03:35.350380 test begin: paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=2, keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=2, keepdim=True, )
2025-03-03 17:03:35.355097 test begin: paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=None, keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=None, keepdim=False, )
2025-03-03 17:03:35.358296 test begin: paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=None, keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=None, keepdim=True, )
2025-03-03 17:03:35.361509 test begin: paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=list[0,1,2,3,], keepdim=False, )
2025-03-03 17:03:35.365502 test begin: paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=list[0,2,], keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=list[0,2,], keepdim=False, )
2025-03-03 17:03:35.367730 test begin: paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=tuple(0,1,3,), keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=tuple(0,1,3,), keepdim=False, )
2025-03-03 17:03:35.370836 test begin: paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=tuple(0,2,), keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 0, 5],"float32"), axis=tuple(0,2,), keepdim=False, )
2025-03-03 17:03:35.373319 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=-1, keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=-1, keepdim=False, )
2025-03-03 17:03:35.375947 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=2, keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=2, keepdim=True, )
2025-03-03 17:03:35.377707 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=None, keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=None, keepdim=False, )
2025-03-03 17:03:35.379526 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=None, keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=None, keepdim=True, )
2025-03-03 17:03:35.382353 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=list[0,1,2,3,], keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=list[0,1,2,3,], keepdim=False, )
2025-03-03 17:03:35.385131 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=list[0,2,], keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=list[0,2,], keepdim=False, )
2025-03-03 17:03:35.387381 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=tuple(0,1,3,), keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=tuple(0,1,3,), keepdim=False, )
2025-03-03 17:03:35.389200 test begin: paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=tuple(0,2,), keepdim=False, )

[Pass] paddle.count_nonzero(Tensor([2, 3, 4, 0],"float32"), axis=tuple(0,2,), keepdim=False, )
2025-03-03 17:03:35.392061 test begin: paddle.count_nonzero(Tensor([3, 0],"float32"), keepdim=True, )

[Pass] paddle.count_nonzero(Tensor([3, 0],"float32"), keepdim=True, )
2025-03-03 17:03:35.393874 test begin: paddle.crop(x=Tensor([0, 3, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )

[paddle error] paddle.crop(x=Tensor([0, 3, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 
 (InvalidArgument) The sum of the 0th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 0th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.396361 test begin: paddle.crop(x=Tensor([0, 3],"float32"), shape=list[2,2,], )

[paddle error] paddle.crop(x=Tensor([0, 3],"float32"), shape=list[2,2,], ) 
 (InvalidArgument) The sum of the 0th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 0th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.398820 test begin: paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], )

[paddle error] paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], ) 
 (InvalidArgument) The sum of the 0th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 0th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.400943 test begin: paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], offsets=list[1,0,], )

[paddle error] paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], offsets=list[1,0,], ) 
 (InvalidArgument) The sum of the 0th elements of offsets (1) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 0th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:3 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.403022 test begin: paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], offsets=tuple(0,0,), )

[paddle error] paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], offsets=tuple(0,0,), ) 
 (InvalidArgument) The sum of the 0th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 0th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.405175 test begin: paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], offsets=tuple(0,1,), )

[paddle error] paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], offsets=tuple(0,1,), ) 
 (InvalidArgument) The sum of the 0th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 0th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.407342 test begin: paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], offsets=tuple(1,1,), )

[paddle error] paddle.crop(x=Tensor([0, 3],"float64"), shape=list[2,2,], offsets=tuple(1,1,), ) 
 (InvalidArgument) The sum of the 0th elements of offsets (1) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 0th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:3 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.409723 test begin: paddle.crop(x=Tensor([2, 0, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )

[paddle error] paddle.crop(x=Tensor([2, 0, 3, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 
 (InvalidArgument) The sum of the 1th elements of offsets (0) and shape (1) of Op(crop_tensor) should be less than or equal to the size of 1th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:1 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.412174 test begin: paddle.crop(x=Tensor([2, 3, 0, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )

[paddle error] paddle.crop(x=Tensor([2, 3, 0, 3],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-03 17:03:35.413249 test begin: paddle.crop(x=Tensor([2, 3, 3, 0],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], )

[paddle error] paddle.crop(x=Tensor([2, 3, 3, 0],"float64"), shape=list[2,1,-1,2,], offsets=list[0,0,1,1,], ) 
 (InvalidArgument) The sum of the 3th elements of offsets (1) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 3th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:3 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.415284 test begin: paddle.crop(x=Tensor([3, 0],"float32"), shape=list[2,2,], )

[paddle error] paddle.crop(x=Tensor([3, 0],"float32"), shape=list[2,2,], ) 
 (InvalidArgument) The sum of the 1th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 1th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.417314 test begin: paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], )

[paddle error] paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], ) 
 (InvalidArgument) The sum of the 1th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 1th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.419249 test begin: paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], offsets=list[1,0,], )

[paddle error] paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], offsets=list[1,0,], ) 
 (InvalidArgument) The sum of the 1th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 1th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.421080 test begin: paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], offsets=tuple(0,0,), )

[paddle error] paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], offsets=tuple(0,0,), ) 
 (InvalidArgument) The sum of the 1th elements of offsets (0) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 1th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:2 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.422879 test begin: paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], offsets=tuple(0,1,), )

[paddle error] paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], offsets=tuple(0,1,), ) 
 (InvalidArgument) The sum of the 1th elements of offsets (1) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 1th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:3 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.425621 test begin: paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], offsets=tuple(1,1,), )

[paddle error] paddle.crop(x=Tensor([3, 0],"float64"), shape=list[2,2,], offsets=tuple(1,1,), ) 
 (InvalidArgument) The sum of the 1th elements of offsets (1) and shape (2) of Op(crop_tensor) should be less than or equal to the size of 1th dimension of the input.
  [Hint: Expected offsets_vec[i] + shape_vec[i] <= x_dims[i], but received offsets_vec[i] + shape_vec[i]:3 > x_dims[i]:0.] (at ../paddle/phi/kernels/impl/crop_kernel_impl.h:123)

2025-03-03 17:03:35.427554 test begin: paddle.cummax(Tensor([0, 100],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cummax(_object*, _object*, _object*)
1   cummax_ad_func(paddle::Tensor const&, int, phi::DataType)
2   paddle::experimental::cummax(paddle::Tensor const&, int, phi::DataType)
3   void phi::ScanWithIndicesKernel<float, long, std::greater_equal<float>, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, float, phi::DenseTensor*, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992615 (unix time) try "date -d @1740992615" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5c23fd7da8) received by PID 123644 (TID 0x7f5b39f48700) from PID 603815336 ***]

2025-03-03 17:03:47.164643 test begin: paddle.cummax(Tensor([0, 100],"float32"), axis=-1, )

W0303 17:03:50.945973 132980 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:03:50.946961 132980 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.cummax(Tensor([0, 100],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:03:50.953093 test begin: paddle.cummax(Tensor([0, 100],"float32"), axis=-2, )

[Pass] paddle.cummax(Tensor([0, 100],"float32"), axis=-2, )
2025-03-03 17:03:50.956403 test begin: paddle.cummax(Tensor([0, 100],"float32"), axis=-2, dtype="int32", )

[Pass] paddle.cummax(Tensor([0, 100],"float32"), axis=-2, dtype="int32", )
2025-03-03 17:03:50.958793 test begin: paddle.cummax(Tensor([0, 100],"float32"), axis=0, )

[Pass] paddle.cummax(Tensor([0, 100],"float32"), axis=0, )
2025-03-03 17:03:50.960921 test begin: paddle.cummax(Tensor([0, 100],"int32"), axis=0, )

[Pass] paddle.cummax(Tensor([0, 100],"int32"), axis=0, )
2025-03-03 17:03:50.962964 test begin: paddle.cummax(Tensor([100, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cummax(_object*, _object*, _object*)
1   cummax_ad_func(paddle::Tensor const&, int, phi::DataType)
2   paddle::experimental::cummax(paddle::Tensor const&, int, phi::DataType)
3   void phi::ScanWithIndicesKernel<float, long, std::greater_equal<float>, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, float, phi::DenseTensor*, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992630 (unix time) try "date -d @1740992630" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4c8e22bda8) received by PID 131863 (TID 0x7f4bbfdc2700) from PID 18446744071799225768 ***]

2025-03-03 17:03:56.298708 test begin: paddle.cummax(Tensor([100, 0],"float32"), axis=-1, )

W0303 17:03:59.401836 141059 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:03:59.403522 141059 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cummax(_object*, _object*, _object*)
1   cummax_ad_func(paddle::Tensor const&, int, phi::DataType)
2   paddle::experimental::cummax(paddle::Tensor const&, int, phi::DataType)
3   void phi::ScanWithIndicesKernel<float, long, std::greater_equal<float>, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, float, phi::DenseTensor*, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992639 (unix time) try "date -d @1740992639" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f57a31f5da8) received by PID 137004 (TID 0x7f56ab4f4700) from PID 18446744072151326120 ***]

2025-03-03 17:04:13.497120 test begin: paddle.cummax(Tensor([100, 0],"float32"), axis=-2, )

W0303 17:04:18.357525 145178 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:04:18.358528 145178 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.cummax(Tensor([100, 0],"float32"), axis=-2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:18.370446 test begin: paddle.cummax(Tensor([100, 0],"float32"), axis=-2, dtype="int32", )

[cuda error] paddle.cummax(Tensor([100, 0],"float32"), axis=-2, dtype="int32", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:18.373537 test begin: paddle.cummax(Tensor([100, 0],"float32"), axis=0, )

[cuda error] paddle.cummax(Tensor([100, 0],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:18.375321 test begin: paddle.cummax(Tensor([100, 0],"int32"), axis=0, )

[cuda error] paddle.cummax(Tensor([100, 0],"int32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:18.376824 test begin: paddle.cummin(Tensor([0, 100],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cummin(_object*, _object*, _object*)
1   cummin_ad_func(paddle::Tensor const&, int, phi::DataType)
2   paddle::experimental::cummin(paddle::Tensor const&, int, phi::DataType)
3   void phi::ScanWithIndicesKernel<float, long, std::less_equal<float>, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, float, phi::DenseTensor*, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992658 (unix time) try "date -d @1740992658" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f458c84adc8) received by PID 144670 (TID 0x7f448a949700) from PID 18446744071772089800 ***]

2025-03-03 17:04:23.415570 test begin: paddle.cummin(Tensor([0, 100],"float32"), axis=-1, )

W0303 17:04:26.728521 146511 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:04:26.729846 146511 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.cummin(Tensor([0, 100],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:26.747021 test begin: paddle.cummin(Tensor([0, 100],"float32"), axis=-2, )

[Pass] paddle.cummin(Tensor([0, 100],"float32"), axis=-2, )
2025-03-03 17:04:26.765334 test begin: paddle.cummin(Tensor([0, 100],"float32"), axis=-2, dtype="int32", )

[Pass] paddle.cummin(Tensor([0, 100],"float32"), axis=-2, dtype="int32", )
2025-03-03 17:04:26.773606 test begin: paddle.cummin(Tensor([0, 100],"float32"), axis=0, )

[Pass] paddle.cummin(Tensor([0, 100],"float32"), axis=0, )
2025-03-03 17:04:26.790592 test begin: paddle.cummin(Tensor([0, 100],"int32"), axis=0, )

[Pass] paddle.cummin(Tensor([0, 100],"int32"), axis=0, )
2025-03-03 17:04:26.799173 test begin: paddle.cummin(Tensor([100, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cummin(_object*, _object*, _object*)
1   cummin_ad_func(paddle::Tensor const&, int, phi::DataType)
2   paddle::experimental::cummin(paddle::Tensor const&, int, phi::DataType)
3   void phi::ScanWithIndicesKernel<float, long, std::less_equal<float>, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, float, phi::DenseTensor*, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992666 (unix time) try "date -d @1740992666" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f02a1363dc8) received by PID 145783 (TID 0x7f01c134a700) from PID 18446744072119270856 ***]

2025-03-03 17:04:31.000171 test begin: paddle.cummin(Tensor([100, 0],"float32"), axis=-1, )

W0303 17:04:33.843230 147568 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:04:33.844326 147568 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cummin(_object*, _object*, _object*)
1   cummin_ad_func(paddle::Tensor const&, int, phi::DataType)
2   paddle::experimental::cummin(paddle::Tensor const&, int, phi::DataType)
3   void phi::ScanWithIndicesKernel<float, long, std::less_equal<float>, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, float, phi::DenseTensor*, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992673 (unix time) try "date -d @1740992673" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f142f918dc8) received by PID 147222 (TID 0x7f1351abb700) from PID 798068168 ***]

2025-03-03 17:04:38.645436 test begin: paddle.cummin(Tensor([100, 0],"float32"), axis=-2, )

W0303 17:04:41.580816 148639 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:04:41.582016 148639 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.cummin(Tensor([100, 0],"float32"), axis=-2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:41.596112 test begin: paddle.cummin(Tensor([100, 0],"float32"), axis=-2, dtype="int32", )

[cuda error] paddle.cummin(Tensor([100, 0],"float32"), axis=-2, dtype="int32", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:41.599597 test begin: paddle.cummin(Tensor([100, 0],"float32"), axis=0, )

[cuda error] paddle.cummin(Tensor([100, 0],"float32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:41.614360 test begin: paddle.cummin(Tensor([100, 0],"int32"), axis=0, )

[cuda error] paddle.cummin(Tensor([100, 0],"int32"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:04:41.616701 test begin: paddle.cumsum(Tensor([0, 12],"float16"), dtype="float16", )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992681 (unix time) try "date -d @1740992681" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f42fc789bd8) received by PID 148052 (TID 0x7f4228949700) from PID 18446744073650346968 ***]

2025-03-03 17:04:53.229795 test begin: paddle.cumsum(Tensor([0, 12],"float32"), dtype="float32", )

W0303 17:04:56.544772 150549 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:04:56.545928 150549 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.cumsum(Tensor([0, 12],"float32"), dtype="float32", )
2025-03-03 17:04:56.555228 test begin: paddle.cumsum(Tensor([0, 1],"int32"), )

[Pass] paddle.cumsum(Tensor([0, 1],"int32"), )
2025-03-03 17:04:56.561738 test begin: paddle.cumsum(Tensor([0, 20, 1],"float32"), dtype="float32", )

[Pass] paddle.cumsum(Tensor([0, 20, 1],"float32"), dtype="float32", )
2025-03-03 17:04:56.567617 test begin: paddle.cumsum(Tensor([0, 4],"int64"), dtype="float64", )

[Pass] paddle.cumsum(Tensor([0, 4],"int64"), dtype="float64", )
2025-03-03 17:04:56.572775 test begin: paddle.cumsum(Tensor([0, 4],"int64"), dtype=type(numpy.int32), )

[Pass] paddle.cumsum(Tensor([0, 4],"int64"), dtype=type(numpy.int32), )
2025-03-03 17:04:56.578310 test begin: paddle.cumsum(Tensor([0],"float32"), dtype="float32", )

[Pass] paddle.cumsum(Tensor([0],"float32"), dtype="float32", )
2025-03-03 17:04:56.582810 test begin: paddle.cumsum(Tensor([0],"int32"), )

[Pass] paddle.cumsum(Tensor([0],"int32"), )
2025-03-03 17:04:56.588022 test begin: paddle.cumsum(Tensor([1, 0],"int32"), )

[Pass] paddle.cumsum(Tensor([1, 0],"int32"), )
2025-03-03 17:04:56.595415 test begin: paddle.cumsum(Tensor([10, 0, 1],"float32"), dtype="float32", )

[Pass] paddle.cumsum(Tensor([10, 0, 1],"float32"), dtype="float32", )
2025-03-03 17:04:56.606336 test begin: paddle.cumsum(Tensor([10, 0],"float16"), dtype="float16", )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992696 (unix time) try "date -d @1740992696" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2cd007bbd8) received by PID 149801 (TID 0x7f2baff48700) from PID 18446744072904752088 ***]

2025-03-03 17:05:01.752161 test begin: paddle.cumsum(Tensor([10, 0],"float32"), dtype="float32", )

W0303 17:05:05.194155 153722 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:05:05.195916 153722 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.cumsum(Tensor([10, 0],"float32"), dtype="float32", )
2025-03-03 17:05:05.209227 test begin: paddle.cumsum(Tensor([10, 20, 0],"float32"), dtype="float32", )

[Pass] paddle.cumsum(Tensor([10, 20, 0],"float32"), dtype="float32", )
2025-03-03 17:05:05.218893 test begin: paddle.cumsum(Tensor([2, 0],"int32"), )

[Pass] paddle.cumsum(Tensor([2, 0],"int32"), )
2025-03-03 17:05:05.225366 test begin: paddle.cumsum(Tensor([3, 0],"int64"), dtype="float64", )

[Pass] paddle.cumsum(Tensor([3, 0],"int64"), dtype="float64", )
2025-03-03 17:05:05.233165 test begin: paddle.cumsum(Tensor([3, 0],"int64"), dtype=type(numpy.int32), )

[Pass] paddle.cumsum(Tensor([3, 0],"int64"), dtype=type(numpy.int32), )
2025-03-03 17:05:05.239379 test begin: paddle.cumsum(x=Tensor([0, 2, 1, 3],"float64"), axis=Tensor([1],"float64"), )

[Pass] paddle.cumsum(x=Tensor([0, 2, 1, 3],"float64"), axis=Tensor([1],"float64"), )
2025-03-03 17:05:05.244191 test begin: paddle.cumsum(x=Tensor([0],"float32"), )

[Pass] paddle.cumsum(x=Tensor([0],"float32"), )
2025-03-03 17:05:05.253502 test begin: paddle.cumsum(x=Tensor([0],"float64"), )

[Pass] paddle.cumsum(x=Tensor([0],"float64"), )
2025-03-03 17:05:05.259617 test begin: paddle.cumsum(x=Tensor([1, 0, 1, 3],"float64"), axis=Tensor([1],"float64"), )

[cuda error] paddle.cumsum(x=Tensor([1, 0, 1, 3],"float64"), axis=Tensor([1],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:05:05.263095 test begin: paddle.cumsum(x=Tensor([1, 2, 0, 3],"float64"), axis=Tensor([1],"float64"), )

[cuda error] paddle.cumsum(x=Tensor([1, 2, 0, 3],"float64"), axis=Tensor([1],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:05:05.266649 test begin: paddle.cumsum(x=Tensor([1, 2, 1, 0],"float64"), axis=Tensor([1],"float64"), )

[cuda error] paddle.cumsum(x=Tensor([1, 2, 1, 0],"float64"), axis=Tensor([1],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 17:05:05.274261 test begin: paddle.cumsum(x=Tensor([1, 2, 1, 3],"float64"), axis=Tensor([0],"float64"), )

[paddle error] paddle.cumsum(x=Tensor([1, 2, 1, 3],"float64"), axis=Tensor([0],"float64"), ) 
 (InvalidArgument) The Scalar only supports Tensor with 1 element, but now Tensor has `0` elements
  [Hint: Expected tensor_in.numel() == 1, but received tensor_in.numel():0 != 1:1.] (at ../paddle/phi/api/lib/scalar.cc:32)

2025-03-03 17:05:05.282244 test begin: paddle.cumulative_trapezoid(y=Tensor([0, 3, 4],"float32"), x=Tensor([3],"float32"), dx=None, axis=1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([0, 3, 4],"float32"), x=Tensor([3],"float32"), dx=None, axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:65)

2025-03-03 17:05:05.290840 test begin: paddle.cumulative_trapezoid(y=Tensor([0, 3],"float32"), x=None, dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([0, 3],"float32"), x=None, dx=None, axis=-1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:65)

2025-03-03 17:05:05.299768 test begin: paddle.cumulative_trapezoid(y=Tensor([0, 3],"float32"), x=Tensor([0, 3],"float32"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([0, 3],"float32"), x=Tensor([0, 3],"float32"), dx=None, axis=-1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:65)

2025-03-03 17:05:05.312020 test begin: paddle.cumulative_trapezoid(y=Tensor([0, 3],"float32"), x=Tensor([2, 3],"float32"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([0, 3],"float32"), x=Tensor([2, 3],"float32"), dx=None, axis=-1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:65)

2025-03-03 17:05:05.318847 test begin: paddle.cumulative_trapezoid(y=Tensor([0, 3],"float64"), x=None, dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([0, 3],"float64"), x=None, dx=None, axis=-1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:65)

2025-03-03 17:05:05.322542 test begin: paddle.cumulative_trapezoid(y=Tensor([0, 3],"float64"), x=Tensor([0, 3],"float64"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([0, 3],"float64"), x=Tensor([0, 3],"float64"), dx=None, axis=-1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:65)

2025-03-03 17:05:05.325671 test begin: paddle.cumulative_trapezoid(y=Tensor([0, 3],"float64"), x=Tensor([2, 3],"float64"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([0, 3],"float64"), x=Tensor([2, 3],"float64"), dx=None, axis=-1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:65)

2025-03-03 17:05:05.329493 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 0],"float32"), x=None, dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 0],"float32"), x=None, dx=None, axis=-1, ) 
 (InvalidArgument) The step should be less than 0 while start > end.
  [Hint: Expected step < 0, but received step:1 >= 0:0.] (at ../paddle/phi/kernels/funcs/range_function.h:40)

2025-03-03 17:05:05.332008 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 0],"float32"), x=Tensor([2, 0],"float32"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 0],"float32"), x=Tensor([2, 0],"float32"), dx=None, axis=-1, ) 
 (InvalidArgument) The step should be less than 0 while start > end.
  [Hint: Expected step < 0, but received step:1 >= 0:0.] (at ../paddle/phi/kernels/funcs/range_function.h:40)

2025-03-03 17:05:05.333189 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 0],"float32"), x=Tensor([2, 3],"float32"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 0],"float32"), x=Tensor([2, 3],"float32"), dx=None, axis=-1, ) 
 (InvalidArgument) The step should be less than 0 while start > end.
  [Hint: Expected step < 0, but received step:1 >= 0:0.] (at ../paddle/phi/kernels/funcs/range_function.h:40)

2025-03-03 17:05:05.336516 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 0],"float64"), x=None, dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 0],"float64"), x=None, dx=None, axis=-1, ) 
 (InvalidArgument) The step should be less than 0 while start > end.
  [Hint: Expected step < 0, but received step:1 >= 0:0.] (at ../paddle/phi/kernels/funcs/range_function.h:40)

2025-03-03 17:05:05.338332 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 0],"float64"), x=Tensor([2, 0],"float64"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 0],"float64"), x=Tensor([2, 0],"float64"), dx=None, axis=-1, ) 
 (InvalidArgument) The step should be less than 0 while start > end.
  [Hint: Expected step < 0, but received step:1 >= 0:0.] (at ../paddle/phi/kernels/funcs/range_function.h:40)

2025-03-03 17:05:05.339344 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 0],"float64"), x=Tensor([2, 3],"float64"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 0],"float64"), x=Tensor([2, 3],"float64"), dx=None, axis=-1, ) 
 (InvalidArgument) The step should be less than 0 while start > end.
  [Hint: Expected step < 0, but received step:1 >= 0:0.] (at ../paddle/phi/kernels/funcs/range_function.h:40)

2025-03-03 17:05:05.342033 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 3],"float32"), x=Tensor([0, 3],"float32"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 3],"float32"), x=Tensor([0, 3],"float32"), dx=None, axis=-1, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 2] and the shape of Y = [0, 2]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.350080 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 3],"float32"), x=Tensor([2, 0],"float32"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 3],"float32"), x=Tensor([2, 0],"float32"), dx=None, axis=-1, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 2] and the shape of Y = [2, 0]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.355938 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 3],"float64"), x=Tensor([0, 3],"float64"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 3],"float64"), x=Tensor([0, 3],"float64"), dx=None, axis=-1, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 2] and the shape of Y = [0, 2]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.359701 test begin: paddle.cumulative_trapezoid(y=Tensor([2, 3],"float64"), x=Tensor([2, 0],"float64"), dx=None, axis=-1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([2, 3],"float64"), x=Tensor([2, 0],"float64"), dx=None, axis=-1, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 2] and the shape of Y = [2, 0]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.363987 test begin: paddle.cumulative_trapezoid(y=Tensor([3, 0, 4],"float32"), x=Tensor([3],"float32"), dx=None, axis=1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([3, 0, 4],"float32"), x=Tensor([3],"float32"), dx=None, axis=1, ) 
 (InvalidArgument) The step should be less than 0 while start > end.
  [Hint: Expected step < 0, but received step:1 >= 0:0.] (at ../paddle/phi/kernels/funcs/range_function.h:40)

2025-03-03 17:05:05.372115 test begin: paddle.cumulative_trapezoid(y=Tensor([3, 3, 0],"float32"), x=Tensor([3],"float32"), dx=None, axis=1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([3, 3, 0],"float32"), x=Tensor([3],"float32"), dx=None, axis=1, ) 
 (PreconditionNotMet) Tensor not initialized yet when DenseTensor::place() is called.
  [Hint: holder_ should not be null.] (at ../paddle/phi/core/dense_tensor_impl.cc:65)

2025-03-03 17:05:05.376337 test begin: paddle.cumulative_trapezoid(y=Tensor([3, 3, 4],"float32"), x=Tensor([0],"float32"), dx=None, axis=1, )

[paddle error] paddle.cumulative_trapezoid(y=Tensor([3, 3, 4],"float32"), x=Tensor([0],"float32"), dx=None, axis=1, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 2, 4] and the shape of Y = [1, 0, 1]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.381016 test begin: paddle.diag(Tensor([0],"float64"), padding_value=8, )

[Pass] paddle.diag(Tensor([0],"float64"), padding_value=8, )
2025-03-03 17:05:05.387155 test begin: paddle.diag(Tensor([0],"int64"), padding_value=-8, )

[Pass] paddle.diag(Tensor([0],"int64"), padding_value=-8, )
2025-03-03 17:05:05.390701 test begin: paddle.diag(Tensor([0],"int64"), padding_value=8.0, )

[Pass] paddle.diag(Tensor([0],"int64"), padding_value=8.0, )
2025-03-03 17:05:05.395253 test begin: paddle.diag(x=Tensor([0],"float64"), offset=-1, padding_value=-0.001, )

[Pass] paddle.diag(x=Tensor([0],"float64"), offset=-1, padding_value=-0.001, )
2025-03-03 17:05:05.398524 test begin: paddle.diag(x=Tensor([0],"float64"), offset=-2, padding_value=0, )

[Pass] paddle.diag(x=Tensor([0],"float64"), offset=-2, padding_value=0, )
2025-03-03 17:05:05.402135 test begin: paddle.diag(x=Tensor([0],"float64"), offset=1, padding_value=1.1, )

[Pass] paddle.diag(x=Tensor([0],"float64"), offset=1, padding_value=1.1, )
2025-03-03 17:05:05.405601 test begin: paddle.divide(Tensor([0, 1, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), )

[Pass] paddle.divide(Tensor([0, 1, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), )
2025-03-03 17:05:05.408932 test begin: paddle.divide(Tensor([0, 1, 1, 1],"float32"), Tensor([1, 1, 1, 1],"float32"), )

[Pass] paddle.divide(Tensor([0, 1, 1, 1],"float32"), Tensor([1, 1, 1, 1],"float32"), )
2025-03-03 17:05:05.412014 test begin: paddle.divide(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), )

[Pass] paddle.divide(Tensor([0, 1],"float32"), Tensor([0, 1],"float32"), )
2025-03-03 17:05:05.419222 test begin: paddle.divide(Tensor([0, 1],"float32"), Tensor([1, 1],"float32"), )

[Pass] paddle.divide(Tensor([0, 1],"float32"), Tensor([1, 1],"float32"), )
2025-03-03 17:05:05.427725 test begin: paddle.divide(Tensor([0, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )

[Pass] paddle.divide(Tensor([0, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )
2025-03-03 17:05:05.432053 test begin: paddle.divide(Tensor([0, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), )

[paddle error] paddle.divide(Tensor([0, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 20, 1] and the shape of Y = [10, 20, 1]. Received [0] in X is not equal to [10] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.434528 test begin: paddle.divide(Tensor([0, 3, 1, 5],"float32"), Tensor([0, 3, 1, 5],"float32"), )

[Pass] paddle.divide(Tensor([0, 3, 1, 5],"float32"), Tensor([0, 3, 1, 5],"float32"), )
2025-03-03 17:05:05.439145 test begin: paddle.divide(Tensor([0, 3, 1, 5],"float32"), Tensor([2, 3, 1, 5],"float32"), )

[paddle error] paddle.divide(Tensor([0, 3, 1, 5],"float32"), Tensor([2, 3, 1, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 1, 5] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.443860 test begin: paddle.divide(Tensor([0, 3, 3, 40, 40],"float32"), Tensor([0, 3, 3, 40, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([0, 3, 3, 40, 40],"float32"), Tensor([0, 3, 3, 40, 40],"float32"), name=None, )
2025-03-03 17:05:05.450144 test begin: paddle.divide(Tensor([0, 3, 3, 40, 40],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([0, 3, 3, 40, 40],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 3, 40, 40] and the shape of Y = [3, 3, 3, 40, 40]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.454936 test begin: paddle.divide(Tensor([0, 3, 3, 4],"float32"), Tensor([3, 1, 4],"float32"), )

[Pass] paddle.divide(Tensor([0, 3, 3, 4],"float32"), Tensor([3, 1, 4],"float32"), )
2025-03-03 17:05:05.459545 test begin: paddle.divide(Tensor([0, 3, 4, 5],"complex128"), Tensor([4, 5],"float64"), )

W0303 17:05:05.461349 157864 dygraph_functions.cc:87502] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.divide(Tensor([0, 3, 4, 5],"complex128"), Tensor([4, 5],"float64"), )
2025-03-03 17:05:05.462694 test begin: paddle.divide(Tensor([0, 3, 4, 5],"complex64"), Tensor([4, 5],"float32"), )

[Pass] paddle.divide(Tensor([0, 3, 4, 5],"complex64"), Tensor([4, 5],"float32"), )
2025-03-03 17:05:05.525247 test begin: paddle.divide(Tensor([0, 3, 4, 5],"float32"), Tensor([5],"complex64"), )

[Pass] paddle.divide(Tensor([0, 3, 4, 5],"float32"), Tensor([5],"complex64"), )
2025-03-03 17:05:05.529342 test begin: paddle.divide(Tensor([0, 3, 4, 5],"float64"), Tensor([5],"complex128"), )

[Pass] paddle.divide(Tensor([0, 3, 4, 5],"float64"), Tensor([5],"complex128"), )
2025-03-03 17:05:05.540337 test begin: paddle.divide(Tensor([0, 3, 40, 40, 3],"float32"), Tensor([0, 3, 40, 40, 3],"float32"), name=None, )

[Pass] paddle.divide(Tensor([0, 3, 40, 40, 3],"float32"), Tensor([0, 3, 40, 40, 3],"float32"), name=None, )
2025-03-03 17:05:05.543501 test begin: paddle.divide(Tensor([0, 3, 40, 40, 3],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([0, 3, 40, 40, 3],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 40, 40, 3] and the shape of Y = [3, 3, 40, 40, 3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.547112 test begin: paddle.divide(Tensor([0, 3, 40, 40],"float32"), Tensor([0, 3, 40, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([0, 3, 40, 40],"float32"), Tensor([0, 3, 40, 40],"float32"), name=None, )
2025-03-03 17:05:05.549551 test begin: paddle.divide(Tensor([0, 3, 40, 40],"float32"), Tensor([3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([0, 3, 40, 40],"float32"), Tensor([3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 40, 40] and the shape of Y = [3, 3, 40, 40]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.551324 test begin: paddle.divide(Tensor([0, 3, 4],"float32"), Tensor([0, 3, 4],"float32"), )

[Pass] paddle.divide(Tensor([0, 3, 4],"float32"), Tensor([0, 3, 4],"float32"), )
2025-03-03 17:05:05.554087 test begin: paddle.divide(Tensor([0, 3, 4],"float32"), Tensor([2, 3, 4],"float32"), )

[paddle error] paddle.divide(Tensor([0, 3, 4],"float32"), Tensor([2, 3, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 4] and the shape of Y = [2, 3, 4]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.556994 test begin: paddle.divide(Tensor([0, 3],"complex128"), Tensor([0, 3],"float64"), name="Normal_probs", )

[Pass] paddle.divide(Tensor([0, 3],"complex128"), Tensor([0, 3],"float64"), name="Normal_probs", )
2025-03-03 17:05:05.559844 test begin: paddle.divide(Tensor([0, 3],"complex128"), Tensor([2, 3],"float64"), name="Normal_probs", )

[paddle error] paddle.divide(Tensor([0, 3],"complex128"), Tensor([2, 3],"float64"), name="Normal_probs", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.562685 test begin: paddle.divide(Tensor([0, 3],"complex64"), Tensor([0, 3],"float32"), name="Normal_probs", )

[Pass] paddle.divide(Tensor([0, 3],"complex64"), Tensor([0, 3],"float32"), name="Normal_probs", )
2025-03-03 17:05:05.565014 test begin: paddle.divide(Tensor([0, 3],"complex64"), Tensor([2, 3],"float32"), name="Normal_probs", )

[paddle error] paddle.divide(Tensor([0, 3],"complex64"), Tensor([2, 3],"float32"), name="Normal_probs", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [2, 3]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.567833 test begin: paddle.divide(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), )

[Pass] paddle.divide(Tensor([0, 3],"float32"), Tensor([0, 3],"float32"), )
2025-03-03 17:05:05.569502 test begin: paddle.divide(Tensor([0, 3],"float32"), Tensor([1, 3],"float32"), )

[Pass] paddle.divide(Tensor([0, 3],"float32"), Tensor([1, 3],"float32"), )
2025-03-03 17:05:05.571716 test begin: paddle.divide(Tensor([0, 40, 40, 3],"float32"), Tensor([0, 40, 40, 3],"float32"), name=None, )

[Pass] paddle.divide(Tensor([0, 40, 40, 3],"float32"), Tensor([0, 40, 40, 3],"float32"), name=None, )
2025-03-03 17:05:05.573357 test begin: paddle.divide(Tensor([0, 40, 40, 3],"float32"), Tensor([3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([0, 40, 40, 3],"float32"), Tensor([3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 40, 40, 3] and the shape of Y = [3, 40, 40, 3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.576910 test begin: paddle.divide(Tensor([0, 40, 40],"float32"), Tensor([0, 40, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([0, 40, 40],"float32"), Tensor([0, 40, 40],"float32"), name=None, )
2025-03-03 17:05:05.579435 test begin: paddle.divide(Tensor([0, 40, 40],"float32"), Tensor([3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([0, 40, 40],"float32"), Tensor([3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 40, 40] and the shape of Y = [3, 40, 40]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.582243 test begin: paddle.divide(Tensor([0],"float32"), Tensor([0],"float32"), )

[Pass] paddle.divide(Tensor([0],"float32"), Tensor([0],"float32"), )
2025-03-03 17:05:05.584434 test begin: paddle.divide(Tensor([0],"float32"), Tensor([1],"float32"), )

[Pass] paddle.divide(Tensor([0],"float32"), Tensor([1],"float32"), )
2025-03-03 17:05:05.587347 test begin: paddle.divide(Tensor([0],"float64"), Tensor([0],"float64"), )

[Pass] paddle.divide(Tensor([0],"float64"), Tensor([0],"float64"), )
2025-03-03 17:05:05.589701 test begin: paddle.divide(Tensor([0],"float64"), Tensor([1],"float64"), )

[Pass] paddle.divide(Tensor([0],"float64"), Tensor([1],"float64"), )
2025-03-03 17:05:05.592132 test begin: paddle.divide(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 0, 1, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 0, 1, 1],"float32"), )
2025-03-03 17:05:05.594242 test begin: paddle.divide(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 1, 1, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 1, 1, 1],"float32"), )
2025-03-03 17:05:05.597187 test begin: paddle.divide(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), )

[Pass] paddle.divide(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), )
2025-03-03 17:05:05.600105 test begin: paddle.divide(Tensor([1, 0],"float32"), Tensor([1, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 0],"float32"), Tensor([1, 1],"float32"), )
2025-03-03 17:05:05.602441 test begin: paddle.divide(Tensor([1, 0],"float32"), Tensor([1, 3],"float32"), )

[paddle error] paddle.divide(Tensor([1, 0],"float32"), Tensor([1, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0] and the shape of Y = [1, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.607164 test begin: paddle.divide(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 1, 0, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 1, 0, 1],"float32"), )
2025-03-03 17:05:05.610410 test begin: paddle.divide(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 1, 1, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 1, 0, 1],"float32"), Tensor([1, 1, 1, 1],"float32"), )
2025-03-03 17:05:05.613645 test begin: paddle.divide(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 0],"float32"), )

[Pass] paddle.divide(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 0],"float32"), )
2025-03-03 17:05:05.615883 test begin: paddle.divide(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 1, 1, 0],"float32"), Tensor([1, 1, 1, 1],"float32"), )
2025-03-03 17:05:05.617876 test begin: paddle.divide(Tensor([1, 1, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 1, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), )
2025-03-03 17:05:05.619573 test begin: paddle.divide(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 0, 1, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 0, 1, 1],"float32"), )
2025-03-03 17:05:05.621304 test begin: paddle.divide(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 1, 0, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 1, 0, 1],"float32"), )
2025-03-03 17:05:05.622984 test begin: paddle.divide(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 1, 1, 0],"float32"), )

[Pass] paddle.divide(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 1, 1, 0],"float32"), )
2025-03-03 17:05:05.625008 test begin: paddle.divide(Tensor([1, 1],"float32"), Tensor([0, 1],"float32"), )

[Pass] paddle.divide(Tensor([1, 1],"float32"), Tensor([0, 1],"float32"), )
2025-03-03 17:05:05.630231 test begin: paddle.divide(Tensor([1, 1],"float32"), Tensor([1, 0],"float32"), )

[Pass] paddle.divide(Tensor([1, 1],"float32"), Tensor([1, 0],"float32"), )
2025-03-03 17:05:05.635404 test begin: paddle.divide(Tensor([1, 3],"float32"), Tensor([0, 3],"float32"), )

[Pass] paddle.divide(Tensor([1, 3],"float32"), Tensor([0, 3],"float32"), )
2025-03-03 17:05:05.637876 test begin: paddle.divide(Tensor([1, 3],"float32"), Tensor([1, 0],"float32"), )

[paddle error] paddle.divide(Tensor([1, 3],"float32"), Tensor([1, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 3] and the shape of Y = [1, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.639788 test begin: paddle.divide(Tensor([10, 0, 1],"float32"), Tensor([10, 0, 1],"float32"), )

[Pass] paddle.divide(Tensor([10, 0, 1],"float32"), Tensor([10, 0, 1],"float32"), )
2025-03-03 17:05:05.642058 test begin: paddle.divide(Tensor([10, 0, 1],"float32"), Tensor([10, 20, 1],"float32"), )

[paddle error] paddle.divide(Tensor([10, 0, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 0, 1] and the shape of Y = [10, 20, 1]. Received [0] in X is not equal to [20] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.644837 test begin: paddle.divide(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 0],"float32"), )

[Pass] paddle.divide(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 0],"float32"), )
2025-03-03 17:05:05.647771 test begin: paddle.divide(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 1],"float32"), )

[Pass] paddle.divide(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 1],"float32"), )
2025-03-03 17:05:05.651239 test begin: paddle.divide(Tensor([10, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )

[paddle error] paddle.divide(Tensor([10, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 20, 1] and the shape of Y = [0, 20, 1]. Received [10] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.653750 test begin: paddle.divide(Tensor([10, 20, 1],"float32"), Tensor([10, 0, 1],"float32"), )

[paddle error] paddle.divide(Tensor([10, 20, 1],"float32"), Tensor([10, 0, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 20, 1] and the shape of Y = [10, 0, 1]. Received [20] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.656070 test begin: paddle.divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 0],"float32"), )

[Pass] paddle.divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 0],"float32"), )
2025-03-03 17:05:05.658593 test begin: paddle.divide(Tensor([1],"float32"), Tensor([0],"float32"), )

[Pass] paddle.divide(Tensor([1],"float32"), Tensor([0],"float32"), )
2025-03-03 17:05:05.660725 test begin: paddle.divide(Tensor([1],"float64"), Tensor([0],"float64"), )

[Pass] paddle.divide(Tensor([1],"float64"), Tensor([0],"float64"), )
2025-03-03 17:05:05.663885 test begin: paddle.divide(Tensor([2, 0, 1, 5],"float32"), Tensor([2, 0, 1, 5],"float32"), )

[Pass] paddle.divide(Tensor([2, 0, 1, 5],"float32"), Tensor([2, 0, 1, 5],"float32"), )
2025-03-03 17:05:05.666833 test begin: paddle.divide(Tensor([2, 0, 1, 5],"float32"), Tensor([2, 3, 1, 5],"float32"), )

[paddle error] paddle.divide(Tensor([2, 0, 1, 5],"float32"), Tensor([2, 3, 1, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0, 1, 5] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.669728 test begin: paddle.divide(Tensor([2, 0, 3, 4],"float32"), Tensor([3, 1, 4],"float32"), )

[paddle error] paddle.divide(Tensor([2, 0, 3, 4],"float32"), Tensor([3, 1, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0, 3, 4] and the shape of Y = [3, 1, 4]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.671964 test begin: paddle.divide(Tensor([2, 0, 4, 5],"complex128"), Tensor([4, 5],"float64"), )

[Pass] paddle.divide(Tensor([2, 0, 4, 5],"complex128"), Tensor([4, 5],"float64"), )
2025-03-03 17:05:05.675228 test begin: paddle.divide(Tensor([2, 0, 4, 5],"complex64"), Tensor([4, 5],"float32"), )

[Pass] paddle.divide(Tensor([2, 0, 4, 5],"complex64"), Tensor([4, 5],"float32"), )
2025-03-03 17:05:05.678453 test begin: paddle.divide(Tensor([2, 0, 4, 5],"float32"), Tensor([5],"complex64"), )

[Pass] paddle.divide(Tensor([2, 0, 4, 5],"float32"), Tensor([5],"complex64"), )
2025-03-03 17:05:05.681846 test begin: paddle.divide(Tensor([2, 0, 4, 5],"float64"), Tensor([5],"complex128"), )

[Pass] paddle.divide(Tensor([2, 0, 4, 5],"float64"), Tensor([5],"complex128"), )
2025-03-03 17:05:05.683987 test begin: paddle.divide(Tensor([2, 0, 4],"float32"), Tensor([2, 0, 4],"float32"), )

[Pass] paddle.divide(Tensor([2, 0, 4],"float32"), Tensor([2, 0, 4],"float32"), )
2025-03-03 17:05:05.685713 test begin: paddle.divide(Tensor([2, 0, 4],"float32"), Tensor([2, 3, 4],"float32"), )

[paddle error] paddle.divide(Tensor([2, 0, 4],"float32"), Tensor([2, 3, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0, 4] and the shape of Y = [2, 3, 4]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.687807 test begin: paddle.divide(Tensor([2, 0],"complex128"), Tensor([2, 0],"float64"), name="Normal_probs", )

[Pass] paddle.divide(Tensor([2, 0],"complex128"), Tensor([2, 0],"float64"), name="Normal_probs", )
2025-03-03 17:05:05.689793 test begin: paddle.divide(Tensor([2, 0],"complex128"), Tensor([2, 3],"float64"), name="Normal_probs", )

[paddle error] paddle.divide(Tensor([2, 0],"complex128"), Tensor([2, 3],"float64"), name="Normal_probs", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.692101 test begin: paddle.divide(Tensor([2, 0],"complex64"), Tensor([2, 0],"float32"), name="Normal_probs", )

[Pass] paddle.divide(Tensor([2, 0],"complex64"), Tensor([2, 0],"float32"), name="Normal_probs", )
2025-03-03 17:05:05.693904 test begin: paddle.divide(Tensor([2, 0],"complex64"), Tensor([2, 3],"float32"), name="Normal_probs", )

[paddle error] paddle.divide(Tensor([2, 0],"complex64"), Tensor([2, 3],"float32"), name="Normal_probs", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0] and the shape of Y = [2, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.696182 test begin: paddle.divide(Tensor([2, 3, 0, 4],"float32"), Tensor([3, 1, 4],"float32"), )

[Pass] paddle.divide(Tensor([2, 3, 0, 4],"float32"), Tensor([3, 1, 4],"float32"), )
2025-03-03 17:05:05.699679 test begin: paddle.divide(Tensor([2, 3, 0, 5],"complex128"), Tensor([4, 5],"float64"), )

[paddle error] paddle.divide(Tensor([2, 3, 0, 5],"complex128"), Tensor([4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 0, 5] and the shape of Y = [4, 5]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.720940 test begin: paddle.divide(Tensor([2, 3, 0, 5],"complex64"), Tensor([4, 5],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 0, 5],"complex64"), Tensor([4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 0, 5] and the shape of Y = [4, 5]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.739980 test begin: paddle.divide(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), )

[Pass] paddle.divide(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), )
2025-03-03 17:05:05.742484 test begin: paddle.divide(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 1, 5],"float32"), )

[Pass] paddle.divide(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 1, 5],"float32"), )
2025-03-03 17:05:05.745457 test begin: paddle.divide(Tensor([2, 3, 0, 5],"float32"), Tensor([5],"complex64"), )

[Pass] paddle.divide(Tensor([2, 3, 0, 5],"float32"), Tensor([5],"complex64"), )
2025-03-03 17:05:05.747843 test begin: paddle.divide(Tensor([2, 3, 0, 5],"float64"), Tensor([5],"complex128"), )

[Pass] paddle.divide(Tensor([2, 3, 0, 5],"float64"), Tensor([5],"complex128"), )
2025-03-03 17:05:05.750599 test begin: paddle.divide(Tensor([2, 3, 0],"float32"), Tensor([2, 3, 0],"float32"), )

[Pass] paddle.divide(Tensor([2, 3, 0],"float32"), Tensor([2, 3, 0],"float32"), )
2025-03-03 17:05:05.752262 test begin: paddle.divide(Tensor([2, 3, 0],"float32"), Tensor([2, 3, 4],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 0],"float32"), Tensor([2, 3, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 0] and the shape of Y = [2, 3, 4]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.754291 test begin: paddle.divide(Tensor([2, 3, 1, 0],"float32"), Tensor([2, 3, 1, 0],"float32"), )

[Pass] paddle.divide(Tensor([2, 3, 1, 0],"float32"), Tensor([2, 3, 1, 0],"float32"), )
2025-03-03 17:05:05.756177 test begin: paddle.divide(Tensor([2, 3, 1, 0],"float32"), Tensor([2, 3, 1, 5],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 1, 0],"float32"), Tensor([2, 3, 1, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 1, 0] and the shape of Y = [2, 3, 1, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.759687 test begin: paddle.divide(Tensor([2, 3, 1, 5],"float32"), Tensor([0, 3, 1, 5],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 1, 5],"float32"), Tensor([0, 3, 1, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 1, 5] and the shape of Y = [0, 3, 1, 5]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.761752 test begin: paddle.divide(Tensor([2, 3, 1, 5],"float32"), Tensor([2, 0, 1, 5],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 1, 5],"float32"), Tensor([2, 0, 1, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 1, 5] and the shape of Y = [2, 0, 1, 5]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.763724 test begin: paddle.divide(Tensor([2, 3, 1, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), )

[Pass] paddle.divide(Tensor([2, 3, 1, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), )
2025-03-03 17:05:05.766070 test begin: paddle.divide(Tensor([2, 3, 1, 5],"float32"), Tensor([2, 3, 1, 0],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 1, 5],"float32"), Tensor([2, 3, 1, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 1, 5] and the shape of Y = [2, 3, 1, 0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.767768 test begin: paddle.divide(Tensor([2, 3, 3, 0],"float32"), Tensor([3, 1, 4],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 3, 0],"float32"), Tensor([3, 1, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 0] and the shape of Y = [3, 1, 4]. Received [0] in X is not equal to [4] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.769614 test begin: paddle.divide(Tensor([2, 3, 3, 4],"float32"), Tensor([0, 1, 4],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 3, 4],"float32"), Tensor([0, 1, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 4] and the shape of Y = [0, 1, 4]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.772330 test begin: paddle.divide(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 0, 4],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 0, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 4] and the shape of Y = [3, 0, 4]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.774608 test begin: paddle.divide(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 1, 0],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 3, 4],"float32"), Tensor([3, 1, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 3, 4] and the shape of Y = [3, 1, 0]. Received [4] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.777470 test begin: paddle.divide(Tensor([2, 3, 4, 0],"complex128"), Tensor([4, 5],"float64"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 0],"complex128"), Tensor([4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 0] and the shape of Y = [4, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.779770 test begin: paddle.divide(Tensor([2, 3, 4, 0],"complex64"), Tensor([4, 5],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 0],"complex64"), Tensor([4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 0] and the shape of Y = [4, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.782529 test begin: paddle.divide(Tensor([2, 3, 4, 0],"float32"), Tensor([5],"complex64"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 0],"float32"), Tensor([5],"complex64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 0] and the shape of Y = [5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.786380 test begin: paddle.divide(Tensor([2, 3, 4, 0],"float64"), Tensor([5],"complex128"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 0],"float64"), Tensor([5],"complex128"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 0] and the shape of Y = [5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.788404 test begin: paddle.divide(Tensor([2, 3, 4, 5],"complex128"), Tensor([0, 5],"float64"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 5],"complex128"), Tensor([0, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.790252 test begin: paddle.divide(Tensor([2, 3, 4, 5],"complex128"), Tensor([4, 0],"float64"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 5],"complex128"), Tensor([4, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [4, 0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.792046 test begin: paddle.divide(Tensor([2, 3, 4, 5],"complex64"), Tensor([0, 5],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 5],"complex64"), Tensor([0, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.793488 test begin: paddle.divide(Tensor([2, 3, 4, 5],"complex64"), Tensor([4, 0],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 5],"complex64"), Tensor([4, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [4, 0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.795575 test begin: paddle.divide(Tensor([2, 3, 4, 5],"float32"), Tensor([0],"complex64"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 5],"float32"), Tensor([0],"complex64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.797218 test begin: paddle.divide(Tensor([2, 3, 4, 5],"float64"), Tensor([0],"complex128"), )

[paddle error] paddle.divide(Tensor([2, 3, 4, 5],"float64"), Tensor([0],"complex128"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.799542 test begin: paddle.divide(Tensor([2, 3, 4],"float32"), Tensor([0, 3, 4],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 4],"float32"), Tensor([0, 3, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4] and the shape of Y = [0, 3, 4]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.801930 test begin: paddle.divide(Tensor([2, 3, 4],"float32"), Tensor([2, 0, 4],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 4],"float32"), Tensor([2, 0, 4],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4] and the shape of Y = [2, 0, 4]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.803695 test begin: paddle.divide(Tensor([2, 3, 4],"float32"), Tensor([2, 3, 0],"float32"), )

[paddle error] paddle.divide(Tensor([2, 3, 4],"float32"), Tensor([2, 3, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4] and the shape of Y = [2, 3, 0]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.805752 test begin: paddle.divide(Tensor([2, 3],"complex128"), Tensor([0, 3],"float64"), name="Normal_probs", )

[paddle error] paddle.divide(Tensor([2, 3],"complex128"), Tensor([0, 3],"float64"), name="Normal_probs", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [0, 3]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.807913 test begin: paddle.divide(Tensor([2, 3],"complex128"), Tensor([2, 0],"float64"), name="Normal_probs", )

[paddle error] paddle.divide(Tensor([2, 3],"complex128"), Tensor([2, 0],"float64"), name="Normal_probs", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [2, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.810029 test begin: paddle.divide(Tensor([2, 3],"complex64"), Tensor([0, 3],"float32"), name="Normal_probs", )

[paddle error] paddle.divide(Tensor([2, 3],"complex64"), Tensor([0, 3],"float32"), name="Normal_probs", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [0, 3]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.812201 test begin: paddle.divide(Tensor([2, 3],"complex64"), Tensor([2, 0],"float32"), name="Normal_probs", )

[paddle error] paddle.divide(Tensor([2, 3],"complex64"), Tensor([2, 0],"float32"), name="Normal_probs", ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3] and the shape of Y = [2, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.814335 test begin: paddle.divide(Tensor([3, 0, 3, 40, 40],"float32"), Tensor([3, 0, 3, 40, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 0, 3, 40, 40],"float32"), Tensor([3, 0, 3, 40, 40],"float32"), name=None, )
2025-03-03 17:05:05.816450 test begin: paddle.divide(Tensor([3, 0, 3, 40, 40],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 0, 3, 40, 40],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 3, 40, 40] and the shape of Y = [3, 3, 3, 40, 40]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.819211 test begin: paddle.divide(Tensor([3, 0, 40, 3],"float32"), Tensor([3, 0, 40, 3],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 0, 40, 3],"float32"), Tensor([3, 0, 40, 3],"float32"), name=None, )
2025-03-03 17:05:05.821101 test begin: paddle.divide(Tensor([3, 0, 40, 3],"float32"), Tensor([3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 0, 40, 3],"float32"), Tensor([3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 40, 3] and the shape of Y = [3, 40, 40, 3]. Received [0] in X is not equal to [40] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.823694 test begin: paddle.divide(Tensor([3, 0, 40, 40, 3],"float32"), Tensor([3, 0, 40, 40, 3],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 0, 40, 40, 3],"float32"), Tensor([3, 0, 40, 40, 3],"float32"), name=None, )
2025-03-03 17:05:05.826403 test begin: paddle.divide(Tensor([3, 0, 40, 40, 3],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 0, 40, 40, 3],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 40, 40, 3] and the shape of Y = [3, 3, 40, 40, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.828948 test begin: paddle.divide(Tensor([3, 0, 40, 40],"float32"), Tensor([3, 0, 40, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 0, 40, 40],"float32"), Tensor([3, 0, 40, 40],"float32"), name=None, )
2025-03-03 17:05:05.832225 test begin: paddle.divide(Tensor([3, 0, 40, 40],"float32"), Tensor([3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 0, 40, 40],"float32"), Tensor([3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 40, 40] and the shape of Y = [3, 3, 40, 40]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.834414 test begin: paddle.divide(Tensor([3, 0, 40],"float32"), Tensor([3, 0, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 0, 40],"float32"), Tensor([3, 0, 40],"float32"), name=None, )
2025-03-03 17:05:05.836244 test begin: paddle.divide(Tensor([3, 0, 40],"float32"), Tensor([3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 0, 40],"float32"), Tensor([3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0, 40] and the shape of Y = [3, 40, 40]. Received [0] in X is not equal to [40] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.838433 test begin: paddle.divide(Tensor([3, 3, 0, 40, 3],"float32"), Tensor([3, 3, 0, 40, 3],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 3, 0, 40, 3],"float32"), Tensor([3, 3, 0, 40, 3],"float32"), name=None, )
2025-03-03 17:05:05.840217 test begin: paddle.divide(Tensor([3, 3, 0, 40, 3],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 0, 40, 3],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 0, 40, 3] and the shape of Y = [3, 3, 40, 40, 3]. Received [0] in X is not equal to [40] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.842906 test begin: paddle.divide(Tensor([3, 3, 0, 40, 40],"float32"), Tensor([3, 3, 0, 40, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 3, 0, 40, 40],"float32"), Tensor([3, 3, 0, 40, 40],"float32"), name=None, )
2025-03-03 17:05:05.844733 test begin: paddle.divide(Tensor([3, 3, 0, 40, 40],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 0, 40, 40],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 0, 40, 40] and the shape of Y = [3, 3, 3, 40, 40]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.847514 test begin: paddle.divide(Tensor([3, 3, 0, 40],"float32"), Tensor([3, 3, 0, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 3, 0, 40],"float32"), Tensor([3, 3, 0, 40],"float32"), name=None, )
2025-03-03 17:05:05.849262 test begin: paddle.divide(Tensor([3, 3, 0, 40],"float32"), Tensor([3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 0, 40],"float32"), Tensor([3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 0, 40] and the shape of Y = [3, 3, 40, 40]. Received [0] in X is not equal to [40] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.851123 test begin: paddle.divide(Tensor([3, 3, 3, 0, 40],"float32"), Tensor([3, 3, 3, 0, 40],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 3, 3, 0, 40],"float32"), Tensor([3, 3, 3, 0, 40],"float32"), name=None, )
2025-03-03 17:05:05.854131 test begin: paddle.divide(Tensor([3, 3, 3, 0, 40],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 3, 0, 40],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3, 0, 40] and the shape of Y = [3, 3, 3, 40, 40]. Received [0] in X is not equal to [40] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.857783 test begin: paddle.divide(Tensor([3, 3, 3, 40, 0],"float32"), Tensor([3, 3, 3, 40, 0],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 3, 3, 40, 0],"float32"), Tensor([3, 3, 3, 40, 0],"float32"), name=None, )
2025-03-03 17:05:05.859783 test begin: paddle.divide(Tensor([3, 3, 3, 40, 0],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 3, 40, 0],"float32"), Tensor([3, 3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3, 40, 0] and the shape of Y = [3, 3, 3, 40, 40]. Received [0] in X is not equal to [40] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.862502 test begin: paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([0, 3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([0, 3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3, 40, 40] and the shape of Y = [0, 3, 3, 40, 40]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.864650 test begin: paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([3, 0, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([3, 0, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3, 40, 40] and the shape of Y = [3, 0, 3, 40, 40]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.866921 test begin: paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([3, 3, 0, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([3, 3, 0, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3, 40, 40] and the shape of Y = [3, 3, 0, 40, 40]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.869422 test begin: paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([3, 3, 3, 0, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([3, 3, 3, 0, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3, 40, 40] and the shape of Y = [3, 3, 3, 0, 40]. Received [40] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.872066 test begin: paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([3, 3, 3, 40, 0],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 3, 40, 40],"float32"), Tensor([3, 3, 3, 40, 0],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3, 40, 40] and the shape of Y = [3, 3, 3, 40, 0]. Received [40] in X is not equal to [0] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.874162 test begin: paddle.divide(Tensor([3, 3, 40, 0, 3],"float32"), Tensor([3, 3, 40, 0, 3],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 3, 40, 0, 3],"float32"), Tensor([3, 3, 40, 0, 3],"float32"), name=None, )
2025-03-03 17:05:05.875970 test begin: paddle.divide(Tensor([3, 3, 40, 0, 3],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 0, 3],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 0, 3] and the shape of Y = [3, 3, 40, 40, 3]. Received [0] in X is not equal to [40] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.878547 test begin: paddle.divide(Tensor([3, 3, 40, 0],"float32"), Tensor([3, 3, 40, 0],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 3, 40, 0],"float32"), Tensor([3, 3, 40, 0],"float32"), name=None, )
2025-03-03 17:05:05.880864 test begin: paddle.divide(Tensor([3, 3, 40, 0],"float32"), Tensor([3, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 0],"float32"), Tensor([3, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 0] and the shape of Y = [3, 3, 40, 40]. Received [0] in X is not equal to [40] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.883258 test begin: paddle.divide(Tensor([3, 3, 40, 40, 0],"float32"), Tensor([3, 3, 40, 40, 0],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 3, 40, 40, 0],"float32"), Tensor([3, 3, 40, 40, 0],"float32"), name=None, )
2025-03-03 17:05:05.885454 test begin: paddle.divide(Tensor([3, 3, 40, 40, 0],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40, 0],"float32"), Tensor([3, 3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40, 0] and the shape of Y = [3, 3, 40, 40, 3]. Received [0] in X is not equal to [3] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.888117 test begin: paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([0, 3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([0, 3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40, 3] and the shape of Y = [0, 3, 40, 40, 3]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.891282 test begin: paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([3, 0, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([3, 0, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40, 3] and the shape of Y = [3, 0, 40, 40, 3]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.893634 test begin: paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([3, 3, 0, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([3, 3, 0, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40, 3] and the shape of Y = [3, 3, 0, 40, 3]. Received [40] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.897244 test begin: paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([3, 3, 40, 0, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([3, 3, 40, 0, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40, 3] and the shape of Y = [3, 3, 40, 0, 3]. Received [40] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.899693 test begin: paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([3, 3, 40, 40, 0],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40, 3],"float32"), Tensor([3, 3, 40, 40, 0],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40, 3] and the shape of Y = [3, 3, 40, 40, 0]. Received [3] in X is not equal to [0] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.902355 test begin: paddle.divide(Tensor([3, 3, 40, 40],"float32"), Tensor([0, 3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40],"float32"), Tensor([0, 3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40] and the shape of Y = [0, 3, 40, 40]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.904963 test begin: paddle.divide(Tensor([3, 3, 40, 40],"float32"), Tensor([3, 0, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40],"float32"), Tensor([3, 0, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40] and the shape of Y = [3, 0, 40, 40]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.907276 test begin: paddle.divide(Tensor([3, 3, 40, 40],"float32"), Tensor([3, 3, 0, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40],"float32"), Tensor([3, 3, 0, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40] and the shape of Y = [3, 3, 0, 40]. Received [40] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.909302 test begin: paddle.divide(Tensor([3, 3, 40, 40],"float32"), Tensor([3, 3, 40, 0],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 3, 40, 40],"float32"), Tensor([3, 3, 40, 0],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 40, 40] and the shape of Y = [3, 3, 40, 0]. Received [40] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.912016 test begin: paddle.divide(Tensor([3, 40, 0, 3],"float32"), Tensor([3, 40, 0, 3],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 40, 0, 3],"float32"), Tensor([3, 40, 0, 3],"float32"), name=None, )
2025-03-03 17:05:05.914974 test begin: paddle.divide(Tensor([3, 40, 0, 3],"float32"), Tensor([3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 0, 3],"float32"), Tensor([3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 0, 3] and the shape of Y = [3, 40, 40, 3]. Received [0] in X is not equal to [40] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.917941 test begin: paddle.divide(Tensor([3, 40, 0],"float32"), Tensor([3, 40, 0],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 40, 0],"float32"), Tensor([3, 40, 0],"float32"), name=None, )
2025-03-03 17:05:05.920563 test begin: paddle.divide(Tensor([3, 40, 0],"float32"), Tensor([3, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 0],"float32"), Tensor([3, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 0] and the shape of Y = [3, 40, 40]. Received [0] in X is not equal to [40] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.923212 test begin: paddle.divide(Tensor([3, 40, 40, 0],"float32"), Tensor([3, 40, 40, 0],"float32"), name=None, )

[Pass] paddle.divide(Tensor([3, 40, 40, 0],"float32"), Tensor([3, 40, 40, 0],"float32"), name=None, )
2025-03-03 17:05:05.925958 test begin: paddle.divide(Tensor([3, 40, 40, 0],"float32"), Tensor([3, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 40, 0],"float32"), Tensor([3, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 40, 0] and the shape of Y = [3, 40, 40, 3]. Received [0] in X is not equal to [3] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.929385 test begin: paddle.divide(Tensor([3, 40, 40, 3],"float32"), Tensor([0, 40, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 40, 3],"float32"), Tensor([0, 40, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 40, 3] and the shape of Y = [0, 40, 40, 3]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.932478 test begin: paddle.divide(Tensor([3, 40, 40, 3],"float32"), Tensor([3, 0, 40, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 40, 3],"float32"), Tensor([3, 0, 40, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 40, 3] and the shape of Y = [3, 0, 40, 3]. Received [40] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.935335 test begin: paddle.divide(Tensor([3, 40, 40, 3],"float32"), Tensor([3, 40, 0, 3],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 40, 3],"float32"), Tensor([3, 40, 0, 3],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 40, 3] and the shape of Y = [3, 40, 0, 3]. Received [40] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.938188 test begin: paddle.divide(Tensor([3, 40, 40, 3],"float32"), Tensor([3, 40, 40, 0],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 40, 3],"float32"), Tensor([3, 40, 40, 0],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 40, 3] and the shape of Y = [3, 40, 40, 0]. Received [3] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.941101 test begin: paddle.divide(Tensor([3, 40, 40],"float32"), Tensor([0, 40, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 40],"float32"), Tensor([0, 40, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 40] and the shape of Y = [0, 40, 40]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.943070 test begin: paddle.divide(Tensor([3, 40, 40],"float32"), Tensor([3, 0, 40],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 40],"float32"), Tensor([3, 0, 40],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 40] and the shape of Y = [3, 0, 40]. Received [40] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.944686 test begin: paddle.divide(Tensor([3, 40, 40],"float32"), Tensor([3, 40, 0],"float32"), name=None, )

[paddle error] paddle.divide(Tensor([3, 40, 40],"float32"), Tensor([3, 40, 0],"float32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 40, 40] and the shape of Y = [3, 40, 0]. Received [40] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.946185 test begin: paddle.divide(x=Tensor([0, 1],"float64"), y=Tensor([0, 1],"float64"), name=None, )

[Pass] paddle.divide(x=Tensor([0, 1],"float64"), y=Tensor([0, 1],"float64"), name=None, )
2025-03-03 17:05:05.948236 test begin: paddle.divide(x=Tensor([0, 1],"float64"), y=Tensor([3, 1],"float64"), name=None, )

[paddle error] paddle.divide(x=Tensor([0, 1],"float64"), y=Tensor([3, 1],"float64"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1] and the shape of Y = [3, 1]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.949666 test begin: paddle.divide(x=Tensor([0, 3, 3],"float64"), y=Tensor([3],"float64"), )

[Pass] paddle.divide(x=Tensor([0, 3, 3],"float64"), y=Tensor([3],"float64"), )
2025-03-03 17:05:05.952024 test begin: paddle.divide(x=Tensor([0],"float32"), y=Tensor([0],"float32"), )

[Pass] paddle.divide(x=Tensor([0],"float32"), y=Tensor([0],"float32"), )
2025-03-03 17:05:05.953666 test begin: paddle.divide(x=Tensor([0],"float32"), y=Tensor([3],"float32"), )

[paddle error] paddle.divide(x=Tensor([0],"float32"), y=Tensor([3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.956142 test begin: paddle.divide(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )

[Pass] paddle.divide(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )
2025-03-03 17:05:05.958570 test begin: paddle.divide(x=Tensor([0],"float64"), y=Tensor([2],"float64"), )

[paddle error] paddle.divide(x=Tensor([0],"float64"), y=Tensor([2],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [2]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.961124 test begin: paddle.divide(x=Tensor([0],"float64"), y=Tensor([3, 1],"float64"), )

[Pass] paddle.divide(x=Tensor([0],"float64"), y=Tensor([3, 1],"float64"), )
2025-03-03 17:05:05.964809 test begin: paddle.divide(x=Tensor([2],"float64"), y=Tensor([0],"float64"), )

[paddle error] paddle.divide(x=Tensor([2],"float64"), y=Tensor([0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2] and the shape of Y = [0]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.967435 test begin: paddle.divide(x=Tensor([3, 0, 3],"float64"), y=Tensor([3],"float64"), )

[Pass] paddle.divide(x=Tensor([3, 0, 3],"float64"), y=Tensor([3],"float64"), )
2025-03-03 17:05:05.970287 test begin: paddle.divide(x=Tensor([3, 0],"float64"), y=Tensor([3, 0],"float64"), name=None, )

[Pass] paddle.divide(x=Tensor([3, 0],"float64"), y=Tensor([3, 0],"float64"), name=None, )
2025-03-03 17:05:05.972538 test begin: paddle.divide(x=Tensor([3, 0],"float64"), y=Tensor([3, 1],"float64"), name=None, )

[Pass] paddle.divide(x=Tensor([3, 0],"float64"), y=Tensor([3, 1],"float64"), name=None, )
2025-03-03 17:05:05.975579 test begin: paddle.divide(x=Tensor([3, 1],"float64"), y=Tensor([0, 1],"float64"), name=None, )

[paddle error] paddle.divide(x=Tensor([3, 1],"float64"), y=Tensor([0, 1],"float64"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 1] and the shape of Y = [0, 1]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.977569 test begin: paddle.divide(x=Tensor([3, 1],"float64"), y=Tensor([3, 0],"float64"), name=None, )

[Pass] paddle.divide(x=Tensor([3, 1],"float64"), y=Tensor([3, 0],"float64"), name=None, )
2025-03-03 17:05:05.979895 test begin: paddle.divide(x=Tensor([3, 3, 0],"float64"), y=Tensor([3],"float64"), )

[paddle error] paddle.divide(x=Tensor([3, 3, 0],"float64"), y=Tensor([3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.981335 test begin: paddle.divide(x=Tensor([3, 3, 3],"float64"), y=Tensor([0],"float64"), )

[paddle error] paddle.divide(x=Tensor([3, 3, 3],"float64"), y=Tensor([0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3] and the shape of Y = [0]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.982842 test begin: paddle.divide(x=Tensor([3],"float32"), y=Tensor([0],"float32"), )

[paddle error] paddle.divide(x=Tensor([3],"float32"), y=Tensor([0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3] and the shape of Y = [0]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.984296 test begin: paddle.divide(x=Tensor([3],"float64"), y=Tensor([0, 1],"float64"), )

[Pass] paddle.divide(x=Tensor([3],"float64"), y=Tensor([0, 1],"float64"), )
2025-03-03 17:05:05.986039 test begin: paddle.divide(x=Tensor([3],"float64"), y=Tensor([3, 0],"float64"), )

[paddle error] paddle.divide(x=Tensor([3],"float64"), y=Tensor([3, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3] and the shape of Y = [3, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 17:05:05.987528 test begin: paddle.dot(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), )

[Pass] paddle.dot(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), )
2025-03-03 17:05:05.989115 test begin: paddle.dot(Tensor([0, 2],"float32"), Tensor([2, 2],"float32"), )

[paddle error] paddle.dot(Tensor([0, 2],"float32"), Tensor([2, 2],"float32"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [0, 2] should be exactly the same with input tensor Y: [2, 2]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:05.991014 test begin: paddle.dot(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), )

[Pass] paddle.dot(Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), )
2025-03-03 17:05:05.993375 test begin: paddle.dot(Tensor([2, 0],"float32"), Tensor([2, 2],"float32"), )

[paddle error] paddle.dot(Tensor([2, 0],"float32"), Tensor([2, 2],"float32"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [2, 0] should be exactly the same with input tensor Y: [2, 2]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:05.994850 test begin: paddle.dot(Tensor([2, 2],"float32"), Tensor([0, 2],"float32"), )

[paddle error] paddle.dot(Tensor([2, 2],"float32"), Tensor([0, 2],"float32"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [2, 2] should be exactly the same with input tensor Y: [0, 2]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:05.996845 test begin: paddle.dot(Tensor([2, 2],"float32"), Tensor([2, 0],"float32"), )

[paddle error] paddle.dot(Tensor([2, 2],"float32"), Tensor([2, 0],"float32"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [2, 2] should be exactly the same with input tensor Y: [2, 0]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:05.998813 test begin: paddle.dot(x=Tensor([0, 3],"float64"), y=Tensor([0, 3],"float64"), )

[Pass] paddle.dot(x=Tensor([0, 3],"float64"), y=Tensor([0, 3],"float64"), )
2025-03-03 17:05:06.000378 test begin: paddle.dot(x=Tensor([0, 3],"float64"), y=Tensor([2, 3],"float64"), )

[paddle error] paddle.dot(x=Tensor([0, 3],"float64"), y=Tensor([2, 3],"float64"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [0, 3] should be exactly the same with input tensor Y: [2, 3]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:06.003213 test begin: paddle.dot(x=Tensor([0],"int32"), y=Tensor([0],"int32"), )

[Pass] paddle.dot(x=Tensor([0],"int32"), y=Tensor([0],"int32"), )
2025-03-03 17:05:06.005649 test begin: paddle.dot(x=Tensor([0],"int32"), y=Tensor([6],"int32"), )

[paddle error] paddle.dot(x=Tensor([0],"int32"), y=Tensor([6],"int32"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [0] should be exactly the same with input tensor Y: [6]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:06.007435 test begin: paddle.dot(x=Tensor([0],"int64"), y=Tensor([0],"int64"), )

[Pass] paddle.dot(x=Tensor([0],"int64"), y=Tensor([0],"int64"), )
2025-03-03 17:05:06.009608 test begin: paddle.dot(x=Tensor([0],"int64"), y=Tensor([6],"int64"), )

[paddle error] paddle.dot(x=Tensor([0],"int64"), y=Tensor([6],"int64"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [0] should be exactly the same with input tensor Y: [6]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:06.011325 test begin: paddle.dot(x=Tensor([2, 0],"float64"), y=Tensor([2, 0],"float64"), )

[Pass] paddle.dot(x=Tensor([2, 0],"float64"), y=Tensor([2, 0],"float64"), )
2025-03-03 17:05:06.013294 test begin: paddle.dot(x=Tensor([2, 0],"float64"), y=Tensor([2, 3],"float64"), )

[paddle error] paddle.dot(x=Tensor([2, 0],"float64"), y=Tensor([2, 3],"float64"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [2, 0] should be exactly the same with input tensor Y: [2, 3]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:06.014815 test begin: paddle.dot(x=Tensor([2, 3],"float64"), y=Tensor([0, 3],"float64"), )

[paddle error] paddle.dot(x=Tensor([2, 3],"float64"), y=Tensor([0, 3],"float64"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [2, 3] should be exactly the same with input tensor Y: [0, 3]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:06.016270 test begin: paddle.dot(x=Tensor([2, 3],"float64"), y=Tensor([2, 0],"float64"), )

[paddle error] paddle.dot(x=Tensor([2, 3],"float64"), y=Tensor([2, 0],"float64"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [2, 3] should be exactly the same with input tensor Y: [2, 0]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:06.017714 test begin: paddle.dot(x=Tensor([6],"int32"), y=Tensor([0],"int32"), )

[paddle error] paddle.dot(x=Tensor([6],"int32"), y=Tensor([0],"int32"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [6] should be exactly the same with input tensor Y: [0]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:06.019361 test begin: paddle.dot(x=Tensor([6],"int64"), y=Tensor([0],"int64"), )

[paddle error] paddle.dot(x=Tensor([6],"int64"), y=Tensor([0],"int64"), ) 
 (PreconditionNotMet) ShapeError: The shape of input tensor X: [6] should be exactly the same with input tensor Y: [0]
  [Hint: Expected true == shape_match, but received true:1 != shape_match:0.] (at ../paddle/phi/infermeta/binary.cc:1641)

2025-03-03 17:05:06.020850 test begin: paddle.dsplit(Tensor([0, 2, 6],"bool"), 3, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<bool, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<bool, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992706 (unix time) try "date -d @1740992706" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 151252 (TID 0x7f2b4c949700) from PID 0 ***]

2025-03-03 17:05:10.663911 test begin: paddle.dsplit(Tensor([0, 2, 6],"float16"), 3, )

W0303 17:05:13.720541 162058 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:05:13.722121 162058 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<phi::dtype::float16, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992713 (unix time) try "date -d @1740992713" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 158994 (TID 0x7faf1ff48700) from PID 0 ***]

2025-03-03 17:05:18.210732 test begin: paddle.dsplit(Tensor([0, 3, 6],"int64"), 2, )

W0303 17:05:22.108022 163191 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:05:22.109053 163191 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992722 (unix time) try "date -d @1740992722" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 162596 (TID 0x7efccfdc2700) from PID 0 ***]

2025-03-03 17:05:33.619884 test begin: paddle.dsplit(Tensor([4, 0, 6],"bool"), 3, )

W0303 17:05:37.012240  2248 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:05:37.013568  2248 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<bool, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<bool, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992737 (unix time) try "date -d @1740992737" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 1593 (TID 0x7f0329744700) from PID 0 ***]

2025-03-03 17:05:41.415357 test begin: paddle.dsplit(Tensor([4, 0, 6],"float16"), 3, )

W0303 17:05:44.466681  3172 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:05:44.467753  3172 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<phi::dtype::float16, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992744 (unix time) try "date -d @1740992744" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 2526 (TID 0x7f9c9787e700) from PID 0 ***]

2025-03-03 17:05:58.082319 test begin: paddle.dsplit(Tensor([4, 0, 6],"int64"), 2, )

W0303 17:06:01.310496  5771 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:06:01.311501  5771 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992761 (unix time) try "date -d @1740992761" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 5035 (TID 0x7f24cddc2700) from PID 0 ***]

2025-03-03 17:06:05.903906 test begin: paddle.dsplit(Tensor([4, 2, 0],"bool"), 3, )

W0303 17:06:09.324239  6734 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:06:09.326721  6734 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<bool, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<bool, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992769 (unix time) try "date -d @1740992769" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 6095 (TID 0x7f146f935700) from PID 0 ***]

2025-03-03 17:06:13.609738 test begin: paddle.dsplit(Tensor([4, 2, 0],"float16"), 3, )

W0303 17:06:17.442909  7789 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:06:17.443802  7789 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<phi::dtype::float16, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992777 (unix time) try "date -d @1740992777" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 7080 (TID 0x7fd31587e700) from PID 0 ***]

2025-03-03 17:06:21.967965 test begin: paddle.dsplit(Tensor([4, 3, 0],"int64"), 2, )

W0303 17:06:25.552822  8854 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:06:25.554279  8854 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992785 (unix time) try "date -d @1740992785" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 8201 (TID 0x7f41767c3700) from PID 0 ***]

2025-03-03 17:06:30.137705 test begin: paddle.einsum("..., ...", Tensor([0, 10],"float64"), Tensor([2, 3, 10],"float64"), )

W0303 17:06:33.415864 10370 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:06:33.416893 10370 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992793 (unix time) try "date -d @1740992793" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0d57a64c32) received by PID 9564 (TID 0x7f0c63abb700) from PID 1470516274 ***]

2025-03-03 17:06:38.368558 test begin: paddle.einsum("..., ...", Tensor([0, 11],"float64"), Tensor([2, 3, 11],"float64"), )

W0303 17:06:41.431483 11283 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:06:41.432619 11283 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992801 (unix time) try "date -d @1740992801" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1b67551c32) received by PID 10754 (TID 0x7f1a8d4f4700) from PID 1733631026 ***]

2025-03-03 17:06:45.943352 test begin: paddle.einsum("..., ...", Tensor([3, 0],"float64"), Tensor([2, 3, 10],"float64"), )

W0303 17:06:49.020089 12622 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:06:49.021935 12622 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992809 (unix time) try "date -d @1740992809" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efb9026fc32) received by PID 11793 (TID 0x7efaaff48700) from PID 18446744071833058354 ***]

2025-03-03 17:06:53.454117 test begin: paddle.einsum("..., ...", Tensor([3, 0],"float64"), Tensor([2, 3, 11],"float64"), )

W0303 17:06:56.602964 13536 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:06:56.603940 13536 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992816 (unix time) try "date -d @1740992816" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f866a6dbc32) received by PID 12878 (TID 0x7f85747c3700) from PID 1785576498 ***]

2025-03-03 17:07:01.095545 test begin: paddle.einsum("..., ...", Tensor([3, 10],"float64"), Tensor([0, 3, 10],"float64"), )

W0303 17:07:05.292797 15137 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:07:05.293910 15137 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("..., ...", Tensor([3, 10],"float64"), Tensor([0, 3, 10],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:07:05.296297 test begin: paddle.einsum("..., ...", Tensor([3, 10],"float64"), Tensor([2, 0, 10],"float64"), )

[paddle error] paddle.einsum("..., ...", Tensor([3, 10],"float64"), Tensor([2, 0, 10],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `e`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:07:05.299907 test begin: paddle.einsum("..., ...", Tensor([3, 10],"float64"), Tensor([2, 3, 0],"float64"), )

[paddle error] paddle.einsum("..., ...", Tensor([3, 10],"float64"), Tensor([2, 3, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:07:05.302599 test begin: paddle.einsum("..., ...", Tensor([3, 11],"float64"), Tensor([0, 3, 11],"float64"), )

[paddle error] paddle.einsum("..., ...", Tensor([3, 11],"float64"), Tensor([0, 3, 11],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:07:05.304950 test begin: paddle.einsum("..., ...", Tensor([3, 11],"float64"), Tensor([2, 0, 11],"float64"), )

[paddle error] paddle.einsum("..., ...", Tensor([3, 11],"float64"), Tensor([2, 0, 11],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `e`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:07:05.307194 test begin: paddle.einsum("..., ...", Tensor([3, 11],"float64"), Tensor([2, 3, 0],"float64"), )

[paddle error] paddle.einsum("..., ...", Tensor([3, 11],"float64"), Tensor([2, 3, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:07:05.309234 test begin: paddle.einsum("..., f -> ... f", Tensor([0],"float32"), Tensor([0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992825 (unix time) try "date -d @1740992825" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc136dc3fc2) received by PID 14484 (TID 0x7fc0467c3700) from PID 920403906 ***]

2025-03-03 17:07:09.664301 test begin: paddle.einsum("..., f -> ... f", Tensor([0],"float32"), Tensor([16],"float32"), )

W0303 17:07:14.023231 16419 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:07:14.025957 16419 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992834 (unix time) try "date -d @1740992834" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efd94e50fc2) received by PID 15786 (TID 0x7efca0949700) from PID 18446744071912624066 ***]

2025-03-03 17:07:25.096873 test begin: paddle.einsum("..., f -> ... f", Tensor([16],"float32"), Tensor([0],"float32"), )

W0303 17:07:28.286608 18411 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:07:28.287693 18411 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992848 (unix time) try "date -d @1740992848" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9370e08fc2) received by PID 17738 (TID 0x7f9264949700) from PID 1893765058 ***]

2025-03-03 17:07:33.541227 test begin: paddle.einsum("..., f -> ... f", Tensor([24],"float32"), Tensor([0],"float32"), )

W0303 17:07:37.443208 20133 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:07:37.444350 20133 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992857 (unix time) try "date -d @1740992857" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fbbc95adfc2) received by PID 19246 (TID 0x7fbaef34a700) from PID 18446744072792760258 ***]

2025-03-03 17:07:50.921081 test begin: paddle.einsum("...,...", Tensor([0],"float64"), Tensor([0],"float64"), )

W0303 17:07:54.383152 22129 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:07:54.385268 22129 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992874 (unix time) try "date -d @1740992874" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efdf1946c32) received by PID 21377 (TID 0x7efcef935700) from PID 18446744073467620402 ***]

2025-03-03 17:07:59.788207 test begin: paddle.einsum("...,...", Tensor([0],"float64"), Tensor([10],"float64"), )

W0303 17:08:02.812717 23826 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:08:02.813707 23826 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992882 (unix time) try "date -d @1740992882" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2ae32e5c32) received by PID 22780 (TID 0x7f2a1d2b7700) from PID 18446744073226050610 ***]

2025-03-03 17:08:07.515113 test begin: paddle.einsum("...,...", Tensor([0],"float64"), Tensor([11],"float64"), )

W0303 17:08:14.201256 24572 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:08:14.202404 24572 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992894 (unix time) try "date -d @1740992894" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fec80d65c32) received by PID 23935 (TID 0x7feb58949700) from PID 18446744071576116274 ***]

2025-03-03 17:08:18.911471 test begin: paddle.einsum("...,...", Tensor([10],"float64"), Tensor([0],"float64"), )

W0303 17:08:24.223445 25871 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:08:24.225059 25871 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...,...", Tensor([10],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `c`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:08:24.227045 test begin: paddle.einsum("...,...", Tensor([11],"float64"), Tensor([0],"float64"), )

[paddle error] paddle.einsum("...,...", Tensor([11],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `c`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:08:24.230820 test begin: paddle.einsum("...,...->...", Tensor([0, 5, 5],"float64"), Tensor([0, 5, 5],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992904 (unix time) try "date -d @1740992904" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc6dc464c32) received by PID 25136 (TID 0x7fc5e3f48700) from PID 18446744073110178866 ***]

2025-03-03 17:08:36.894327 test begin: paddle.einsum("...,...->...", Tensor([0, 5, 5],"float64"), Tensor([5, 5, 5],"float64"), )

W0303 17:08:40.415491 28500 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:08:40.416739 28500 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992920 (unix time) try "date -d @1740992920" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f03581edc32) received by PID 27780 (TID 0x7f0269f48700) from PID 1478417458 ***]

2025-03-03 17:08:44.673530 test begin: paddle.einsum("...,...->...", Tensor([5, 0, 5],"float64"), Tensor([5, 0, 5],"float64"), )

W0303 17:08:48.339738 29631 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:08:48.340855 29631 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992928 (unix time) try "date -d @1740992928" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f49e921fc32) received by PID 28913 (TID 0x7f48ee949700) from PID 18446744073325902898 ***]

2025-03-03 17:09:00.295903 test begin: paddle.einsum("...,...->...", Tensor([5, 0, 5],"float64"), Tensor([5, 5, 5],"float64"), )

W0303 17:09:03.750514 32076 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:09:03.751494 32076 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992943 (unix time) try "date -d @1740992943" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f836dc0dc32) received by PID 31104 (TID 0x7f825db85700) from PID 1841355826 ***]

2025-03-03 17:09:08.074303 test begin: paddle.einsum("...,...->...", Tensor([5, 5, 0],"float64"), Tensor([5, 5, 0],"float64"), )

W0303 17:09:12.260601 32915 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:09:12.261878 32915 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992952 (unix time) try "date -d @1740992952" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fbfc4223c32) received by PID 32363 (TID 0x7fbeddf48700) from PID 18446744072705162290 ***]

2025-03-03 17:09:23.793206 test begin: paddle.einsum("...,...->...", Tensor([5, 5, 0],"float64"), Tensor([5, 5, 5],"float64"), )

W0303 17:09:28.519951 35014 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:09:28.521133 35014 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992968 (unix time) try "date -d @1740992968" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7640e42c32) received by PID 34279 (TID 0x7f755a949700) from PID 1088695346 ***]

2025-03-03 17:09:32.940245 test begin: paddle.einsum("...,...->...", Tensor([5, 5, 5],"float64"), Tensor([0, 5, 5],"float64"), )

W0303 17:09:36.620229 36404 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:09:36.621297 36404 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...,...->...", Tensor([5, 5, 5],"float64"), Tensor([0, 5, 5],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `g`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:09:36.622623 test begin: paddle.einsum("...,...->...", Tensor([5, 5, 5],"float64"), Tensor([5, 0, 5],"float64"), )

[paddle error] paddle.einsum("...,...->...", Tensor([5, 5, 5],"float64"), Tensor([5, 0, 5],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `e`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:09:36.655880 test begin: paddle.einsum("...,...->...", Tensor([5, 5, 5],"float64"), Tensor([5, 5, 0],"float64"), )

[paddle error] paddle.einsum("...,...->...", Tensor([5, 5, 5],"float64"), Tensor([5, 5, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:09:36.659226 test begin: paddle.einsum("...->...", Tensor([0, 5, 5],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992976 (unix time) try "date -d @1740992976" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f45f7804c32) received by PID 35837 (TID 0x7f44e187e700) from PID 18446744073566964786 ***]

2025-03-03 17:09:41.411559 test begin: paddle.einsum("...->...", Tensor([5, 0, 5],"float64"), )

W0303 17:09:44.744536 37660 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:09:44.745597 37660 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992984 (unix time) try "date -d @1740992984" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd85b14dc32) received by PID 37084 (TID 0x7fd77d237700) from PID 1528093746 ***]

2025-03-03 17:09:49.216060 test begin: paddle.einsum("...->...", Tensor([5, 5, 0],"float64"), )

W0303 17:09:52.515625 38672 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:09:52.516744 38672 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740992992 (unix time) try "date -d @1740992992" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f09815d5c32) received by PID 38160 (TID 0x7f08b334a700) from PID 18446744071584963634 ***]

2025-03-03 17:10:04.025188 test begin: paddle.einsum("...a,a...->...", Tensor([0, 2, 2, 10],"float64"), Tensor([0, 3, 2, 2],"float64"), )

W0303 17:10:07.266305 41000 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:10:07.267460 41000 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...a,a...->...", Tensor([0, 2, 2, 10],"float64"), Tensor([0, 3, 2, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `a`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:10:07.268548 test begin: paddle.einsum("...a,a...->...", Tensor([0, 2, 2, 10],"float64"), Tensor([10, 3, 2, 2],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993007 (unix time) try "date -d @1740993007" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fef88051c32) received by PID 40383 (TID 0x7fee9df48700) from PID 18446744071696620594 ***]

2025-03-03 17:10:11.845302 test begin: paddle.einsum("...a,a...->...", Tensor([3, 0, 2, 10],"float64"), Tensor([10, 0, 2, 2],"float64"), )

W0303 17:10:14.658208 42170 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:10:14.659395 42170 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...a,a...->...", Tensor([3, 0, 2, 10],"float64"), Tensor([10, 0, 2, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `x`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:10:14.660776 test begin: paddle.einsum("...a,a...->...", Tensor([3, 0, 2, 10],"float64"), Tensor([10, 3, 2, 2],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993014 (unix time) try "date -d @1740993014" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3a0aaafc32) received by PID 41529 (TID 0x7f38f47c3700) from PID 178977842 ***]

2025-03-03 17:10:19.437795 test begin: paddle.einsum("...a,a...->...", Tensor([3, 2, 0, 10],"float64"), Tensor([10, 3, 0, 2],"float64"), )

W0303 17:10:23.651355 43076 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:10:23.652400 43076 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...a,a...->...", Tensor([3, 2, 0, 10],"float64"), Tensor([10, 3, 0, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `y`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:10:23.653551 test begin: paddle.einsum("...a,a...->...", Tensor([3, 2, 0, 10],"float64"), Tensor([10, 3, 2, 2],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993023 (unix time) try "date -d @1740993023" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f54c2cf8c32) received by PID 42622 (TID 0x7f53d67c3700) from PID 18446744072682966066 ***]

2025-03-03 17:10:27.858313 test begin: paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 0],"float64"), Tensor([10, 3, 2, 0],"float64"), )

W0303 17:10:31.637665 44557 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:10:31.639315 44557 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 0],"float64"), Tensor([10, 3, 2, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `w`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:10:31.640924 test begin: paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 0],"float64"), Tensor([10, 3, 2, 2],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993031 (unix time) try "date -d @1740993031" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f91381aac32) received by PID 43674 (TID 0x7f903bf48700) from PID 941272114 ***]

2025-03-03 17:10:36.492176 test begin: paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 10],"float64"), Tensor([0, 3, 2, 2],"float64"), )

W0303 17:10:40.177275 45577 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:10:40.178359 45577 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 10],"float64"), Tensor([0, 3, 2, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `a`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:10:40.180486 test begin: paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 10],"float64"), Tensor([10, 0, 2, 2],"float64"), )

[paddle error] paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 10],"float64"), Tensor([10, 0, 2, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `x`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:10:40.183502 test begin: paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 10],"float64"), Tensor([10, 3, 0, 2],"float64"), )

[paddle error] paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 10],"float64"), Tensor([10, 3, 0, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `p`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:10:40.185440 test begin: paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 10],"float64"), Tensor([10, 3, 2, 0],"float64"), )

[paddle error] paddle.einsum("...a,a...->...", Tensor([3, 2, 2, 10],"float64"), Tensor([10, 3, 2, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:10:40.187095 test begin: paddle.einsum("...i, ...i", Tensor([0, 3, 10],"float64"), Tensor([10],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993040 (unix time) try "date -d @1740993040" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ffa3ec4ec32) received by PID 45090 (TID 0x7ff92c7c3700) from PID 1053092914 ***]

2025-03-03 17:10:45.303797 test begin: paddle.einsum("...i, ...i", Tensor([0, 3, 11],"float64"), Tensor([11],"float64"), )

W0303 17:10:48.599272 46481 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:10:48.604187 46481 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993048 (unix time) try "date -d @1740993048" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f56cb90cc32) received by PID 45878 (TID 0x7f55c387e700) from PID 18446744072829848626 ***]

2025-03-03 17:10:53.217616 test begin: paddle.einsum("...i, ...i", Tensor([2, 0, 10],"float64"), Tensor([10],"float64"), )

W0303 17:10:56.861445 47872 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:10:56.862591 47872 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993056 (unix time) try "date -d @1740993056" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5b8cb1dc32) received by PID 46984 (TID 0x7f5a84949700) from PID 18446744071775050802 ***]

2025-03-03 17:11:01.732124 test begin: paddle.einsum("...i, ...i", Tensor([2, 0, 11],"float64"), Tensor([11],"float64"), )

W0303 17:11:05.909369 49078 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:11:05.910374 49078 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993065 (unix time) try "date -d @1740993065" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7c6f098c32) received by PID 48438 (TID 0x7f7b85177700) from PID 1862896690 ***]

2025-03-03 17:11:10.574724 test begin: paddle.einsum("...i, ...i", Tensor([2, 3, 0],"float64"), Tensor([10],"float64"), )

W0303 17:11:13.940542 50066 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:11:13.941620 50066 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993073 (unix time) try "date -d @1740993073" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f357277ec32) received by PID 49430 (TID 0x7f34b27c3700) from PID 1920461874 ***]

2025-03-03 17:11:19.587968 test begin: paddle.einsum("...i, ...i", Tensor([2, 3, 0],"float64"), Tensor([11],"float64"), )

W0303 17:11:22.837487 51382 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:11:22.838642 51382 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993082 (unix time) try "date -d @1740993082" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f31ea28bc32) received by PID 50717 (TID 0x7f30f1dc2700) from PID 18446744073343122482 ***]

2025-03-03 17:11:27.606885 test begin: paddle.einsum("...i, ...i", Tensor([2, 3, 10],"float64"), Tensor([0],"float64"), )

W0303 17:11:30.810436 53063 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:11:30.811475 53063 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...i, ...i", Tensor([2, 3, 10],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:11:30.813482 test begin: paddle.einsum("...i, ...i", Tensor([2, 3, 11],"float64"), Tensor([0],"float64"), )

[paddle error] paddle.einsum("...i, ...i", Tensor([2, 3, 11],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:11:30.820550 test begin: paddle.einsum("...i->...", Tensor([0, 3, 10],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993090 (unix time) try "date -d @1740993090" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff374b3fc32) received by PID 51983 (TID 0x7ff288949700) from PID 1957952562 ***]

2025-03-03 17:11:35.366515 test begin: paddle.einsum("...i->...", Tensor([0, 3, 11],"float64"), )

W0303 17:11:39.467638 53984 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:11:39.468891 53984 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993099 (unix time) try "date -d @1740993099" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd791e8ac32) received by PID 53319 (TID 0x7fd6b5dc2700) from PID 18446744071862529074 ***]

2025-03-03 17:11:51.345998 test begin: paddle.einsum("...i->...", Tensor([2, 0, 10],"float64"), )

W0303 17:11:55.088873 56011 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:11:55.089929 56011 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993115 (unix time) try "date -d @1740993115" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe49b6f6c32) received by PID 55509 (TID 0x7fe3b3744700) from PID 18446744072022354994 ***]

2025-03-03 17:11:59.686588 test begin: paddle.einsum("...i->...", Tensor([2, 0, 11],"float64"), )

W0303 17:12:02.891250 57631 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:12:02.892283 57631 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993122 (unix time) try "date -d @1740993122" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fad5437cc32) received by PID 56659 (TID 0x7fac6df48700) from PID 1412942898 ***]

2025-03-03 17:12:13.998148 test begin: paddle.einsum("...i->...", Tensor([2, 3, 0],"float64"), )

W0303 17:12:17.207579 59541 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:12:17.208978 59541 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993137 (unix time) try "date -d @1740993137" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9248889c32) received by PID 58972 (TID 0x7f917e949700) from PID 1216912434 ***]

2025-03-03 17:12:21.769224 test begin: paddle.einsum("...ii,...i->...i", Tensor([0, 13, 13, 12, 12],"float64"), Tensor([1, 12],"float64"), )

W0303 17:12:25.455463 60666 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:12:25.456588 60666 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993145 (unix time) try "date -d @1740993145" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1114c7dc32) received by PID 60036 (TID 0x7f0ff8949700) from PID 348642354 ***]

2025-03-03 17:12:37.237044 test begin: paddle.einsum("...ii,...i->...i", Tensor([32, 0, 13, 12, 12],"float64"), Tensor([1, 12],"float64"), )

W0303 17:12:41.349067 62958 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:12:41.350256 62958 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993161 (unix time) try "date -d @1740993161" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7febdec1bc32) received by PID 62363 (TID 0x7feab07c3700) from PID 18446744073151822898 ***]

2025-03-03 17:12:46.315212 test begin: paddle.einsum("...ii,...i->...i", Tensor([32, 13, 0, 12, 12],"float64"), Tensor([1, 12],"float64"), )

W0303 17:12:49.769208 64628 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:12:49.770179 64628 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993169 (unix time) try "date -d @1740993169" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f28600afc32) received by PID 63606 (TID 0x7f2769f48700) from PID 1611332658 ***]

2025-03-03 17:13:01.679631 test begin: paddle.einsum("...ii,...i->...i", Tensor([32, 13, 13, 0, 12],"float64"), Tensor([1, 12],"float64"), )

W0303 17:13:05.105896 66935 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:13:05.107415 66935 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993185 (unix time) try "date -d @1740993185" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f059aa83c32) received by PID 66290 (TID 0x7f048e7c3700) from PID 18446744072009301042 ***]

2025-03-03 17:13:10.426888 test begin: paddle.einsum("...ii,...i->...i", Tensor([32, 13, 13, 12, 0],"float64"), Tensor([1, 12],"float64"), )

W0303 17:13:15.350155 68043 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:13:15.351308 68043 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...ii,...i->...i", Tensor([32, 13, 13, 12, 0],"float64"), Tensor([1, 12],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:12 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:13:15.353385 test begin: paddle.einsum("...ii,...i->...i", Tensor([32, 13, 13, 12, 12],"float64"), Tensor([0, 12],"float64"), )

[paddle error] paddle.einsum("...ii,...i->...i", Tensor([32, 13, 13, 12, 12],"float64"), Tensor([0, 12],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `l`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:13:15.370015 test begin: paddle.einsum("...ii,...i->...i", Tensor([32, 13, 13, 12, 12],"float64"), Tensor([1, 0],"float64"), )

[paddle error] paddle.einsum("...ii,...i->...i", Tensor([32, 13, 13, 12, 12],"float64"), Tensor([1, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:12 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:13:15.387464 test begin: paddle.einsum("...ij,...i->j...", Tensor([0, 11],"float64"), Tensor([3, 4, 5, 10],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993195 (unix time) try "date -d @1740993195" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f308ba56c32) received by PID 67333 (TID 0x7f2fa1abb700) from PID 18446744071757458482 ***]

2025-03-03 17:13:27.834360 test begin: paddle.einsum("...ij,...i->j...", Tensor([10, 0],"float64"), Tensor([3, 4, 5, 10],"float64"), )

W0303 17:13:32.262720 70482 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:13:32.264039 70482 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993212 (unix time) try "date -d @1740993212" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd756308c32) received by PID 69671 (TID 0x7fd63fdc2700) from PID 1446022194 ***]

2025-03-03 17:13:37.272694 test begin: paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([0, 4, 5, 10],"float64"), )

W0303 17:13:41.046703 71617 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:13:41.047835 71617 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([0, 4, 5, 10],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:13:41.049772 test begin: paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([3, 0, 5, 10],"float64"), )

[paddle error] paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([3, 0, 5, 10],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:13:41.053324 test begin: paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([3, 4, 0, 10],"float64"), )

[paddle error] paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([3, 4, 0, 10],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:13:41.055724 test begin: paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([3, 4, 5, 0],"float64"), )

[paddle error] paddle.einsum("...ij,...i->j...", Tensor([10, 11],"float64"), Tensor([3, 4, 5, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:13:41.057624 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([0, 5],"float64"), Tensor([0, 1],"float64"), )

[paddle error] paddle.einsum("...ij,...jk->...ik", Tensor([0, 5],"float64"), Tensor([0, 1],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:13:41.058771 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), )

[paddle error] paddle.einsum("...ij,...jk->...ik", Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:13:41.059476 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([0, 5],"float64"), Tensor([5, 1],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993221 (unix time) try "date -d @1740993221" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f932ef79c32) received by PID 70970 (TID 0x7f92187c3700) from PID 787979314 ***]

2025-03-03 17:13:45.575341 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([0, 5],"float64"), Tensor([5, 5],"float64"), )

W0303 17:13:48.509339 72467 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:13:48.510350 72467 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993228 (unix time) try "date -d @1740993228" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fead0ffac32) received by PID 71876 (TID 0x7fe9ca949700) from PID 18446744072921001010 ***]

2025-03-03 17:13:53.300864 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([1, 0],"float64"), Tensor([5, 0],"float64"), )

W0303 17:13:56.862114 74033 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:13:56.863211 74033 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993236 (unix time) try "date -d @1740993236" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f08b0adac32) received by PID 72892 (TID 0x7f078e949700) from PID 18446744072378756146 ***]

2025-03-03 17:14:08.733555 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([1, 0],"float64"), Tensor([5, 1],"float64"), )

W0303 17:14:13.976950 75971 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:14:13.977962 75971 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993253 (unix time) try "date -d @1740993253" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa62e8d4c32) received by PID 75470 (TID 0x7fa54a7c3700) from PID 781012018 ***]

2025-03-03 17:14:19.900633 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([1, 0],"float64"), Tensor([5, 5],"float64"), )

W0303 17:14:23.217784 77168 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:14:23.218809 77168 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993263 (unix time) try "date -d @1740993263" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5c24b53c32) received by PID 76607 (TID 0x7f5b3c949700) from PID 615857202 ***]

2025-03-03 17:14:27.501579 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([1, 5],"float64"), Tensor([0, 1],"float64"), )

W0303 17:14:30.424782 78942 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:14:30.425951 78942 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...ij,...jk->...ik", Tensor([1, 5],"float64"), Tensor([0, 1],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:14:30.427484 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), )

[paddle error] paddle.einsum("...ij,...jk->...ik", Tensor([1, 5],"float64"), Tensor([0, 5],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:14:30.431463 test begin: paddle.einsum("...ij,...jk->...ik", Tensor([1, 5],"float64"), Tensor([5, 0],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993270 (unix time) try "date -d @1740993270" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f02dd7ffc32) received by PID 77922 (TID 0x7f01fb6f8700) from PID 18446744073130736690 ***]

2025-03-03 17:14:35.087729 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([0, 28, 28],"float32"), Tensor([3, 6, 28, 28],"float32"), )

W0303 17:14:38.889350 79710 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:14:38.890484 79710 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993278 (unix time) try "date -d @1740993278" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8a892ebfc2) received by PID 79133 (TID 0x7f89af34a700) from PID 18446744071716126658 ***]

2025-03-03 17:14:43.305388 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([0, 6, 8, 8],"float32"), Tensor([2, 3, 6, 8, 8],"float32"), )

W0303 17:14:46.452561 80863 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:14:46.454245 80863 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993286 (unix time) try "date -d @1740993286" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2e2b845fc2) received by PID 80358 (TID 0x7f2d5387e700) from PID 730095554 ***]

2025-03-03 17:14:51.936885 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([0, 8, 8],"float32"), Tensor([3, 6, 8, 8],"float32"), )

W0303 17:14:55.819694 81730 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:14:55.821141 81730 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993295 (unix time) try "date -d @1740993295" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f12c6368fc2) received by PID 81291 (TID 0x7f11e1dc2700) from PID 18446744072740048834 ***]

2025-03-03 17:15:00.045351 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 0, 8, 8],"float32"), Tensor([2, 3, 6, 8, 8],"float32"), )

W0303 17:15:05.277081 83313 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:15:05.278331 83313 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993305 (unix time) try "date -d @1740993305" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f08222a3fc2) received by PID 82508 (TID 0x7f0733dc2700) from PID 573194178 ***]

2025-03-03 17:15:09.830952 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 0, 8],"float32"), Tensor([2, 3, 6, 8, 8],"float32"), )

W0303 17:15:12.632022 84613 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:15:12.633121 84613 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993312 (unix time) try "date -d @1740993312" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa280d6afc2) received by PID 83877 (TID 0x7fa148949700) from PID 18446744071576137666 ***]

2025-03-03 17:15:17.002454 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 0],"float32"), Tensor([2, 3, 6, 8, 8],"float32"), )

W0303 17:15:21.064515 85501 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:15:21.066026 85501 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993321 (unix time) try "date -d @1740993321" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f619d83cfc2) received by PID 84933 (TID 0x7f60bf6f8700) from PID 18446744072057245634 ***]

2025-03-03 17:15:25.287748 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([0, 3, 6, 8, 8],"float32"), )

W0303 17:15:28.759135 86793 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:15:28.760286 86793 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([0, 3, 6, 8, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:15:28.767140 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([2, 0, 6, 8, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993328 (unix time) try "date -d @1740993328" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8d301b6fc2) received by PID 86071 (TID 0x7f8c65f48700) from PID 807104450 ***]

2025-03-03 17:15:33.164087 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([2, 3, 0, 8, 8],"float32"), )

W0303 17:15:36.672111 88085 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:15:36.673195 88085 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([2, 3, 0, 8, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:6 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:15:36.674852 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([2, 3, 6, 0, 8],"float32"), )

[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([2, 3, 6, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:15:36.679393 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([2, 3, 6, 8, 0],"float32"), )

[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([2, 6, 8, 8],"float32"), Tensor([2, 3, 6, 8, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:15:36.683132 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 0, 28],"float32"), Tensor([3, 6, 28, 28],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993336 (unix time) try "date -d @1740993336" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f94fb8bcfc2) received by PID 87598 (TID 0x7f942387e700) from PID 18446744073634828226 ***]

2025-03-03 17:15:41.452173 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 0, 8],"float32"), Tensor([3, 6, 8, 8],"float32"), )

W0303 17:15:44.703651 89102 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:15:44.704630 89102 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993344 (unix time) try "date -d @1740993344" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f979ec68fc2) received by PID 88526 (TID 0x7f96a67c3700) from PID 18446744072078397378 ***]

2025-03-03 17:15:49.296206 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 0],"float32"), Tensor([3, 6, 28, 28],"float32"), )

W0303 17:15:52.428077 90311 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:15:52.429685 90311 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993352 (unix time) try "date -d @1740993352" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9412ba2fc2) received by PID 89664 (TID 0x7f92f07c3700) from PID 314191810 ***]

2025-03-03 17:15:56.930554 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 28],"float32"), Tensor([0, 6, 28, 28],"float32"), )

W0303 17:16:00.516729 91540 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:16:00.517822 91540 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993360 (unix time) try "date -d @1740993360" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0f530f7fc2) received by PID 90817 (TID 0x7f0e371b7700) from PID 1393524674 ***]

2025-03-03 17:16:05.928010 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 28],"float32"), Tensor([3, 0, 28, 28],"float32"), )

W0303 17:16:08.916689 92662 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:16:08.918196 92662 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 28],"float32"), Tensor([3, 0, 28, 28],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:6 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:16:08.919767 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 28],"float32"), Tensor([3, 6, 0, 28],"float32"), )

[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 28],"float32"), Tensor([3, 6, 0, 28],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:28 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:16:08.924031 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 28],"float32"), Tensor([3, 6, 28, 0],"float32"), )

[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 28, 28],"float32"), Tensor([3, 6, 28, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:28 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:16:08.927235 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 0],"float32"), Tensor([3, 6, 8, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993368 (unix time) try "date -d @1740993368" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8fddc67fc2) received by PID 92097 (TID 0x7f8ef3b85700) from PID 18446744073135357890 ***]

2025-03-03 17:16:13.143043 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 8],"float32"), Tensor([0, 6, 8, 8],"float32"), )

W0303 17:16:16.148365 93788 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:16:16.149397 93788 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993376 (unix time) try "date -d @1740993376" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6de6986fc2) received by PID 93178 (TID 0x7f6cd67c3700) from PID 18446744073283334082 ***]

2025-03-03 17:16:20.597015 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 8],"float32"), Tensor([3, 0, 8, 8],"float32"), )

W0303 17:16:24.741550 94778 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:16:24.742635 94778 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 8],"float32"), Tensor([3, 0, 8, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:6 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:16:24.744155 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 8],"float32"), Tensor([3, 6, 0, 8],"float32"), )

[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 8],"float32"), Tensor([3, 6, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:16:24.754905 test begin: paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 8],"float32"), Tensor([3, 6, 8, 0],"float32"), )

[paddle error] paddle.einsum("...ijk, ...xijk -> ...xjk", Tensor([6, 8, 8],"float32"), Tensor([3, 6, 8, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:16:24.758155 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([0, 10, 3],"float64"), Tensor([0, 3, 10],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993384 (unix time) try "date -d @1740993384" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fefaa12cc32) received by PID 94131 (TID 0x7feea1dc2700) from PID 18446744072267942962 ***]

2025-03-03 17:16:36.861893 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([0, 10, 3],"float64"), Tensor([3, 3, 10],"float64"), )

W0303 17:16:39.567049 97304 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:16:39.568084 97304 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993399 (unix time) try "date -d @1740993399" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fca308b9c32) received by PID 96722 (TID 0x7fc95e949700) from PID 814455858 ***]

2025-03-03 17:16:43.894844 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([3, 0, 3],"float64"), Tensor([3, 0, 10],"float64"), )

W0303 17:16:49.509538 98147 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:16:49.511003 98147 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...jk, ...kl->...jl", Tensor([3, 0, 3],"float64"), Tensor([3, 0, 10],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:16:49.512300 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([3, 0, 3],"float64"), Tensor([3, 3, 10],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993409 (unix time) try "date -d @1740993409" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fcc22abbc32) received by PID 97631 (TID 0x7fcb2e7c3700) from PID 581680178 ***]

2025-03-03 17:16:54.269242 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 0],"float64"), Tensor([3, 3, 0],"float64"), )

W0303 17:16:59.420974 99891 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:16:59.422070 99891 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993419 (unix time) try "date -d @1740993419" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4daaa28c32) received by PID 98961 (TID 0x7f4ca47c3700) from PID 18446744072277363762 ***]

2025-03-03 17:17:03.893022 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 0],"float64"), Tensor([3, 3, 10],"float64"), )

W0303 17:17:07.325016 101399 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:17:07.326305 101399 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993427 (unix time) try "date -d @1740993427" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efd7fc55c32) received by PID 100755 (TID 0x7efc93d0b700) from PID 2143640626 ***]

2025-03-03 17:17:13.747264 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 3],"float64"), Tensor([0, 3, 10],"float64"), )

W0303 17:17:16.861202 102527 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:17:16.862299 102527 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 3],"float64"), Tensor([0, 3, 10],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `s`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:17:16.864081 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 3],"float64"), Tensor([3, 0, 10],"float64"), )

[paddle error] paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 3],"float64"), Tensor([3, 0, 10],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:17:16.867600 test begin: paddle.einsum("...jk, ...kl->...jl", Tensor([3, 10, 3],"float64"), Tensor([3, 3, 0],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993436 (unix time) try "date -d @1740993436" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb036051c32) received by PID 101817 (TID 0x7faf41dc2700) from PID 906304562 ***]

2025-03-03 17:17:22.587343 test begin: paddle.einsum("...jk->...kj", Tensor([0, 10, 3],"float64"), )

W0303 17:17:25.869737 103808 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:17:25.871017 103808 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993445 (unix time) try "date -d @1740993445" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdc297e5c32) received by PID 103003 (TID 0x7fdb3b6f8700) from PID 696146994 ***]

2025-03-03 17:17:30.486253 test begin: paddle.einsum("...jk->...kj", Tensor([3, 0, 3],"float64"), )

W0303 17:17:33.974192 104979 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:17:33.975777 104979 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993453 (unix time) try "date -d @1740993453" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fea306f2c32) received by PID 104312 (TID 0x7fe957f48700) from PID 812592178 ***]

2025-03-03 17:17:38.768598 test begin: paddle.einsum("...jk->...kj", Tensor([3, 10, 0],"float64"), )

W0303 17:17:41.868817 106038 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:17:41.869972 106038 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993461 (unix time) try "date -d @1740993461" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd2894bac32) received by PID 105526 (TID 0x7fd18734a700) from PID 18446744071718022194 ***]

2025-03-03 17:17:46.233911 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([0, 4, 3, 1, 2],"float32"), Tensor([0, 4, 3, 2, 8],"float32"), )

W0303 17:17:49.313009 107095 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:17:49.314731 107095 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993469 (unix time) try "date -d @1740993469" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fab526cafc2) received by PID 106602 (TID 0x7faa55dc2700) from PID 1382854594 ***]

2025-03-03 17:17:53.776009 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([0, 4, 3, 1, 2],"float32"), Tensor([13, 4, 3, 2, 8],"float32"), )

W0303 17:17:56.923722 108242 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:17:56.924979 108242 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993476 (unix time) try "date -d @1740993476" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fce0ad18fc2) received by PID 107444 (TID 0x7fccec7c3700) from PID 181505986 ***]

2025-03-03 17:18:01.031501 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([0, 4, 3, 1, 2],"float32"), Tensor([52, 4, 3, 2, 8],"float32"), )

W0303 17:18:04.014555 109444 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:18:04.015650 109444 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993484 (unix time) try "date -d @1740993484" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f76b95bbfc2) received by PID 108790 (TID 0x7f75dd34a700) from PID 18446744072524382146 ***]

2025-03-03 17:18:08.227776 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 0, 3, 1, 2],"float32"), Tensor([13, 0, 3, 2, 8],"float32"), )

W0303 17:18:11.406391 110355 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:18:11.407318 110355 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993491 (unix time) try "date -d @1740993491" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f42aa4c9fc2) received by PID 109776 (TID 0x7f41c7dc2700) from PID 18446744072271732674 ***]

2025-03-03 17:18:15.705577 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 0, 3, 1, 2],"float32"), Tensor([13, 4, 3, 2, 8],"float32"), )

W0303 17:18:20.256458 111558 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:18:20.257795 111558 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993500 (unix time) try "date -d @1740993500" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdb19337fc2) received by PID 110909 (TID 0x7fda4734a700) from PID 422805442 ***]

2025-03-03 17:18:25.148357 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 0, 1, 2],"float32"), Tensor([13, 4, 0, 2, 8],"float32"), )

W0303 17:18:28.880007 112827 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:18:28.881084 112827 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993508 (unix time) try "date -d @1740993508" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc20b7c2fc2) received by PID 111945 (TID 0x7fc0f387e700) from PID 192688066 ***]

2025-03-03 17:18:33.666596 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 0, 1, 2],"float32"), Tensor([13, 4, 3, 2, 8],"float32"), )

W0303 17:18:37.363232 114124 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:18:37.364391 114124 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993517 (unix time) try "date -d @1740993517" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7faa5515cfc2) received by PID 113463 (TID 0x7fa920949700) from PID 1427492802 ***]

2025-03-03 17:18:41.616412 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 0, 2],"float32"), Tensor([13, 4, 3, 0, 8],"float32"), )

W0303 17:18:44.563325 115275 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:18:44.564622 115275 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 0, 2],"float32"), Tensor([13, 4, 3, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:18:44.565741 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 0, 2],"float32"), Tensor([13, 4, 3, 2, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993524 (unix time) try "date -d @1740993524" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4dc0660fc2) received by PID 114697 (TID 0x7f4cd5f48700) from PID 18446744072642498498 ***]

2025-03-03 17:18:49.994603 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 0],"float32"), Tensor([13, 4, 3, 2, 0],"float32"), )

W0303 17:18:53.965432 116270 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:18:53.966563 116270 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993533 (unix time) try "date -d @1740993533" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff98907ffc2) received by PID 115694 (TID 0x7ff88c949700) from PID 18446744071713587138 ***]

2025-03-03 17:18:58.527494 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 0],"float32"), Tensor([13, 4, 3, 2, 8],"float32"), )

W0303 17:19:01.671136 117730 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:19:01.672348 117730 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993541 (unix time) try "date -d @1740993541" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7032eacfc2) received by PID 116917 (TID 0x7f6f167c3700) from PID 854249410 ***]

2025-03-03 17:19:05.888257 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([0, 4, 3, 2, 8],"float32"), )

W0303 17:19:11.143911 118527 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:19:11.145143 118527 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([0, 4, 3, 2, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `g`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:19:11.147058 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([13, 0, 3, 2, 8],"float32"), )

[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([13, 0, 3, 2, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `r`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:19:11.151754 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([13, 4, 0, 2, 8],"float32"), )

[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([13, 4, 0, 2, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `a`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:19:11.155723 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([13, 4, 3, 0, 8],"float32"), )

[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([13, 4, 3, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:19:11.159832 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([13, 4, 3, 1, 2],"float32"), Tensor([13, 4, 3, 2, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993551 (unix time) try "date -d @1740993551" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6e83d3dfc2) received by PID 118135 (TID 0x7f6db3d0b700) from PID 18446744071626284994 ***]

2025-03-03 17:19:16.086256 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 0, 3, 1, 2],"float32"), Tensor([52, 0, 3, 2, 8],"float32"), )

W0303 17:19:19.142328 119784 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:19:19.143213 119784 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993559 (unix time) try "date -d @1740993559" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f675a6d9fc2) received by PID 119193 (TID 0x7f66527c3700) from PID 1517133762 ***]

2025-03-03 17:19:23.688998 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 0, 3, 1, 2],"float32"), Tensor([52, 4, 3, 2, 8],"float32"), )

W0303 17:19:26.951563 121104 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:19:26.952771 121104 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993566 (unix time) try "date -d @1740993566" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9be0064fc2) received by PID 120377 (TID 0x7f9ac1f48700) from PID 18446744073173094338 ***]

2025-03-03 17:19:31.044140 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 0, 1, 2],"float32"), Tensor([52, 4, 0, 2, 8],"float32"), )

W0303 17:19:37.107717 122565 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:19:37.108840 122565 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993577 (unix time) try "date -d @1740993577" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1556c1dfc2) received by PID 121997 (TID 0x7f146e7c3700) from PID 1455546306 ***]

2025-03-03 17:19:41.390793 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 0, 1, 2],"float32"), Tensor([52, 4, 3, 2, 8],"float32"), )

W0303 17:19:45.444123 123582 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:19:45.445309 123582 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993585 (unix time) try "date -d @1740993585" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3f577bbfc2) received by PID 123244 (TID 0x7f3e3d87e700) from PID 1467727810 ***]

2025-03-03 17:19:50.235844 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 0, 2],"float32"), Tensor([52, 4, 3, 0, 8],"float32"), )

W0303 17:19:54.957481 124660 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:19:54.958557 124660 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 0, 2],"float32"), Tensor([52, 4, 3, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:19:54.959446 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 0, 2],"float32"), Tensor([52, 4, 3, 2, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993594 (unix time) try "date -d @1740993594" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f96473f1fc2) received by PID 124092 (TID 0x7f95452b7700) from PID 1195319234 ***]

2025-03-03 17:19:59.485239 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 0],"float32"), Tensor([52, 4, 3, 2, 0],"float32"), )

W0303 17:20:02.804977 126255 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:20:02.806007 126255 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993602 (unix time) try "date -d @1740993602" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f40ced9efc2) received by PID 125383 (TID 0x7f3fee7c3700) from PID 18446744072884973506 ***]

2025-03-03 17:20:07.202119 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 0],"float32"), Tensor([52, 4, 3, 2, 8],"float32"), )

W0303 17:20:10.350356 127337 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:20:10.351433 127337 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993610 (unix time) try "date -d @1740993610" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fafcd9dafc2) received by PID 126689 (TID 0x7faec5935700) from PID 18446744072864247746 ***]

2025-03-03 17:20:14.647183 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([0, 4, 3, 2, 8],"float32"), )

W0303 17:20:18.637558 128392 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:20:18.638473 128392 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([0, 4, 3, 2, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:52 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:20:18.639744 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([52, 0, 3, 2, 8],"float32"), )

[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([52, 0, 3, 2, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `w`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:20:18.642147 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([52, 4, 0, 2, 8],"float32"), )

[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([52, 4, 0, 2, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `p`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:20:18.643882 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([52, 4, 3, 0, 8],"float32"), )

[paddle error] paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([52, 4, 3, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:20:18.645627 test begin: paddle.einsum("...qk,...kd->...qd", Tensor([52, 4, 3, 1, 2],"float32"), Tensor([52, 4, 3, 2, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993618 (unix time) try "date -d @1740993618" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5bac423fc2) received by PID 127745 (TID 0x7f5ad1f48700) from PID 18446744072304607170 ***]

2025-03-03 17:20:23.140895 test begin: paddle.einsum("a...a->...", Tensor([0, 3, 2, 1, 4, 5],"float64"), )

W0303 17:20:26.351960 129476 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:20:26.352964 129476 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993626 (unix time) try "date -d @1740993626" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7faefb77ac32) received by PID 128874 (TID 0x7fae13744700) from PID 18446744073633508402 ***]

2025-03-03 17:20:31.012505 test begin: paddle.einsum("a...a->...", Tensor([5, 0, 2, 1, 4, 5],"float64"), )

W0303 17:20:34.866634 130819 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:20:34.868105 130819 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993634 (unix time) try "date -d @1740993634" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fab7bbc4c32) received by PID 129952 (TID 0x7faa75abb700) from PID 2075937842 ***]

2025-03-03 17:20:39.187746 test begin: paddle.einsum("a...a->...", Tensor([5, 3, 0, 1, 4, 5],"float64"), )

W0303 17:20:42.237627 131879 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:20:42.238750 131879 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993642 (unix time) try "date -d @1740993642" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f25d231cc32) received by PID 131473 (TID 0x7f24e5dc2700) from PID 18446744072941063218 ***]

2025-03-03 17:20:47.184759 test begin: paddle.einsum("a...a->...", Tensor([5, 3, 2, 0, 4, 5],"float64"), )

W0303 17:20:50.104806 133075 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:20:50.105700 133075 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993650 (unix time) try "date -d @1740993650" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb3961b6c32) received by PID 132240 (TID 0x7fb2b5dc2700) from PID 18446744071932963890 ***]

2025-03-03 17:20:55.347106 test begin: paddle.einsum("a...a->...", Tensor([5, 3, 2, 1, 0, 5],"float64"), )

W0303 17:20:58.571388 134510 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:20:58.572454 134510 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993658 (unix time) try "date -d @1740993658" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5d84fe5c32) received by PID 133627 (TID 0x7f5c86949700) from PID 18446744071645846578 ***]

2025-03-03 17:21:03.196262 test begin: paddle.einsum("a...a->...", Tensor([5, 3, 2, 1, 4, 0],"float64"), )

W0303 17:21:06.351936 135810 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:21:06.352922 135810 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("a...a->...", Tensor([5, 3, 2, 1, 4, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `a`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:21:06.354072 test begin: paddle.einsum("a...a->a...", Tensor([0, 3, 2, 1, 4, 5],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993666 (unix time) try "date -d @1740993666" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa083476c32) received by PID 135255 (TID 0x7f9f914f4700) from PID 18446744071617080370 ***]

2025-03-03 17:21:10.494641 test begin: paddle.einsum("a...a->a...", Tensor([5, 0, 2, 1, 4, 5],"float64"), )

W0303 17:21:14.078994 136681 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:21:14.080616 136681 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993674 (unix time) try "date -d @1740993674" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc90260cc32) received by PID 136055 (TID 0x7fc827dc2700) from PID 39898162 ***]

2025-03-03 17:21:18.512569 test begin: paddle.einsum("a...a->a...", Tensor([5, 3, 0, 1, 4, 5],"float64"), )

W0303 17:21:21.480067 137774 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:21:21.481436 137774 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993681 (unix time) try "date -d @1740993681" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ffa4e690c32) received by PID 137120 (TID 0x7ff92bdc2700) from PID 1315507250 ***]

2025-03-03 17:21:25.911651 test begin: paddle.einsum("a...a->a...", Tensor([5, 3, 2, 0, 4, 5],"float64"), )

W0303 17:21:30.605888 138985 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:21:30.606873 138985 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993690 (unix time) try "date -d @1740993690" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f13fb6f6c32) received by PID 138102 (TID 0x7f131f744700) from PID 18446744073632967730 ***]

2025-03-03 17:21:34.982785 test begin: paddle.einsum("a...a->a...", Tensor([5, 3, 2, 1, 0, 5],"float64"), )

W0303 17:21:37.977241 140280 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:21:37.978430 140280 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993697 (unix time) try "date -d @1740993697" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f318ab8cc32) received by PID 139560 (TID 0x7f30607c3700) from PID 18446744071741951026 ***]

2025-03-03 17:21:42.436541 test begin: paddle.einsum("a...a->a...", Tensor([5, 3, 2, 1, 4, 0],"float64"), )

W0303 17:21:45.404722 141407 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:21:45.405774 141407 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("a...a->a...", Tensor([5, 3, 2, 1, 4, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `a`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:21:45.407040 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([0, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([0, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), ) 
 Size of label 'b' for operand 1 (4) does not match previous terms (0).
2025-03-03 17:21:45.409919 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([0, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([0, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'a' for operand 2 (0) does not match previous terms (4).
2025-03-03 17:21:45.412294 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 0, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993705 (unix time) try "date -d @1740993705" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7d8d959c32) received by PID 140683 (TID 0x7f7c73935700) from PID 18446744071789976626 ***]

2025-03-03 17:21:49.719940 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 0, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

W0303 17:21:52.375067 142193 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:21:52.376096 142193 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 0, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'i' for operand 1 (0) does not match previous terms (3).
2025-03-03 17:21:52.377234 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993712 (unix time) try "date -d @1740993712" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f095ca65c32) received by PID 141554 (TID 0x7f0858949700) from PID 1554406450 ***]

2025-03-03 17:21:57.214017 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

W0303 17:21:59.980513 143469 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:21:59.981742 143469 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:21:59.984115 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 0],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 0],"float64"), ) 
 Size of label 'b' for operand 1 (0) does not match previous terms (4).
2025-03-03 17:21:59.986153 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'b' for operand 1 (0) does not match previous terms (4).
2025-03-03 17:21:59.988258 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'b' for operand 1 (4) does not match previous terms (0).
2025-03-03 17:21:59.989899 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'g' for operand 1 (3) does not match previous terms (0).
2025-03-03 17:21:59.991406 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993719 (unix time) try "date -d @1740993719" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1006a80c32) received by PID 142809 (TID 0x7f0ef67c3700) from PID 111676466 ***]

2025-03-03 17:22:05.054734 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 4],"float64"), )

W0303 17:22:08.825261 144581 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:22:08.826268 144581 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'c' for operand 2 (0) does not match previous terms (4).
2025-03-03 17:22:08.827700 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), ) 
 Size of label 'c' for operand 2 (4) does not match previous terms (0).
2025-03-03 17:22:08.830726 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), ) 
 Size of label 's' for operand 2 (3) does not match previous terms (0).
2025-03-03 17:22:08.835041 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:22:10.346662 test begin: paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 0],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...a", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 0],"float64"), ) 
 Size of label 'a' for operand 2 (4) does not match previous terms (0).
2025-03-03 17:22:10.350149 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([0, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([0, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), ) 
 Size of label 'b' for operand 1 (4) does not match previous terms (0).
2025-03-03 17:22:10.351768 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([0, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993730 (unix time) try "date -d @1740993730" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0950101c32) received by PID 143975 (TID 0x7f0827f48700) from PID 1343233074 ***]

2025-03-03 17:22:15.502179 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 0, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), )

W0303 17:22:18.147432 146045 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:22:18.148478 146045 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993738 (unix time) try "date -d @1740993738" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2d37930c32) received by PID 145322 (TID 0x7f2c3d87e700) from PID 932383794 ***]

2025-03-03 17:22:23.283630 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 0, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

W0303 17:22:26.306648 147167 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:22:26.307631 147167 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 0, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'n' for operand 1 (0) does not match previous terms (3).
2025-03-03 17:22:26.308738 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993746 (unix time) try "date -d @1740993746" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4eaf1adc32) received by PID 146374 (TID 0x7f4d7f277700) from PID 18446744072352357426 ***]

2025-03-03 17:22:37.964351 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

W0303 17:22:40.978066 149370 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:22:40.979447 149370 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:22:40.983857 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 0],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 0],"float64"), ) 
 Size of label 'b' for operand 1 (0) does not match previous terms (4).
2025-03-03 17:22:40.985688 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'b' for operand 1 (0) does not match previous terms (4).
2025-03-03 17:22:40.988997 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'b' for operand 1 (4) does not match previous terms (0).
2025-03-03 17:22:40.993380 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'e' for operand 1 (3) does not match previous terms (0).
2025-03-03 17:22:40.996909 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993761 (unix time) try "date -d @1740993761" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f77ff3aac32) received by PID 148718 (TID 0x7f77172b7700) from PID 18446744073696619570 ***]

2025-03-03 17:22:46.565529 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 4],"float64"), )

W0303 17:22:49.189620 150602 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:22:49.191476 150602 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 0],"float64"), Tensor([4, 3, 1, 4],"float64"), ) 
 Size of label 'c' for operand 2 (0) does not match previous terms (4).
2025-03-03 17:22:49.193812 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([0, 3, 1, 4],"float64"), ) 
 Size of label 'c' for operand 2 (4) does not match previous terms (0).
2025-03-03 17:22:49.198301 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 0, 1, 4],"float64"), ) 
 Size of label 'r' for operand 2 (3) does not match previous terms (0).
2025-03-03 17:22:49.200402 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), )

[paddle error] paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 0, 4],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:22:50.798383 test begin: paddle.einsum("a...b,b...c,c...d", Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 4],"float64"), Tensor([4, 3, 1, 0],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993770 (unix time) try "date -d @1740993770" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f445ac32c32) received by PID 149867 (TID 0x7f438c7c3700) from PID 1522740274 ***]

2025-03-03 17:23:08.347178 test begin: paddle.einsum("a...d,...cb->...abcd", Tensor([0, 3, 2, 3, 4],"float64"), Tensor([12, 10],"float64"), )

W0303 17:23:11.525542 153554 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:23:11.526547 153554 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993791 (unix time) try "date -d @1740993791" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f00d2a6cc32) received by PID 152934 (TID 0x7effc47c3700) from PID 18446744072948730930 ***]

2025-03-03 17:23:22.987763 test begin: paddle.einsum("a...d,...cb->...abcd", Tensor([10, 0, 2, 3, 4],"float64"), Tensor([12, 10],"float64"), )

W0303 17:23:26.266686 160739 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:23:26.267779 160739 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993806 (unix time) try "date -d @1740993806" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa338260c32) received by PID 159024 (TID 0x7fa239f48700) from PID 942017586 ***]

2025-03-03 17:23:30.702868 test begin: paddle.einsum("a...d,...cb->...abcd", Tensor([10, 3, 0, 3, 4],"float64"), Tensor([12, 10],"float64"), )

W0303 17:23:33.737959  1902 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:23:33.738902  1902 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993813 (unix time) try "date -d @1740993813" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f80e54fbc32) received by PID 162978 (TID 0x7f802334a700) from PID 18446744073261792306 ***]

2025-03-03 17:23:39.421768 test begin: paddle.einsum("a...d,...cb->...abcd", Tensor([10, 3, 2, 0, 4],"float64"), Tensor([12, 10],"float64"), )

W0303 17:23:42.436309  2909 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:23:42.437223  2909 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993822 (unix time) try "date -d @1740993822" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa07c2aac32) received by PID 2317 (TID 0x7f9f99f48700) from PID 2083171378 ***]

2025-03-03 17:23:46.913541 test begin: paddle.einsum("a...d,...cb->...abcd", Tensor([10, 3, 2, 3, 0],"float64"), Tensor([12, 10],"float64"), )

W0303 17:23:50.822314  3979 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:23:50.823414  3979 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993830 (unix time) try "date -d @1740993830" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9689728c32) received by PID 3413 (TID 0x7f956d6f8700) from PID 18446744071720569906 ***]

2025-03-03 17:23:55.340150 test begin: paddle.einsum("a...d,...cb->...abcd", Tensor([10, 3, 2, 3, 4],"float64"), Tensor([0, 10],"float64"), )

W0303 17:23:58.505561  5126 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:23:58.506928  5126 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993838 (unix time) try "date -d @1740993838" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdee31d8c32) received by PID 4420 (TID 0x7fdddb2b7700) from PID 18446744073224948786 ***]

2025-03-03 17:24:03.470181 test begin: paddle.einsum("a...d,...cb->...abcd", Tensor([10, 3, 2, 3, 4],"float64"), Tensor([12, 0],"float64"), )

W0303 17:24:09.591979  6475 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:24:09.593178  6475 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993849 (unix time) try "date -d @1740993849" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7febdd91fc32) received by PID 5976 (TID 0x7feaf7935700) from PID 18446744073131916338 ***]

2025-03-03 17:24:14.044989 test begin: paddle.einsum("aaa->a", Tensor([0, 5, 5],"float64"), )

W0303 17:24:17.348553  7862 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:24:17.349989  7862 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993857 (unix time) try "date -d @1740993857" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdca6402c32) received by PID 7375 (TID 0x7fdbabdc2700) from PID 18446744072203807794 ***]

2025-03-03 17:24:23.536297 test begin: paddle.einsum("aaa->a", Tensor([5, 0, 5],"float64"), )

W0303 17:24:26.591353  8950 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:24:26.592511  8950 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("aaa->a", Tensor([5, 0, 5],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `a`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:24:26.593455 test begin: paddle.einsum("aaa->a", Tensor([5, 5, 0],"float64"), )

[paddle error] paddle.einsum("aaa->a", Tensor([5, 5, 0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `a`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:24:26.595775 test begin: paddle.einsum("abcd,dfg->abcfg", Tensor([0, 4, 5, 3],"float64"), Tensor([3, 4, 5],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993866 (unix time) try "date -d @1740993866" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fad839dbc32) received by PID 8299 (TID 0x7fac99abb700) from PID 18446744071622736946 ***]

2025-03-03 17:24:31.131409 test begin: paddle.einsum("abcd,dfg->abcfg", Tensor([2, 0, 5, 3],"float64"), Tensor([3, 4, 5],"float64"), )

W0303 17:24:34.225886 10480 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:24:34.227234 10480 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993874 (unix time) try "date -d @1740993874" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f08e7682c32) received by PID 9812 (TID 0x7f0813744700) from PID 18446744073296948274 ***]

2025-03-03 17:24:47.341278 test begin: paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 0, 3],"float64"), Tensor([3, 4, 5],"float64"), )

W0303 17:24:51.870664 13542 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:24:51.871752 13542 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993891 (unix time) try "date -d @1740993891" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9fe68e8c32) received by PID 12973 (TID 0x7f9ef87c3700) from PID 18446744073282686002 ***]

2025-03-03 17:24:56.361307 test begin: paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 5, 0],"float64"), Tensor([3, 4, 5],"float64"), )

W0303 17:24:59.160451 15001 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:24:59.161404 15001 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993899 (unix time) try "date -d @1740993899" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f90e370fc32) received by PID 14550 (TID 0x7f8fdd744700) from PID 18446744073230416946 ***]

2025-03-03 17:25:03.847219 test begin: paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 5, 3],"float64"), Tensor([0, 4, 5],"float64"), )

W0303 17:25:06.332788 16371 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:25:06.334120 16371 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 5, 3],"float64"), Tensor([0, 4, 5],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:25:06.335753 test begin: paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 0, 5],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993906 (unix time) try "date -d @1740993906" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd69dd46c32) received by PID 15653 (TID 0x7fd5b5dc2700) from PID 18446744072062528562 ***]

2025-03-03 17:25:11.027087 test begin: paddle.einsum("abcd,dfg->abcfg", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 4, 0],"float64"), )

W0303 17:25:15.186743 17342 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:25:15.187922 17342 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993915 (unix time) try "date -d @1740993915" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f70036f9c32) received by PID 16702 (TID 0x7f6f19744700) from PID 57646130 ***]

2025-03-03 17:25:27.752579 test begin: paddle.einsum("ak, kn-> an", Tensor([0, 11],"float32"), Tensor([0, 50],"float32"), )

W0303 17:25:31.453622 19640 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:25:31.454790 19640 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("ak, kn-> an", Tensor([0, 11],"float32"), Tensor([0, 50],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:25:31.455839 test begin: paddle.einsum("ak, kn-> an", Tensor([0, 11],"float32"), Tensor([11, 50],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993931 (unix time) try "date -d @1740993931" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f65912a2fc2) received by PID 18905 (TID 0x7f64af34a700) from PID 18446744071850045378 ***]

2025-03-03 17:25:35.840653 test begin: paddle.einsum("ak, kn-> an", Tensor([15000, 0],"float32"), Tensor([11, 0],"float32"), )

W0303 17:25:39.049962 20626 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:25:39.051097 20626 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993939 (unix time) try "date -d @1740993939" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f88e8dbcfc2) received by PID 19988 (TID 0x7f87ec949700) from PID 18446744073321304002 ***]

2025-03-03 17:25:50.665932 test begin: paddle.einsum("ak, kn-> an", Tensor([15000, 0],"float32"), Tensor([11, 50],"float32"), )

W0303 17:25:55.188462 22544 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:25:55.189617 22544 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993955 (unix time) try "date -d @1740993955" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fab23a50fc2) received by PID 21960 (TID 0x7faa09abb700) from PID 598020034 ***]

2025-03-03 17:26:00.225335 test begin: paddle.einsum("ak, kn-> an", Tensor([15000, 11],"float32"), Tensor([0, 50],"float32"), )

W0303 17:26:04.605324 24477 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:26:04.606434 24477 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("ak, kn-> an", Tensor([15000, 11],"float32"), Tensor([0, 50],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:26:04.607755 test begin: paddle.einsum("ak, kn-> an", Tensor([15000, 11],"float32"), Tensor([11, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993964 (unix time) try "date -d @1740993964" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2198d91fc2) received by PID 23334 (TID 0x7f20d0949700) from PID 18446744071978950594 ***]

2025-03-03 17:26:17.123039 test begin: paddle.einsum("ak, kn-> an", Tensor([60000, 0],"float32"), Tensor([11, 0],"float32"), )

W0303 17:26:20.002622 26584 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:26:20.003720 26584 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993980 (unix time) try "date -d @1740993980" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0ffcad7fc2) received by PID 26231 (TID 0x7f0f00949700) from PID 18446744073653813186 ***]

2025-03-03 17:26:24.687016 test begin: paddle.einsum("ak, kn-> an", Tensor([60000, 0],"float32"), Tensor([11, 50],"float32"), )

W0303 17:26:29.184803 27959 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:26:29.185768 27959 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740993989 (unix time) try "date -d @1740993989" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f31344b3fc2) received by PID 27070 (TID 0x7f3027f48700) from PID 877346754 ***]

2025-03-03 17:26:40.235997 test begin: paddle.einsum("ak, kn-> an", Tensor([60000, 11],"float32"), Tensor([0, 50],"float32"), )

W0303 17:26:44.432207 30142 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:26:44.434724 30142 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("ak, kn-> an", Tensor([60000, 11],"float32"), Tensor([0, 50],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:26:44.436850 test begin: paddle.einsum("ak, kn-> an", Tensor([60000, 11],"float32"), Tensor([11, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994004 (unix time) try "date -d @1740994004" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f770eb17fc2) received by PID 29427 (TID 0x7f76367c3700) from PID 246513602 ***]

2025-03-03 17:26:49.128878 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([0, 8, 1, 64],"float32"), Tensor([0, 8, 1, 64],"float32"), )

W0303 17:26:52.217451 31305 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:26:52.218621 31305 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994012 (unix time) try "date -d @1740994012" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f33f33b3fc2) received by PID 30584 (TID 0x7f32e12b7700) from PID 18446744073495330754 ***]

2025-03-03 17:27:03.523225 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([0, 8, 1, 64],"float32"), Tensor([0, 8, 109, 64],"float32"), )

W0303 17:27:07.228825 33809 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:27:07.230396 33809 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994027 (unix time) try "date -d @1740994027" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6f6dc24fc2) received by PID 33080 (TID 0x7f6e83b85700) from PID 1841450946 ***]

2025-03-03 17:27:11.711111 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([0, 8, 1, 64],"float32"), Tensor([1, 8, 1, 64],"float32"), )

W0303 17:27:15.908774 35029 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:27:15.909893 35029 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994035 (unix time) try "date -d @1740994035" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f97bb3edfc2) received by PID 34284 (TID 0x7f96e92b7700) from PID 18446744072556044226 ***]

2025-03-03 17:27:27.612401 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([0, 8, 1, 64],"float32"), Tensor([1, 8, 109, 64],"float32"), )

W0303 17:27:30.378289 37434 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:27:30.379370 37434 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994050 (unix time) try "date -d @1740994050" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2d56f4efc2) received by PID 36449 (TID 0x7f2c7a7c3700) from PID 1458892738 ***]

2025-03-03 17:27:34.746395 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 0, 1, 64],"float32"), Tensor([1, 0, 1, 64],"float32"), )

W0303 17:27:37.856606 38511 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:27:37.858367 38511 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994057 (unix time) try "date -d @1740994057" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd521b95fc2) received by PID 37795 (TID 0x7fd447b85700) from PID 565796802 ***]

2025-03-03 17:27:49.404052 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 0, 1, 64],"float32"), Tensor([1, 0, 109, 64],"float32"), )

W0303 17:27:52.318143 40389 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:27:52.319218 40389 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994072 (unix time) try "date -d @1740994072" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc389b5cfc2) received by PID 39858 (TID 0x7fc27bb85700) from PID 18446744071724978114 ***]

2025-03-03 17:27:57.021533 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 0, 1, 64],"float32"), Tensor([1, 8, 1, 64],"float32"), )

W0303 17:27:59.799731 42011 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:27:59.800869 42011 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994079 (unix time) try "date -d @1740994079" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9b363b1fc2) received by PID 41070 (TID 0x7f9a13dc2700) from PID 909844418 ***]

2025-03-03 17:28:11.337761 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 0, 1, 64],"float32"), Tensor([1, 8, 109, 64],"float32"), )

W0303 17:28:14.356441 44363 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:28:14.357517 44363 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994094 (unix time) try "date -d @1740994094" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3783d24fc2) received by PID 43245 (TID 0x7f3695d0b700) from PID 18446744071626182594 ***]

2025-03-03 17:28:19.246541 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 0, 64],"float32"), Tensor([1, 8, 0, 64],"float32"), )

W0303 17:28:22.066846 45572 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:28:22.068138 45572 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994102 (unix time) try "date -d @1740994102" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ffa527c6fc2) received by PID 44913 (TID 0x7ff9487c3700) from PID 1383886786 ***]

2025-03-03 17:28:33.985156 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 0, 64],"float32"), Tensor([1, 8, 1, 64],"float32"), )

W0303 17:28:36.809291 47937 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:28:36.810245 47937 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994116 (unix time) try "date -d @1740994116" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8c317c0fc2) received by PID 47276 (TID 0x7f8b576f8700) from PID 830214082 ***]

2025-03-03 17:28:41.224115 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 0, 64],"float32"), Tensor([1, 8, 109, 64],"float32"), )

W0303 17:28:44.064363 48988 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:28:44.066016 48988 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994124 (unix time) try "date -d @1740994124" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3f31527fc2) received by PID 48411 (TID 0x7f3e5934a700) from PID 827490242 ***]

2025-03-03 17:28:56.168097 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), )

W0303 17:28:59.453538 51934 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:28:59.455487 51934 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994139 (unix time) try "date -d @1740994139" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4625c04fc2) received by PID 50321 (TID 0x7f44ffb85700) from PID 633360322 ***]

2025-03-03 17:29:03.902689 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 64],"float32"), )

W0303 17:29:06.500419 53691 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:29:06.501721 53691 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994146 (unix time) try "date -d @1740994146" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f77ead0efc2) received by PID 53051 (TID 0x7f76e67c3700) from PID 18446744073354145730 ***]

2025-03-03 17:29:10.760387 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 109, 0],"float32"), )

W0303 17:29:13.659623 54599 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:29:13.660683 54599 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994153 (unix time) try "date -d @1740994153" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f059b74efc2) received by PID 54022 (TID 0x7f04c7744700) from PID 18446744072022716354 ***]

2025-03-03 17:29:18.149824 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 109, 64],"float32"), )

W0303 17:29:21.122459 55615 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:29:21.123366 55615 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994161 (unix time) try "date -d @1740994161" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb76d26efc2) received by PID 55168 (TID 0x7fb69b34a700) from PID 1831268290 ***]

2025-03-03 17:29:25.630678 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([0, 8, 1, 64],"float32"), )

W0303 17:29:28.359119 56826 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:29:28.360137 56826 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([0, 8, 1, 64],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:29:28.362326 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([0, 8, 109, 64],"float32"), )

[paddle error] paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([0, 8, 109, 64],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:29:28.366654 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 0, 1, 64],"float32"), )

[paddle error] paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 0, 1, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:29:28.370908 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 0, 109, 64],"float32"), )

[paddle error] paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 0, 109, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:29:28.373270 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 8, 0, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994168 (unix time) try "date -d @1740994168" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6c55f77fc2) received by PID 56083 (TID 0x7f6b83dc2700) from PID 1442283458 ***]

2025-03-03 17:29:32.815714 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 8, 1, 0],"float32"), )

W0303 17:29:37.038278 58217 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:29:37.039599 58217 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 8, 1, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:64 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:29:37.040787 test begin: paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 8, 109, 0],"float32"), )

[paddle error] paddle.einsum("b h i d, b h j d -> b h i j", Tensor([1, 8, 1, 64],"float32"), Tensor([1, 8, 109, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:64 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:29:37.042984 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([0, 8, 1, 109],"float32"), Tensor([0, 8, 109, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994177 (unix time) try "date -d @1740994177" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f54913f7fc2) received by PID 57521 (TID 0x7f53a334a700) from PID 18446744071851442114 ***]

2025-03-03 17:29:41.582374 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([0, 8, 1, 109],"float32"), Tensor([1, 8, 109, 64],"float32"), )

W0303 17:29:46.077749 59389 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:29:46.079331 59389 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994186 (unix time) try "date -d @1740994186" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f31ea19bfc2) received by PID 58824 (TID 0x7f30fddc2700) from PID 18446744073342140354 ***]

2025-03-03 17:29:57.450689 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([0, 8, 1, 113],"float32"), Tensor([0, 8, 113, 64],"float32"), )

W0303 17:30:00.770169 61751 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:30:00.771112 61751 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994200 (unix time) try "date -d @1740994200" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f81f08b9fc2) received by PID 60861 (TID 0x7f80f2949700) from PID 18446744073450266562 ***]

2025-03-03 17:30:05.612466 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([0, 8, 1, 113],"float32"), Tensor([1, 8, 113, 64],"float32"), )

W0303 17:30:09.123927 62835 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:30:09.124909 62835 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994209 (unix time) try "date -d @1740994209" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f566f6dbfc2) received by PID 62303 (TID 0x7f5593744700) from PID 1869463490 ***]

2025-03-03 17:30:20.839035 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 0, 1, 109],"float32"), Tensor([1, 0, 109, 64],"float32"), )

W0303 17:30:25.926136 65160 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:30:25.927028 65160 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994225 (unix time) try "date -d @1740994225" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8a86ae8fc2) received by PID 64588 (TID 0x7f898c7c3700) from PID 18446744071674171330 ***]

2025-03-03 17:30:30.676697 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 0, 1, 109],"float32"), Tensor([1, 8, 109, 64],"float32"), )

W0303 17:30:34.077242 66907 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:30:34.078831 66907 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994234 (unix time) try "date -d @1740994234" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f147b3effc2) received by PID 65957 (TID 0x7f139d2b7700) from PID 2067726274 ***]

2025-03-03 17:30:45.569734 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 0, 1, 113],"float32"), Tensor([1, 0, 113, 64],"float32"), )

W0303 17:30:48.603736 68897 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:30:48.604763 68897 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994248 (unix time) try "date -d @1740994248" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fcad3238fc2) received by PID 68093 (TID 0x7fc9e12b7700) from PID 18446744072956907458 ***]

2025-03-03 17:30:52.945692 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 0, 1, 113],"float32"), Tensor([1, 8, 113, 64],"float32"), )

W0303 17:30:56.214411 70256 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:30:56.215528 70256 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994256 (unix time) try "date -d @1740994256" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa3c0421fc2) received by PID 69310 (TID 0x7fa2c5f48700) from PID 18446744072640143298 ***]

2025-03-03 17:31:07.611205 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 0, 109],"float32"), Tensor([1, 8, 0, 64],"float32"), )

W0303 17:31:10.417779 72280 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:31:10.418821 72280 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 0, 109],"float32"), Tensor([1, 8, 0, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:109 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:31:10.419819 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 0, 109],"float32"), Tensor([1, 8, 109, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994270 (unix time) try "date -d @1740994270" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9bfa989fc2) received by PID 71653 (TID 0x7f9b267c3700) from PID 18446744073618890690 ***]

2025-03-03 17:31:15.014999 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 0, 113],"float32"), Tensor([1, 8, 0, 64],"float32"), )

W0303 17:31:17.792438 73258 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:31:17.793402 73258 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 0, 113],"float32"), Tensor([1, 8, 0, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:113 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:31:17.794294 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 0, 113],"float32"), Tensor([1, 8, 113, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994277 (unix time) try "date -d @1740994277" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7feebd0fbfc2) received by PID 72693 (TID 0x7fedcc949700) from PID 18446744072586510274 ***]

2025-03-03 17:31:29.730938 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 109, 0],"float32"), )

W0303 17:31:32.800787 76070 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:31:32.801896 76070 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994292 (unix time) try "date -d @1740994292" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdc4e80bfc2) received by PID 75198 (TID 0x7fdb5a7c3700) from PID 1317060546 ***]

2025-03-03 17:31:37.191853 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 109, 64],"float32"), )

W0303 17:31:40.051551 76999 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:31:40.052565 76999 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994300 (unix time) try "date -d @1740994300" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5f606dcfc2) received by PID 76419 (TID 0x7f5e8bf48700) from PID 1617809346 ***]

2025-03-03 17:31:51.583520 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 113, 0],"float32"), )

W0303 17:31:54.513166 79049 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:31:54.514142 79049 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994314 (unix time) try "date -d @1740994314" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc91bd19fc2) received by PID 78383 (TID 0x7fc843d0b700) from PID 466722754 ***]

2025-03-03 17:31:58.987784 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 113, 64],"float32"), )

W0303 17:32:02.094797 80589 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:32:02.096136 80589 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994322 (unix time) try "date -d @1740994322" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2bd1598fc2) received by PID 79924 (TID 0x7f2ac934a700) from PID 18446744072926891970 ***]

2025-03-03 17:32:13.287769 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([0, 8, 109, 64],"float32"), )

W0303 17:32:16.177343 82381 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:32:16.178427 82381 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([0, 8, 109, 64],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:32:16.179768 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([1, 0, 109, 64],"float32"), )

[paddle error] paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([1, 0, 109, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:32:16.183130 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([1, 8, 0, 64],"float32"), )

[paddle error] paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([1, 8, 0, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:109 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:32:16.185998 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 109],"float32"), Tensor([1, 8, 109, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994336 (unix time) try "date -d @1740994336" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4e45b15fc2) received by PID 82029 (TID 0x7f4d43b85700) from PID 1169252290 ***]

2025-03-03 17:32:20.960300 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([0, 8, 113, 64],"float32"), )

W0303 17:32:24.322376 83435 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:32:24.323511 83435 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([0, 8, 113, 64],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:32:24.329537 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([1, 0, 113, 64],"float32"), )

[paddle error] paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([1, 0, 113, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:32:24.338645 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([1, 8, 0, 64],"float32"), )

[paddle error] paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([1, 8, 0, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:113 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:32:24.344234 test begin: paddle.einsum("b h i j, b h j d -> b h i d", Tensor([1, 8, 1, 113],"float32"), Tensor([1, 8, 113, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994344 (unix time) try "date -d @1740994344" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0de6e18fc2) received by PID 82920 (TID 0x7f0cc07c3700) from PID 18446744073288126402 ***]

2025-03-03 17:32:35.839147 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([0, 4, 5, 1, 8],"float32"), Tensor([0, 4, 5, 7, 8],"float32"), )

W0303 17:32:38.820866 85975 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:32:38.821794 85975 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994358 (unix time) try "date -d @1740994358" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb6d8573fc2) received by PID 85416 (TID 0x7fb5d3f48700) from PID 18446744073044180930 ***]

2025-03-03 17:32:43.311087 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([0, 4, 5, 1, 8],"float32"), Tensor([13, 4, 5, 7, 8],"float32"), )

W0303 17:32:46.138247 87027 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:32:46.139261 87027 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994366 (unix time) try "date -d @1740994366" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6914b35fc2) received by PID 86474 (TID 0x7f683c949700) from PID 347299778 ***]

2025-03-03 17:32:58.136010 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([0, 4, 5, 1, 8],"float32"), Tensor([52, 4, 5, 7, 8],"float32"), )

W0303 17:33:00.851768 89425 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:00.852710 89425 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994380 (unix time) try "date -d @1740994380" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0076f19fc2) received by PID 88570 (TID 0x7eff987c3700) from PID 1995546562 ***]

2025-03-03 17:33:05.195270 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 0, 5, 1, 8],"float32"), Tensor([13, 0, 5, 7, 8],"float32"), )

W0303 17:33:07.903318 90251 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:07.904285 90251 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994387 (unix time) try "date -d @1740994387" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1c0b120fc2) received by PID 89762 (TID 0x7f1b291f7700) from PID 185733058 ***]

2025-03-03 17:33:12.045700 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 0, 5, 1, 8],"float32"), Tensor([13, 4, 5, 7, 8],"float32"), )

W0303 17:33:15.006511 91188 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:15.007601 91188 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994395 (unix time) try "date -d @1740994395" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1de2341fc2) received by PID 90703 (TID 0x7f1cc7dc2700) from PID 18446744073209651138 ***]

2025-03-03 17:33:19.599831 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 0, 1, 8],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), )

W0303 17:33:22.332124 92157 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:22.333158 92157 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994402 (unix time) try "date -d @1740994402" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f990e380fc2) received by PID 91719 (TID 0x7f9807dc2700) from PID 238555074 ***]

2025-03-03 17:33:26.611010 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 0, 1, 8],"float32"), Tensor([13, 4, 5, 7, 8],"float32"), )

W0303 17:33:29.360399 93528 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:29.361433 93528 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994409 (unix time) try "date -d @1740994409" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8949b1afc2) received by PID 92964 (TID 0x7f8885b85700) from PID 1236381634 ***]

2025-03-03 17:33:33.860321 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 0, 8],"float32"), Tensor([13, 4, 5, 0, 8],"float32"), )

W0303 17:33:36.944643 94691 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:36.946003 94691 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994416 (unix time) try "date -d @1740994416" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f637beccfc2) received by PID 94306 (TID 0x7f62a1f48700) from PID 2079117250 ***]

2025-03-03 17:33:41.549539 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 0, 8],"float32"), Tensor([13, 4, 5, 7, 8],"float32"), )

W0303 17:33:44.113472 95808 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:44.114557 95808 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994424 (unix time) try "date -d @1740994424" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5b9ea00fc2) received by PID 95238 (TID 0x7f5aba7c3700) from PID 18446744072075874242 ***]

2025-03-03 17:33:48.528497 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 0],"float32"), Tensor([13, 4, 5, 7, 0],"float32"), )

W0303 17:33:51.707679 96674 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:51.708986 96674 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994431 (unix time) try "date -d @1740994431" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f78c01fefc2) received by PID 96292 (TID 0x7f77b7f48700) from PID 18446744072637902786 ***]

2025-03-03 17:33:56.222699 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 0],"float32"), Tensor([13, 4, 5, 7, 8],"float32"), )

W0303 17:33:59.726974 97934 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:33:59.728207 97934 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994439 (unix time) try "date -d @1740994439" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa522fa1fc2) received by PID 97209 (TID 0x7fa43c7c3700) from PID 586817474 ***]

2025-03-03 17:34:04.078930 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([0, 4, 5, 7, 8],"float32"), )

W0303 17:34:10.151868 99547 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:34:10.152830 99547 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([0, 4, 5, 7, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:34:10.154256 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([13, 0, 5, 7, 8],"float32"), )

[paddle error] paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([13, 0, 5, 7, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:34:10.157478 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), )

[paddle error] paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `l`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:34:10.159426 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([13, 4, 5, 0, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994450 (unix time) try "date -d @1740994450" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f877760cfc2) received by PID 98666 (TID 0x7f867d4f4700) from PID 2002833346 ***]

2025-03-03 17:34:14.512847 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([13, 4, 5, 7, 0],"float32"), )

W0303 17:34:18.015892 100660 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:34:18.016875 100660 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([13, 4, 5, 1, 8],"float32"), Tensor([13, 4, 5, 7, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:34:18.024427 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 0, 5, 1, 8],"float32"), Tensor([52, 0, 5, 7, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994458 (unix time) try "date -d @1740994458" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc5acdedfc2) received by PID 100044 (TID 0x7fc4d4949700) from PID 18446744072314871746 ***]

2025-03-03 17:34:24.067633 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 0, 5, 1, 8],"float32"), Tensor([52, 4, 5, 7, 8],"float32"), )

W0303 17:34:27.741214 101981 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:34:27.742415 101981 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994467 (unix time) try "date -d @1740994467" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc5f6af4fc2) received by PID 101145 (TID 0x7fc4f07c3700) from PID 18446744073553268674 ***]

2025-03-03 17:34:32.253595 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 0, 1, 8],"float32"), Tensor([52, 4, 0, 7, 8],"float32"), )

W0303 17:34:35.509629 103270 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:34:35.510731 103270 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994475 (unix time) try "date -d @1740994475" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f180b828fc2) received by PID 102547 (TID 0x7f171187e700) from PID 193105858 ***]

2025-03-03 17:34:47.254944 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 0, 1, 8],"float32"), Tensor([52, 4, 5, 7, 8],"float32"), )

W0303 17:34:52.746753 105231 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:34:52.748075 105231 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994492 (unix time) try "date -d @1740994492" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff822f9dfc2) received by PID 104588 (TID 0x7ff7307c3700) from PID 586801090 ***]

2025-03-03 17:34:57.331054 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 0, 8],"float32"), Tensor([52, 4, 5, 0, 8],"float32"), )

W0303 17:35:00.756737 106844 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:35:00.757730 106844 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994500 (unix time) try "date -d @1740994500" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd151f11fc2) received by PID 105966 (TID 0x7fd067dc2700) from PID 1374756802 ***]

2025-03-03 17:35:12.603615 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 0, 8],"float32"), Tensor([52, 4, 5, 7, 8],"float32"), )

W0303 17:35:15.985764 108805 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:35:15.987010 108805 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994515 (unix time) try "date -d @1740994515" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fccfe4a6fc2) received by PID 108154 (TID 0x7fcbfddc2700) from PID 18446744073680875458 ***]

2025-03-03 17:35:20.464610 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 0],"float32"), Tensor([52, 4, 5, 7, 0],"float32"), )

W0303 17:35:23.962711 109794 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:35:23.963768 109794 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994523 (unix time) try "date -d @1740994523" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fca78532fc2) received by PID 109116 (TID 0x7fc969f48700) from PID 2018717634 ***]

2025-03-03 17:35:36.114805 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 0],"float32"), Tensor([52, 4, 5, 7, 8],"float32"), )

W0303 17:35:39.438222 112569 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:35:39.439143 112569 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994539 (unix time) try "date -d @1740994539" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fea1b0f0fc2) received by PID 111967 (TID 0x7fe94b1b7700) from PID 453971906 ***]

2025-03-03 17:35:43.982392 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([0, 4, 5, 7, 8],"float32"), )

W0303 17:35:47.018488 113414 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:35:47.019546 113414 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([0, 4, 5, 7, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:52 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:35:47.021475 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([52, 0, 5, 7, 8],"float32"), )

[paddle error] paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([52, 0, 5, 7, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:35:47.032168 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([52, 4, 0, 7, 8],"float32"), )

[paddle error] paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([52, 4, 0, 7, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `l`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:35:47.044084 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([52, 4, 5, 0, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994547 (unix time) try "date -d @1740994547" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5f1d12dfc2) received by PID 112836 (TID 0x7f5e32949700) from PID 487776194 ***]

2025-03-03 17:35:59.336272 test begin: paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([52, 4, 5, 7, 0],"float32"), )

W0303 17:36:02.284477 115784 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:36:02.285418 115784 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqd,bhlkd->bhlqk", Tensor([52, 4, 5, 1, 8],"float32"), Tensor([52, 4, 5, 7, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:8 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:36:02.287227 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([0, 4, 3, 1, 1],"float32"), Tensor([13, 4, 1, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994562 (unix time) try "date -d @1740994562" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8cbf91cfc2) received by PID 114919 (TID 0x7f8be587e700) from PID 18446744072628588482 ***]

2025-03-03 17:36:06.952792 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([0, 4, 3, 1, 1],"float32"), Tensor([52, 4, 1, 8],"float32"), )

W0303 17:36:09.915479 116620 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:36:09.916591 116620 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994569 (unix time) try "date -d @1740994569" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff66fda3fc2) received by PID 116131 (TID 0x7ff591d0b700) from PID 1876574146 ***]

2025-03-03 17:36:14.489912 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 0, 3, 1, 1],"float32"), Tensor([13, 4, 1, 8],"float32"), )

W0303 17:36:17.646486 117564 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:36:17.647370 117564 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994577 (unix time) try "date -d @1740994577" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efefbb81fc2) received by PID 117015 (TID 0x7efe0dabb700) from PID 18446744073637732290 ***]

2025-03-03 17:36:21.959642 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 0, 1, 1],"float32"), Tensor([13, 4, 1, 8],"float32"), )

W0303 17:36:26.518888 118628 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:36:26.521196 118628 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994586 (unix time) try "date -d @1740994586" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fde26726fc2) received by PID 118208 (TID 0x7fdd4c7c3700) from PID 645033922 ***]

2025-03-03 17:36:30.877616 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 0, 1],"float32"), Tensor([13, 4, 1, 8],"float32"), )

W0303 17:36:34.255303 120014 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:36:34.257150 120014 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994594 (unix time) try "date -d @1740994594" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f77eb14afc2) received by PID 119672 (TID 0x7f770b237700) from PID 18446744073358585794 ***]

2025-03-03 17:36:38.885539 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 0],"float32"), Tensor([13, 4, 1, 8],"float32"), )

W0303 17:36:41.675099 121625 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:36:41.676255 121625 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994601 (unix time) try "date -d @1740994601" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f16ce80cfc2) received by PID 120642 (TID 0x7f15dc7c3700) from PID 18446744072879132610 ***]

2025-03-03 17:36:46.125355 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([0, 4, 1, 8],"float32"), )

W0303 17:36:49.496522 122384 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:36:49.497622 122384 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([0, 4, 1, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:36:49.498839 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([13, 0, 1, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([13, 0, 1, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:36:49.501700 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([13, 4, 0, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([13, 4, 0, 8],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:36:49.504433 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([13, 4, 3, 1, 1],"float32"), Tensor([13, 4, 1, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994609 (unix time) try "date -d @1740994609" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f42ab4e5fc2) received by PID 122022 (TID 0x7f41c94f4700) from PID 18446744072288624578 ***]

2025-03-03 17:37:01.489754 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 0, 3, 1, 1],"float32"), Tensor([52, 4, 1, 8],"float32"), )

W0303 17:37:04.188323 124860 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:37:04.189347 124860 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994624 (unix time) try "date -d @1740994624" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa25c754fc2) received by PID 124357 (TID 0x7fa17ff48700) from PID 1551192002 ***]

2025-03-03 17:37:08.457505 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 0, 1, 1],"float32"), Tensor([52, 4, 1, 8],"float32"), )

W0303 17:37:11.666229 125824 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:37:11.667197 125824 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994631 (unix time) try "date -d @1740994631" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f843aac2fc2) received by PID 125112 (TID 0x7f83567c3700) from PID 984362946 ***]

2025-03-03 17:37:23.726535 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 0, 1],"float32"), Tensor([52, 4, 1, 8],"float32"), )

W0303 17:37:27.233327 127789 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:37:27.234926 127789 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994647 (unix time) try "date -d @1740994647" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efbc667bfc2) received by PID 127159 (TID 0x7efae9dc2700) from PID 18446744072743272386 ***]

2025-03-03 17:37:31.602883 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 0],"float32"), Tensor([52, 4, 1, 8],"float32"), )

W0303 17:37:34.703238 129157 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:37:34.704764 129157 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994654 (unix time) try "date -d @1740994654" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe0b5748fc2) received by PID 128788 (TID 0x7fdfc56f8700) from PID 18446744072458899394 ***]

2025-03-03 17:37:39.414542 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([0, 4, 1, 8],"float32"), )

W0303 17:37:43.257052 130209 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:37:43.258119 130209 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([0, 4, 1, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:52 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:37:43.259470 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([52, 0, 1, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([52, 0, 1, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:37:43.262235 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([52, 4, 0, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([52, 4, 0, 8],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:37:43.264564 test begin: paddle.einsum("bhlqk,bhkd->bhlqd", Tensor([52, 4, 3, 1, 1],"float32"), Tensor([52, 4, 1, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994663 (unix time) try "date -d @1740994663" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7f6555dfc2) received by PID 129705 (TID 0x7f7ea534a700) from PID 1700126658 ***]

2025-03-03 17:37:47.985020 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([0, 4, 1, 1, 7],"float32"), Tensor([0, 4, 1, 7, 8],"float32"), )

W0303 17:37:53.118741 131364 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:37:53.120092 131364 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994673 (unix time) try "date -d @1740994673" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f74c8bb8fc2) received by PID 130722 (TID 0x7f73ec949700) from PID 18446744072782319554 ***]

2025-03-03 17:38:04.982709 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([0, 4, 1, 1, 7],"float32"), Tensor([13, 4, 1, 7, 8],"float32"), )

W0303 17:38:09.043061 134163 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:38:09.044220 134163 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994689 (unix time) try "date -d @1740994689" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f025336cfc2) received by PID 133535 (TID 0x7f01672b7700) from PID 1396101058 ***]

2025-03-03 17:38:13.216459 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([0, 4, 3, 1, 3],"float32"), Tensor([0, 4, 3, 3, 8],"float32"), )

W0303 17:38:48.233484 135206 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:38:48.234467 135206 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994728 (unix time) try "date -d @1740994728" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fbfadadafc2) received by PID 134730 (TID 0x7fbedfb85700) from PID 18446744072328425410 ***]

2025-03-03 17:38:52.894361 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([0, 4, 3, 1, 3],"float32"), Tensor([13, 4, 3, 3, 8],"float32"), )

W0303 17:38:56.026000 137304 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:38:56.026892 137304 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994736 (unix time) try "date -d @1740994736" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa5f1e6bfc2) received by PID 136428 (TID 0x7fa4d9dc2700) from PID 18446744073473015746 ***]

2025-03-03 17:39:00.182700 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 0, 1, 1, 7],"float32"), Tensor([13, 0, 1, 7, 8],"float32"), )

W0303 17:39:34.593129 138293 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:39:34.594635 138293 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994774 (unix time) try "date -d @1740994774" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7441992fc2) received by PID 137725 (TID 0x7f7351935700) from PID 1100558274 ***]

2025-03-03 17:39:38.921260 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 0, 1, 1, 7],"float32"), Tensor([13, 4, 1, 7, 8],"float32"), )

W0303 17:39:42.403800 140075 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:39:42.404894 140075 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994782 (unix time) try "date -d @1740994782" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd6fe428fc2) received by PID 139564 (TID 0x7fd629dc2700) from PID 18446744073680359362 ***]

2025-03-03 17:39:47.530341 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 0, 3, 1, 3],"float32"), Tensor([13, 0, 3, 3, 8],"float32"), )

W0303 17:39:50.865182 141005 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:39:50.866209 141005 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994790 (unix time) try "date -d @1740994790" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb7b68f9fc2) received by PID 140353 (TID 0x7fb6bc7c3700) from PID 18446744072477450178 ***]

2025-03-03 17:40:26.664587 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 0, 3, 1, 3],"float32"), Tensor([13, 4, 3, 3, 8],"float32"), )

W0303 17:40:29.565934 143012 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:40:29.567435 143012 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994829 (unix time) try "date -d @1740994829" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8f97990fc2) received by PID 142239 (TID 0x7f8e9b87e700) from PID 18446744071957974978 ***]

2025-03-03 17:40:33.971306 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 0, 1, 3],"float32"), Tensor([13, 4, 0, 3, 8],"float32"), )

W0303 17:40:37.714395 144420 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:40:37.715534 144420 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994837 (unix time) try "date -d @1740994837" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f27a52c2fc2) received by PID 143786 (TID 0x7f26ab34a700) from PID 18446744072185720770 ***]

2025-03-03 17:40:43.848293 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 0, 1, 3],"float32"), Tensor([13, 4, 3, 3, 8],"float32"), )

W0303 17:40:46.693331 145601 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:40:46.694525 145601 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994846 (unix time) try "date -d @1740994846" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5aca6b3fc2) received by PID 144874 (TID 0x7f59c5dc2700) from PID 18446744072810610626 ***]

2025-03-03 17:40:51.397183 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 0, 1, 7],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), )

W0303 17:40:57.627326 146412 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:40:57.628427 146412 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994857 (unix time) try "date -d @1740994857" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f335da3dfc2) received by PID 145867 (TID 0x7f3245935700) from PID 1571020738 ***]

2025-03-03 17:41:02.316245 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 0, 1, 7],"float32"), Tensor([13, 4, 1, 7, 8],"float32"), )

W0303 17:41:37.216598 148123 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:41:37.217917 148123 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994897 (unix time) try "date -d @1740994897" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7062f31fc2) received by PID 147564 (TID 0x7f6f827c3700) from PID 1660100546 ***]

2025-03-03 17:41:41.564021 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 0, 7],"float32"), Tensor([13, 4, 1, 0, 8],"float32"), )

W0303 17:41:44.350791 149935 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:41:44.351923 149935 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 0, 7],"float32"), Tensor([13, 4, 1, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:7 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:41:44.353043 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 0, 7],"float32"), Tensor([13, 4, 1, 7, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994904 (unix time) try "date -d @1740994904" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc09b832fc2) received by PID 149446 (TID 0x7fbfa787e700) from PID 18446744072023650242 ***]

2025-03-03 17:41:49.004510 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 0],"float32"), Tensor([13, 4, 1, 7, 0],"float32"), )

W0303 17:41:51.773277 150794 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:41:51.774274 150794 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994911 (unix time) try "date -d @1740994911" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efd5b903fc2) received by PID 150129 (TID 0x7efc4d87e700) from PID 1536180162 ***]

2025-03-03 17:41:57.321854 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 0],"float32"), Tensor([13, 4, 1, 7, 8],"float32"), )

W0303 17:42:01.794100 152387 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:42:01.795285 152387 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994921 (unix time) try "date -d @1740994921" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f973588bfc2) received by PID 151268 (TID 0x7f964d935700) from PID 898154434 ***]

2025-03-03 17:42:06.351811 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([0, 4, 1, 7, 8],"float32"), )

W0303 17:42:09.347088 153596 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:42:09.348140 153596 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([0, 4, 1, 7, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:42:09.349608 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 0, 1, 7, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 0, 1, 7, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:42:09.356855 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 4, 0, 7, 8],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:42:09.391542 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 4, 1, 0, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 4, 1, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:7 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:42:09.394978 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 1, 1, 7],"float32"), Tensor([13, 4, 1, 7, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994929 (unix time) try "date -d @1740994929" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f92b5a6afc2) received by PID 152958 (TID 0x7f91a3935700) from PID 18446744072462184386 ***]

2025-03-03 17:42:13.890141 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 0, 3],"float32"), Tensor([13, 4, 3, 0, 8],"float32"), )

W0303 17:42:16.859200 154334 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:42:16.860443 154334 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 0, 3],"float32"), Tensor([13, 4, 3, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:42:16.861175 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 0, 3],"float32"), Tensor([13, 4, 3, 3, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994936 (unix time) try "date -d @1740994936" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff0b5afdfc2) received by PID 153756 (TID 0x7fefcdb85700) from PID 18446744072462786498 ***]

2025-03-03 17:42:21.314787 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 0],"float32"), Tensor([13, 4, 3, 3, 0],"float32"), )

W0303 17:42:24.975960 155432 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:42:24.976943 155432 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994944 (unix time) try "date -d @1740994944" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f15bdfd7fc2) received by PID 154876 (TID 0x7f14b9dc2700) from PID 18446744072602091458 ***]

2025-03-03 17:42:29.461913 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 0],"float32"), Tensor([13, 4, 3, 3, 8],"float32"), )

W0303 17:42:32.402837 156752 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:42:32.403750 156752 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994952 (unix time) try "date -d @1740994952" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1cb4814fc2) received by PID 155940 (TID 0x7f1bf1f48700) from PID 18446744072442957762 ***]

2025-03-03 17:42:36.880716 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([0, 4, 3, 3, 8],"float32"), )

W0303 17:42:40.679232 157882 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:42:40.680269 157882 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([0, 4, 3, 3, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:42:40.681949 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([13, 0, 3, 3, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([13, 0, 3, 3, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:42:40.684944 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([13, 4, 0, 3, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([13, 4, 0, 3, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `l`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:42:40.686804 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([13, 4, 3, 0, 8],"float32"), )

[paddle error] paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([13, 4, 3, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `k`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:42:40.688396 test begin: paddle.einsum("bhlqk,bhlkd->bhlqd", Tensor([13, 4, 3, 1, 3],"float32"), Tensor([13, 4, 3, 3, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994960 (unix time) try "date -d @1740994960" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff3b37defc2) received by PID 157164 (TID 0x7ff2e387e700) from PID 18446744072425959362 ***]

2025-03-03 17:42:45.003593 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([0, 14, 14, 64],"float32"), Tensor([14, 14, 64],"float32"), )

W0303 17:42:50.772861 158725 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:42:50.773773 158725 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994970 (unix time) try "date -d @1740994970" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd19e12afc2) received by PID 158283 (TID 0x7fd0a3dc2700) from PID 18446744072066609090 ***]

2025-03-03 17:42:57.265233 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([0, 32, 32, 64],"float32"), Tensor([32, 32, 64],"float32"), )

W0303 17:43:01.690518 160363 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:43:01.691470 160363 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994981 (unix time) try "date -d @1740994981" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe599e15fc2) received by PID 159218 (TID 0x7fe48fdc2700) from PID 18446744071996268482 ***]

2025-03-03 17:43:12.924754 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 0, 14, 64],"float32"), Tensor([14, 14, 64],"float32"), )

W0303 17:43:17.263131 162183 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:43:17.264384 162183 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740994997 (unix time) try "date -d @1740994997" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd5eb339fc2) received by PID 161703 (TID 0x7fd5112b7700) from PID 18446744073360613314 ***]

2025-03-03 17:43:21.704518 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 0, 64],"float32"), Tensor([14, 14, 64],"float32"), )

W0303 17:43:25.036240 163477 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:43:25.037443 163477 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995005 (unix time) try "date -d @1740995005" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdcd1b03fc2) received by PID 162909 (TID 0x7fdbd3b85700) from PID 18446744072932573122 ***]

2025-03-03 17:43:29.466211 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 14, 0],"float32"), Tensor([14, 14, 64],"float32"), )

W0303 17:43:33.325148  1395 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:43:33.326205  1395 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995013 (unix time) try "date -d @1740995013" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f93bc025fc2) received by PID 372 (TID 0x7f92e9f48700) from PID 18446744072568856514 ***]

2025-03-03 17:43:39.020710 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([0, 14, 64],"float32"), )

W0303 17:43:42.704918  2485 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:43:42.705827  2485 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([0, 14, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:14 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:43:42.717641 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([14, 0, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995022 (unix time) try "date -d @1740995022" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc1c8e99fc2) received by PID 1744 (TID 0x7fc0e8949700) from PID 18446744072785338306 ***]

2025-03-03 17:43:47.005353 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([14, 14, 0],"float32"), )

W0303 17:43:50.239109  3260 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:43:50.240196  3260 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhwc,hkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([14, 14, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `c`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:64 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:43:50.243138 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 0, 32, 64],"float32"), Tensor([32, 32, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995030 (unix time) try "date -d @1740995030" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f151b413fc2) received by PID 2986 (TID 0x7f14314f4700) from PID 457260994 ***]

2025-03-03 17:43:54.918745 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 0, 64],"float32"), Tensor([32, 32, 64],"float32"), )

W0303 17:43:59.206619  4612 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:43:59.208097  4612 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995039 (unix time) try "date -d @1740995039" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe47084cfc2) received by PID 3663 (TID 0x7fe36bf48700) from PID 1887752130 ***]

2025-03-03 17:44:03.730989 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 32, 0],"float32"), Tensor([32, 32, 64],"float32"), )

W0303 17:44:09.587112  5980 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:44:09.588181  5980 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995049 (unix time) try "date -d @1740995049" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff09d332fc2) received by PID 5341 (TID 0x7fefbb34a700) from PID 18446744072051961794 ***]

2025-03-03 17:44:22.378020 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([0, 32, 64],"float32"), )

W0303 17:44:25.514034  8127 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:44:25.515003  8127 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([0, 32, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:32 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:44:25.517152 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([32, 0, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995065 (unix time) try "date -d @1740995065" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f43f55e1fc2) received by PID 7399 (TID 0x7f42e334a700) from PID 18446744073531170754 ***]

2025-03-03 17:44:30.850794 test begin: paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([32, 32, 0],"float32"), )

W0303 17:44:33.998481  9705 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:44:33.999646  9705 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhwc,hkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([32, 32, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `c`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:64 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:44:34.001644 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([0, 14, 14, 64],"float32"), Tensor([14, 14, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995074 (unix time) try "date -d @1740995074" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6a90771fc2) received by PID 8816 (TID 0x7f69b5f48700) from PID 18446744071838310338 ***]

2025-03-03 17:44:38.387522 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([0, 32, 32, 64],"float32"), Tensor([32, 32, 64],"float32"), )

W0303 17:44:41.053079 10553 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:44:41.054098 10553 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995081 (unix time) try "date -d @1740995081" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4742402fc2) received by PID 10256 (TID 0x7f464ddc2700) from PID 1111502786 ***]

2025-03-03 17:45:15.257737 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 0, 14, 64],"float32"), Tensor([14, 14, 64],"float32"), )

W0303 17:45:18.935251 12455 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:45:18.936269 12455 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995118 (unix time) try "date -d @1740995118" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1478e23fc2) received by PID 10947 (TID 0x7f1386949700) from PID 2028093378 ***]

2025-03-03 17:45:24.026427 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 14, 0, 64],"float32"), Tensor([14, 14, 64],"float32"), )

W0303 17:45:26.669154 13841 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:45:26.670289 13841 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995126 (unix time) try "date -d @1740995126" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f52eb27cfc2) received by PID 12952 (TID 0x7f51df2b7700) from PID 18446744073359839170 ***]

2025-03-03 17:45:31.162287 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 14, 14, 0],"float32"), Tensor([14, 14, 64],"float32"), )

W0303 17:45:36.061457 14974 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:45:36.062574 14974 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995136 (unix time) try "date -d @1740995136" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc6df088fc2) received by PID 14352 (TID 0x7fc607177700) from PID 18446744073156464578 ***]

2025-03-03 17:45:48.672461 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([0, 14, 64],"float32"), )

W0303 17:46:25.790777 17429 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:46:25.791872 17429 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([0, 14, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `w`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:14 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:46:25.794861 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([14, 0, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995185 (unix time) try "date -d @1740995185" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fcdf5947fc2) received by PID 16784 (TID 0x7fccd1935700) from PID 18446744073534734274 ***]

2025-03-03 17:46:30.447567 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([14, 14, 0],"float32"), )

W0303 17:46:34.122488 19379 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:46:34.123421 19379 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhwc,wkc->bhwk", Tensor([432, 14, 14, 64],"float32"), Tensor([14, 14, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `c`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:64 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:46:34.126147 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 0, 32, 64],"float32"), Tensor([32, 32, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995194 (unix time) try "date -d @1740995194" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3f534c4fc2) received by PID 18781 (TID 0x7f3e574f4700) from PID 1397510082 ***]

2025-03-03 17:46:39.693130 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 32, 0, 64],"float32"), Tensor([32, 32, 64],"float32"), )

W0303 17:47:11.732162 20510 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:47:11.733060 20510 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995231 (unix time) try "date -d @1740995231" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9535384fc2) received by PID 19864 (TID 0x7f943334a700) from PID 892882882 ***]

2025-03-03 17:47:15.990416 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 32, 32, 0],"float32"), Tensor([32, 32, 64],"float32"), )

W0303 17:47:20.957227 21806 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:47:20.958897 21806 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995240 (unix time) try "date -d @1740995240" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f807b69dfc2) received by PID 21246 (TID 0x7f7fa5744700) from PID 2070536130 ***]

2025-03-03 17:47:26.109716 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([0, 32, 64],"float32"), )

W0303 17:48:01.300060 22996 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:48:01.300936 22996 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([0, 32, 64],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `w`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:32 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:48:01.303308 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([32, 0, 64],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995281 (unix time) try "date -d @1740995281" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5d74086fc2) received by PID 22057 (TID 0x7f5c63f48700) from PID 1946709954 ***]

2025-03-03 17:48:05.763537 test begin: paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([32, 32, 0],"float32"), )

W0303 17:48:10.544358 25189 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:48:10.545481 25189 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bhwc,wkc->bhwk", Tensor([48, 32, 32, 64],"float32"), Tensor([32, 32, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `c`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:64 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:48:10.547728 test begin: paddle.einsum("bij,bjk->bik", Tensor([0, 4, 5],"float64"), Tensor([0, 5, 2],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995290 (unix time) try "date -d @1740995290" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5278840c32) received by PID 24626 (TID 0x7f5189f48700) from PID 2021919794 ***]

2025-03-03 17:48:14.754408 test begin: paddle.einsum("bij,bjk->bik", Tensor([0, 4, 5],"float64"), Tensor([3, 5, 2],"float64"), )

W0303 17:48:17.609380 26486 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:48:17.611057 26486 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995297 (unix time) try "date -d @1740995297" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f50cd6d0c32) received by PID 25842 (TID 0x7f4ff96f8700) from PID 18446744072861060146 ***]

2025-03-03 17:48:59.951470 test begin: paddle.einsum("bij,bjk->bik", Tensor([3, 0, 5],"float64"), Tensor([3, 0, 2],"float64"), )

W0303 17:49:03.402201 29486 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:49:03.403859 29486 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bij,bjk->bik", Tensor([3, 0, 5],"float64"), Tensor([3, 0, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:49:03.404837 test begin: paddle.einsum("bij,bjk->bik", Tensor([3, 0, 5],"float64"), Tensor([3, 5, 2],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995343 (unix time) try "date -d @1740995343" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f443f8e1c32) received by PID 28024 (TID 0x7f435f87e700) from PID 1066277938 ***]

2025-03-03 17:49:08.006428 test begin: paddle.einsum("bij,bjk->bik", Tensor([3, 4, 0],"float64"), Tensor([3, 5, 0],"float64"), )

W0303 17:49:12.541180 30673 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:49:12.542879 30673 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995352 (unix time) try "date -d @1740995352" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ffb31016c32) received by PID 30046 (TID 0x7ffa48949700) from PID 822176818 ***]

2025-03-03 17:49:17.293037 test begin: paddle.einsum("bij,bjk->bik", Tensor([3, 4, 0],"float64"), Tensor([3, 5, 2],"float64"), )

W0303 17:49:20.177217 31614 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:49:20.178395 31614 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995360 (unix time) try "date -d @1740995360" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0031afdc32) received by PID 30899 (TID 0x7eff39b85700) from PID 833608754 ***]

2025-03-03 17:49:24.646500 test begin: paddle.einsum("bij,bjk->bik", Tensor([3, 4, 5],"float64"), Tensor([0, 5, 2],"float64"), )

W0303 17:49:27.363098 33137 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:49:27.364380 33137 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bij,bjk->bik", Tensor([3, 4, 5],"float64"), Tensor([0, 5, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:49:27.365736 test begin: paddle.einsum("bij,bjk->bik", Tensor([3, 4, 5],"float64"), Tensor([3, 0, 2],"float64"), )

[paddle error] paddle.einsum("bij,bjk->bik", Tensor([3, 4, 5],"float64"), Tensor([3, 0, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:49:27.371244 test begin: paddle.einsum("bij,bjk->bik", Tensor([3, 4, 5],"float64"), Tensor([3, 5, 0],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995367 (unix time) try "date -d @1740995367" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7e22a2dc32) received by PID 32276 (TID 0x7f7d3e7c3700) from PID 581098546 ***]

2025-03-03 17:49:31.778801 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([0, 2, 4, 4],"float32"), Tensor([0, 2, 4, 4],"float32"), )

W0303 17:49:35.896004 34052 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:49:35.897336 34052 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995375 (unix time) try "date -d @1740995375" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f793d010fc2) received by PID 33504 (TID 0x7f7854949700) from PID 1023479746 ***]

2025-03-03 17:49:40.342724 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([0, 2, 4, 4],"float32"), Tensor([0, 4, 4, 4],"float32"), )

W0303 17:49:43.603119 35201 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:49:43.604097 35201 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995383 (unix time) try "date -d @1740995383" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5ec7a23fc2) received by PID 34703 (TID 0x7f5debabb700) from PID 18446744072763883458 ***]

2025-03-03 17:49:47.922496 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([0, 2, 4, 4],"float32"), Tensor([13, 2, 4, 4],"float32"), )

W0303 17:49:51.267977 36033 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:49:51.270048 36033 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995391 (unix time) try "date -d @1740995391" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2bf2ca3fc2) received by PID 35546 (TID 0x7f2ae47c3700) from PID 18446744073487925186 ***]

2025-03-03 17:49:55.831044 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([0, 2, 4, 4],"float32"), Tensor([13, 4, 4, 4],"float32"), )

W0303 17:49:59.336964 37353 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:49:59.338037 37353 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995399 (unix time) try "date -d @1740995399" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f44c6b02fc2) received by PID 36579 (TID 0x7f43ca7c3700) from PID 18446744072748019650 ***]

2025-03-03 17:50:11.010007 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 0, 4, 4],"float32"), Tensor([13, 0, 4, 4],"float32"), )

W0303 17:50:14.145607 39607 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:50:14.146512 39607 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995414 (unix time) try "date -d @1740995414" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3ec92d6fc2) received by PID 38962 (TID 0x7f3de334a700) from PID 18446744072789782466 ***]

2025-03-03 17:50:18.939208 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 0, 4, 4],"float32"), Tensor([13, 2, 4, 4],"float32"), )

W0303 17:50:21.716710 40786 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:50:21.718202 40786 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995421 (unix time) try "date -d @1740995421" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd59cc44fc2) received by PID 39956 (TID 0x7fd496949700) from PID 18446744072044695490 ***]

2025-03-03 17:50:25.908627 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 0, 4, 4],"float32"), Tensor([13, 4, 4, 4],"float32"), )

W0303 17:50:29.005265 41823 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:50:29.006171 41823 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995429 (unix time) try "date -d @1740995429" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f35c67acfc2) received by PID 41035 (TID 0x7f34fc7c3700) from PID 18446744072744521666 ***]

2025-03-03 17:50:33.752318 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 0, 4],"float32"), Tensor([13, 2, 0, 4],"float32"), )

W0303 17:50:36.400823 42963 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:50:36.402096 42963 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995436 (unix time) try "date -d @1740995436" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1111687fc2) received by PID 42457 (TID 0x7f103b6f8700) from PID 292061122 ***]

2025-03-03 17:50:41.315927 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 0, 4],"float32"), Tensor([13, 2, 4, 4],"float32"), )

W0303 17:50:44.525081 44013 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:50:44.526520 44013 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995444 (unix time) try "date -d @1740995444" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3c7ebf7fc2) received by PID 43444 (TID 0x7f3b8c7c3700) from PID 2126479298 ***]

2025-03-03 17:50:49.011804 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 0, 4],"float32"), Tensor([13, 4, 0, 4],"float32"), )

W0303 17:50:53.530081 44844 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:50:53.531103 44844 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995453 (unix time) try "date -d @1740995453" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3704cbefc2) received by PID 44288 (TID 0x7f3608949700) from PID 80474050 ***]

2025-03-03 17:50:59.320885 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 0, 4],"float32"), Tensor([13, 4, 4, 4],"float32"), )

W0303 17:51:02.255211 46674 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:51:02.256187 46674 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995462 (unix time) try "date -d @1740995462" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2ee4b69fc2) received by PID 45572 (TID 0x7f2de4949700) from PID 18446744073251758018 ***]

2025-03-03 17:51:13.604770 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 0],"float32"), Tensor([13, 2, 4, 0],"float32"), )

W0303 17:51:16.871523 48323 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:51:16.872574 48323 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995476 (unix time) try "date -d @1740995476" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd309c04fc2) received by PID 47896 (TID 0x7fd239b85700) from PID 163598274 ***]

2025-03-03 17:51:21.745093 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 0],"float32"), Tensor([13, 2, 4, 4],"float32"), )

W0303 17:51:25.940659 49307 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:51:25.942791 49307 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995485 (unix time) try "date -d @1740995485" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2938bb8fc2) received by PID 48588 (TID 0x7f2846949700) from PID 951816130 ***]

2025-03-03 17:51:30.624878 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 0],"float32"), Tensor([13, 4, 4, 0],"float32"), )

W0303 17:51:33.519515 51006 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:51:33.520642 51006 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995493 (unix time) try "date -d @1740995493" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f64ef72cfc2) received by PID 49952 (TID 0x7f63f9744700) from PID 18446744073431863234 ***]

2025-03-03 17:51:38.750508 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 0],"float32"), Tensor([13, 4, 4, 4],"float32"), )

W0303 17:51:41.745007 51851 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:51:41.746117 51851 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995501 (unix time) try "date -d @1740995501" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff3f7429fc2) received by PID 51201 (TID 0x7ff2cd4f4700) from PID 18446744073562922946 ***]

2025-03-03 17:51:46.420709 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([0, 2, 4, 4],"float32"), )

W0303 17:51:49.678232 53146 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:51:49.679515 53146 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([0, 2, 4, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:51:49.681090 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([0, 4, 4, 4],"float32"), )

[paddle error] paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([0, 4, 4, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:51:49.684451 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 0, 4, 4],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995509 (unix time) try "date -d @1740995509" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f94561a2fc2) received by PID 52319 (TID 0x7f9373dc2700) from PID 1444556738 ***]

2025-03-03 17:52:01.921682 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 2, 0, 4],"float32"), )

W0303 17:52:06.351590 55384 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:52:06.353161 55384 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 2, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:52:06.355423 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 2, 4, 0],"float32"), )

[paddle error] paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 2, 4, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:52:06.358667 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 4, 0, 4],"float32"), )

[paddle error] paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 4, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:52:06.360193 test begin: paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 4, 4, 0],"float32"), )

[paddle error] paddle.einsum("bind,bjnd->bnij", Tensor([13, 2, 4, 4],"float32"), Tensor([13, 4, 4, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:52:06.361505 test begin: paddle.einsum("bind,snd->bnis", Tensor([0, 2, 4, 4],"float32"), Tensor([2, 4, 4],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995526 (unix time) try "date -d @1740995526" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f17c5fa1fc2) received by PID 54785 (TID 0x7f16bbdc2700) from PID 18446744072736088002 ***]

2025-03-03 17:52:11.258678 test begin: paddle.einsum("bind,snd->bnis", Tensor([0, 4, 4, 4],"float32"), Tensor([2, 4, 4],"float32"), )

W0303 17:52:14.292243 56363 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:52:14.293334 56363 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995534 (unix time) try "date -d @1740995534" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7b86efcfc2) received by PID 55791 (TID 0x7f7ac67c3700) from PID 18446744071678447554 ***]

2025-03-03 17:52:27.019299 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 0, 4, 4],"float32"), Tensor([2, 4, 4],"float32"), )

W0303 17:52:30.097026 58692 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:52:30.098366 58692 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995550 (unix time) try "date -d @1740995550" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc562089fc2) received by PID 57518 (TID 0x7fc491dc2700) from PID 1644732354 ***]

2025-03-03 17:52:34.955996 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 2, 0, 4],"float32"), Tensor([2, 4, 4],"float32"), )

W0303 17:52:39.052107 60051 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:52:39.053236 60051 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995559 (unix time) try "date -d @1740995559" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5d83b8efc2) received by PID 59405 (TID 0x7f5c95abb700) from PID 18446744071624519618 ***]

2025-03-03 17:52:43.922542 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 2, 4, 0],"float32"), Tensor([2, 4, 4],"float32"), )

W0303 17:52:47.868206 61126 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:52:47.869275 61126 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995567 (unix time) try "date -d @1740995567" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2c29d9afc2) received by PID 60554 (TID 0x7f2b35dc2700) from PID 702132162 ***]

2025-03-03 17:52:52.727787 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 2, 4, 4],"float32"), Tensor([0, 4, 4],"float32"), )

W0303 17:52:56.324235 62583 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:52:56.325309 62583 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995576 (unix time) try "date -d @1740995576" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa7ecc0dfc2) received by PID 61752 (TID 0x7fa6bc949700) from PID 18446744073386647490 ***]

2025-03-03 17:53:08.097842 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 2, 4, 4],"float32"), Tensor([2, 0, 4],"float32"), )

W0303 17:53:11.292119 65057 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:53:11.293128 65057 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bind,snd->bnis", Tensor([13, 2, 4, 4],"float32"), Tensor([2, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:53:11.294456 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 2, 4, 4],"float32"), Tensor([2, 4, 0],"float32"), )

[paddle error] paddle.einsum("bind,snd->bnis", Tensor([13, 2, 4, 4],"float32"), Tensor([2, 4, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:53:11.296880 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 4, 0, 4],"float32"), Tensor([2, 4, 4],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995591 (unix time) try "date -d @1740995591" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6979638fc2) received by PID 64416 (TID 0x7f689f6f8700) from PID 2036568002 ***]

2025-03-03 17:53:15.777984 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 4, 4, 0],"float32"), Tensor([2, 4, 4],"float32"), )

W0303 17:53:19.262517 65826 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:53:19.263496 65826 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995599 (unix time) try "date -d @1740995599" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9e2200afc2) received by PID 65405 (TID 0x7f9d27dc2700) from PID 570470338 ***]

2025-03-03 17:53:24.094848 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 4, 4, 4],"float32"), Tensor([0, 4, 4],"float32"), )

W0303 17:53:26.971422 67185 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:53:26.972411 67185 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995606 (unix time) try "date -d @1740995606" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4847a63fc2) received by PID 66311 (TID 0x7f475babb700) from PID 1202077634 ***]

2025-03-03 17:53:31.558898 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 4, 4, 4],"float32"), Tensor([2, 0, 4],"float32"), )

W0303 17:53:34.692340 68532 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:53:34.693548 68532 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bind,snd->bnis", Tensor([13, 4, 4, 4],"float32"), Tensor([2, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:53:34.694893 test begin: paddle.einsum("bind,snd->bnis", Tensor([13, 4, 4, 4],"float32"), Tensor([2, 4, 0],"float32"), )

[paddle error] paddle.einsum("bind,snd->bnis", Tensor([13, 4, 4, 4],"float32"), Tensor([2, 4, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:53:34.697660 test begin: paddle.einsum("binh,tnh->bnit", Tensor([0, 2, 4, 4],"float32"), Tensor([4, 4, 4],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995614 (unix time) try "date -d @1740995614" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb5934c7fc2) received by PID 67972 (TID 0x7fb4af4f4700) from PID 18446744071885848514 ***]

2025-03-03 17:53:39.111249 test begin: paddle.einsum("binh,tnh->bnit", Tensor([0, 2, 4, 4],"float32"), Tensor([8, 4, 4],"float32"), )

W0303 17:53:42.949725 69391 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:53:42.950896 69391 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995622 (unix time) try "date -d @1740995622" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f72c9509fc2) received by PID 68888 (TID 0x7f71c934a700) from PID 18446744072792088514 ***]

2025-03-03 17:53:47.202402 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 0, 4, 4],"float32"), Tensor([4, 4, 4],"float32"), )

W0303 17:53:50.465301 70455 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:53:50.466569 70455 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995630 (unix time) try "date -d @1740995630" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f20e1a5bfc2) received by PID 69943 (TID 0x7f1fe3935700) from PID 18446744073200320450 ***]

2025-03-03 17:54:24.969934 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 0, 4, 4],"float32"), Tensor([8, 4, 4],"float32"), )

W0303 17:54:30.273466 72114 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:54:30.274606 72114 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995670 (unix time) try "date -d @1740995670" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4461be1fc2) received by PID 70786 (TID 0x7f4371b85700) from PID 1639849922 ***]

2025-03-03 17:54:34.981269 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 0, 4],"float32"), Tensor([4, 4, 4],"float32"), )

W0303 17:54:37.896752 74071 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:54:37.898036 74071 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995677 (unix time) try "date -d @1740995677" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1c45b87fc2) received by PID 73002 (TID 0x7f1b75b85700) from PID 1169719234 ***]

2025-03-03 17:55:12.861621 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 0, 4],"float32"), Tensor([8, 4, 4],"float32"), )

W0303 17:55:16.397761 75738 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:55:16.398780 75738 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995716 (unix time) try "date -d @1740995716" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3c9659cfc2) received by PID 74487 (TID 0x7f3ba7dc2700) from PID 18446744071937052610 ***]

2025-03-03 17:55:21.099452 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 0],"float32"), Tensor([4, 4, 4],"float32"), )

W0303 17:55:24.544148 76938 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:55:24.545416 76938 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995724 (unix time) try "date -d @1740995724" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fbe04209fc2) received by PID 76296 (TID 0x7fbcf7f48700) from PID 69246914 ***]

2025-03-03 17:55:28.953089 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 0],"float32"), Tensor([8, 4, 4],"float32"), )

W0303 17:55:31.758919 78195 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:55:31.759905 78195 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995731 (unix time) try "date -d @1740995731" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f60d4b2bfc2) received by PID 77463 (TID 0x7f5ff8949700) from PID 18446744072983068610 ***]

2025-03-03 17:55:36.387417 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([0, 4, 4],"float32"), )

W0303 17:55:40.042222 79452 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:55:40.043452 79452 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995740 (unix time) try "date -d @1740995740" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc1c9331fc2) received by PID 78654 (TID 0x7fc0ef34a700) from PID 18446744072790155202 ***]

2025-03-03 17:55:44.517390 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([4, 0, 4],"float32"), )

W0303 17:56:19.091851 80455 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:56:19.092986 80455 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([4, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:56:19.094505 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), )

[paddle error] paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([4, 4, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:56:19.099328 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([8, 0, 4],"float32"), )

[paddle error] paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([8, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:56:19.101406 test begin: paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([8, 4, 0],"float32"), )

[paddle error] paddle.einsum("binh,tnh->bnit", Tensor([13, 2, 4, 4],"float32"), Tensor([8, 4, 0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `h`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:56:19.102939 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([0, 5, 1, 1],"float32"), Tensor([0, 5, 1, 3],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995779 (unix time) try "date -d @1740995779" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa9978fbfc2) received by PID 80032 (TID 0x7fa89587e700) from PID 18446744071957364674 ***]

2025-03-03 17:56:24.178688 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([0, 5, 1, 1],"float32"), Tensor([13, 5, 1, 3],"float32"), )

W0303 17:56:27.367620 82138 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:56:27.368670 82138 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995787 (unix time) try "date -d @1740995787" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5fa65c4fc2) received by PID 81357 (TID 0x7f5e91dc2700) from PID 18446744072205651906 ***]

2025-03-03 17:56:31.613226 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([0, 5, 1, 1],"float32"), Tensor([52, 5, 1, 3],"float32"), )

W0303 17:57:06.511250 83652 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:57:06.512204 83652 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995826 (unix time) try "date -d @1740995826" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f456ba48fc2) received by PID 83087 (TID 0x7f447dabb700) from PID 1805946818 ***]

2025-03-03 17:57:10.779385 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 0, 1, 1],"float32"), Tensor([13, 0, 1, 3],"float32"), )

W0303 17:57:13.339262 85580 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:57:13.340366 85580 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995833 (unix time) try "date -d @1740995833" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd31c5aafc2) received by PID 85021 (TID 0x7fd241f48700) from PID 475705282 ***]

2025-03-03 17:57:18.927404 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 0, 1, 1],"float32"), Tensor([13, 5, 1, 3],"float32"), )

W0303 17:57:22.174537 86435 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:57:22.175527 86435 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995842 (unix time) try "date -d @1740995842" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5a13f8bfc2) received by PID 86126 (TID 0x7f5915f48700) from PID 335069122 ***]

2025-03-03 17:57:56.742233 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 0, 1],"float32"), Tensor([13, 5, 0, 3],"float32"), )

W0303 17:58:00.294670 88343 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:58:00.295722 88343 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995880 (unix time) try "date -d @1740995880" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f647ef8afc2) received by PID 86974 (TID 0x7f63907c3700) from PID 2130227138 ***]

2025-03-03 17:58:04.768557 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 0, 1],"float32"), Tensor([13, 5, 1, 3],"float32"), )

W0303 17:58:07.640364 89703 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:58:07.641539 89703 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995887 (unix time) try "date -d @1740995887" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f264725cfc2) received by PID 89128 (TID 0x7f256d2b7700) from PID 1193660354 ***]

2025-03-03 17:58:12.377434 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 0],"float32"), Tensor([13, 5, 1, 0],"float32"), )

W0303 17:58:46.029706 90606 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:58:46.031291 90606 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995926 (unix time) try "date -d @1740995926" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0b89726fc2) received by PID 90043 (TID 0x7f0ab16f8700) from PID 18446744071720562626 ***]

2025-03-03 17:58:50.443181 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 0],"float32"), Tensor([13, 5, 1, 3],"float32"), )

W0303 17:58:55.433104 92384 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:58:55.434158 92384 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995935 (unix time) try "date -d @1740995935" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fde59233fc2) received by PID 91808 (TID 0x7fdd52949700) from PID 1495482306 ***]

2025-03-03 17:58:59.769705 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([0, 5, 1, 3],"float32"), )

W0303 17:59:36.485467 94233 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:59:36.486392 94233 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([0, 5, 1, 3],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:59:36.487576 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 0, 1, 3],"float32"), )

[paddle error] paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 0, 1, 3],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `l`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 17:59:36.493201 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 5, 0, 3],"float32"), )

[paddle error] paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 5, 0, 3],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 17:59:36.495212 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 5, 1, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995976 (unix time) try "date -d @1740995976" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3839629fc2) received by PID 93438 (TID 0x7f37236f8700) from PID 962764738 ***]

2025-03-03 17:59:41.228864 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 0, 1, 1],"float32"), Tensor([52, 0, 1, 3],"float32"), )

W0303 17:59:44.350782 96039 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 17:59:44.351763 96039 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740995984 (unix time) try "date -d @1740995984" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4484181fc2) received by PID 95480 (TID 0x7f439ff48700) from PID 18446744071630757826 ***]

2025-03-03 17:59:49.077591 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 0, 1, 1],"float32"), Tensor([52, 5, 1, 3],"float32"), )

W0303 18:00:24.688241 96909 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:00:24.689850 96909 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996024 (unix time) try "date -d @1740996024" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efea020bfc2) received by PID 96313 (TID 0x7efd81f48700) from PID 18446744072101085122 ***]

2025-03-03 18:00:33.573733 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 0, 1],"float32"), Tensor([52, 5, 0, 3],"float32"), )

W0303 18:00:36.557093 99573 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:00:36.558622 99573 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996036 (unix time) try "date -d @1740996036" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9fb6987fc2) received by PID 98415 (TID 0x7f9eb07c3700) from PID 18446744072478031810 ***]

2025-03-03 18:00:41.509979 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 0, 1],"float32"), Tensor([52, 5, 1, 3],"float32"), )

W0303 18:00:44.816187 100771 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:00:44.817735 100771 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996044 (unix time) try "date -d @1740996044" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7faaf8ac4fc2) received by PID 100132 (TID 0x7fa9f4949700) from PID 18446744073586626498 ***]

2025-03-03 18:00:56.933254 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 0],"float32"), Tensor([52, 5, 1, 0],"float32"), )

W0303 18:01:32.291721 103038 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:01:32.292840 103038 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996092 (unix time) try "date -d @1740996092" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f81050a7fc2) received by PID 102022 (TID 0x7f802e949700) from PID 84574146 ***]

2025-03-03 18:01:36.565719 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 0],"float32"), Tensor([52, 5, 1, 3],"float32"), )

W0303 18:01:43.225445 104601 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:01:43.230039 104601 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996103 (unix time) try "date -d @1740996103" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4c7a75afc2) received by PID 104051 (TID 0x7f4b867c3700) from PID 2054533058 ***]

2025-03-03 18:01:47.846732 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([0, 5, 1, 3],"float32"), )

W0303 18:02:25.537420 105780 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:02:25.539086 105780 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([0, 5, 1, 3],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:52 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:02:25.540434 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 0, 1, 3],"float32"), )

[paddle error] paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 0, 1, 3],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `l`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:02:25.542691 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 5, 0, 3],"float32"), )

[paddle error] paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 5, 0, 3],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 18:02:25.544757 test begin: paddle.einsum("blkd,bldq->blkq", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 5, 1, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996145 (unix time) try "date -d @1740996145" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb86d54ffc2) received by PID 105160 (TID 0x7fb73b34a700) from PID 1834287042 ***]

2025-03-03 18:02:31.599156 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([0, 3, 5],"float64"), Tensor([1, 2, 3, 4],"float64"), )

W0303 18:02:39.272600 108004 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:02:39.273590 108004 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996159 (unix time) try "date -d @1740996159" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0cd5c24c32) received by PID 107124 (TID 0x7f0be9b85700) from PID 18446744073000864818 ***]

2025-03-03 18:02:45.296599 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([0, 5, 1],"float32"), Tensor([13, 4, 5, 2],"float32"), )

W0303 18:02:48.808306 109578 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:02:48.809144 109578 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996168 (unix time) try "date -d @1740996168" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6e99bb4fc2) received by PID 108625 (TID 0x7f6d95b85700) from PID 18446744071993774018 ***]

2025-03-03 18:02:54.911081 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 0, 5],"float64"), Tensor([1, 2, 3, 4],"float64"), )

W0303 18:02:59.262058 110667 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:02:59.263864 110667 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996179 (unix time) try "date -d @1740996179" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4ab4befc32) received by PID 109784 (TID 0x7f49a6949700) from PID 18446744072446999602 ***]

2025-03-03 18:03:04.997761 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 0],"float64"), Tensor([1, 2, 3, 4],"float64"), )

W0303 18:03:39.243880 112300 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:03:39.245229 112300 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996219 (unix time) try "date -d @1740996219" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3437091c32) received by PID 111462 (TID 0x7f334b177700) from PID 923343922 ***]

2025-03-03 18:03:43.950314 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([0, 2, 3, 4],"float64"), )

W0303 18:03:48.249269 113862 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:03:48.250659 113862 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([0, 2, 3, 4],"float64"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 18:03:48.252967 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([1, 0, 3, 4],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996228 (unix time) try "date -d @1740996228" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f92c51e3c32) received by PID 113549 (TID 0x7f91e8949700) from PID 18446744072721677362 ***]

2025-03-03 18:03:53.020431 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([1, 2, 0, 4],"float64"), )

W0303 18:03:58.187201 115274 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:03:58.188702 115274 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([1, 2, 0, 4],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `l`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:3 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:03:58.190380 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([1, 3, 5],"float64"), Tensor([1, 2, 3, 0],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996238 (unix time) try "date -d @1740996238" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7549289c32) received by PID 114541 (TID 0x7f743134a700) from PID 1227398194 ***]

2025-03-03 18:04:33.982217 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 0, 1],"float32"), Tensor([13, 4, 5, 2],"float32"), )

W0303 18:04:37.955860 117479 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:04:37.956923 117479 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996277 (unix time) try "date -d @1740996277" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f702760afc2) received by PID 116924 (TID 0x7f6f414f4700) from PID 660647874 ***]

2025-03-03 18:04:42.579677 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 0],"float32"), Tensor([13, 4, 5, 2],"float32"), )

W0303 18:04:45.707211 118773 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:04:45.708500 118773 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996285 (unix time) try "date -d @1740996285" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3dd0cd7fc2) received by PID 117970 (TID 0x7f3cf2949700) from PID 18446744072917712834 ***]

2025-03-03 18:05:22.931078 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 1],"float32"), Tensor([0, 4, 5, 2],"float32"), )

W0303 18:05:27.061885 120418 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:05:27.063191 120418 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 1],"float32"), Tensor([0, 4, 5, 2],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:05:27.064741 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 1],"float32"), Tensor([13, 0, 5, 2],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996327 (unix time) try "date -d @1740996327" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f25eba03fc2) received by PID 118966 (TID 0x7f24c1abb700) from PID 18446744073367732162 ***]

2025-03-03 18:05:31.399910 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 1],"float32"), Tensor([13, 4, 0, 2],"float32"), )

W0303 18:05:34.773458 122243 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:05:34.775431 122243 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 1],"float32"), Tensor([13, 4, 0, 2],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `l`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:05:34.777201 test begin: paddle.einsum("blq,bhlk->bhlqk", Tensor([13, 5, 1],"float32"), Tensor([13, 4, 5, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996334 (unix time) try "date -d @1740996334" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f19aa4f3fc2) received by PID 121589 (TID 0x7f18dfdc2700) from PID 18446744072271904706 ***]

2025-03-03 18:06:09.973351 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([0, 5, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), )

W0303 18:06:14.735122 123930 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:06:14.736377 123930 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996374 (unix time) try "date -d @1740996374" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f95d6631fc2) received by PID 122658 (TID 0x7f94e9dc2700) from PID 18446744073011404738 ***]

2025-03-03 18:06:19.776363 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([0, 5, 1, 1],"float32"), Tensor([13, 1, 1, 1],"float32"), )

W0303 18:06:23.886452 125142 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:06:23.887406 125142 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996383 (unix time) try "date -d @1740996383" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb4d1e83fc2) received by PID 124496 (TID 0x7fb3cbdc2700) from PID 18446744072936243138 ***]

2025-03-03 18:06:28.430052 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([0, 5, 1, 1],"float32"), Tensor([52, 1, 1, 1],"float32"), )

W0303 18:06:31.372398 126625 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:06:31.373443 126625 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996391 (unix time) try "date -d @1740996391" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7a6a158fc2) received by PID 125760 (TID 0x7f7989dc2700) from PID 1779797954 ***]

2025-03-03 18:06:37.021314 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 0, 1, 1],"float32"), Tensor([13, 0, 1, 1],"float32"), )

W0303 18:07:07.908354 127770 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:07:07.909286 127770 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996427 (unix time) try "date -d @1740996427" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff13a3a8fc2) received by PID 127263 (TID 0x7ff03bdc2700) from PID 976916418 ***]

2025-03-03 18:07:13.535361 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 0, 1, 1],"float32"), Tensor([13, 1, 1, 1],"float32"), )

W0303 18:07:16.527094 129140 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:07:16.527993 129140 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996436 (unix time) try "date -d @1740996436" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6ae04bafc2) received by PID 128579 (TID 0x7f6a01f48700) from PID 18446744073177640898 ***]

2025-03-03 18:07:20.776710 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 0, 1],"float32"), Tensor([13, 1, 0, 1],"float32"), )

W0303 18:07:24.136646 129913 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:07:24.138896 129913 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996444 (unix time) try "date -d @1740996444" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f94a25b8fc2) received by PID 129630 (TID 0x7f939bdc2700) from PID 18446744072138493890 ***]

2025-03-03 18:07:36.419471 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 0, 1],"float32"), Tensor([13, 1, 1, 1],"float32"), )

W0303 18:07:40.713488 132301 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:07:40.714494 132301 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996460 (unix time) try "date -d @1740996460" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe102b36fc2) received by PID 132021 (TID 0x7fdfd07c3700) from PID 45313986 ***]

2025-03-03 18:07:45.412948 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 0],"float32"), Tensor([13, 1, 1, 0],"float32"), )

W0303 18:07:48.050770 133827 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:07:48.051730 133827 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996468 (unix time) try "date -d @1740996468" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1e0857dfc2) received by PID 133186 (TID 0x7f1d39f48700) from PID 139976642 ***]

2025-03-03 18:07:52.320270 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 0],"float32"), Tensor([13, 1, 1, 1],"float32"), )

W0303 18:07:56.652835 134562 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:07:56.653860 134562 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996476 (unix time) try "date -d @1740996476" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0ffa2c3fc2) received by PID 134007 (TID 0x7f0ee5dc2700) from PID 18446744073611788226 ***]

2025-03-03 18:08:01.164814 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), )

W0303 18:08:03.933784 136172 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:08:03.934818 136172 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:08:03.935949 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 0, 1, 1],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996483 (unix time) try "date -d @1740996483" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa7fc1fefc2) received by PID 135598 (TID 0x7fa725f48700) from PID 18446744073644535746 ***]

2025-03-03 18:08:08.172790 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 1, 0, 1],"float32"), )

W0303 18:08:13.870429 137141 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:08:13.872052 137141 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 1, 0, 1],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 18:08:13.873780 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([13, 5, 1, 1],"float32"), Tensor([13, 1, 1, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996493 (unix time) try "date -d @1740996493" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6ff8f1ffc2) received by PID 136586 (TID 0x7f6f28949700) from PID 18446744073591193538 ***]

2025-03-03 18:08:18.752587 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 0, 1, 1],"float32"), Tensor([52, 0, 1, 1],"float32"), )

W0303 18:08:24.007102 138368 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:08:24.008272 138368 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996504 (unix time) try "date -d @1740996504" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f768a7b3fc2) received by PID 137637 (TID 0x7f758e7c3700) from PID 18446744071737917378 ***]

2025-03-03 18:08:28.657920 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 0, 1, 1],"float32"), Tensor([52, 1, 1, 1],"float32"), )

W0303 18:08:32.300362 139883 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:08:32.301473 139883 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996512 (unix time) try "date -d @1740996512" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe95cccdfc2) received by PID 139152 (TID 0x7fe860949700) from PID 1556930498 ***]

2025-03-03 18:08:44.607106 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 0, 1],"float32"), Tensor([52, 1, 0, 1],"float32"), )

W0303 18:08:48.259840 141778 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:08:48.260902 141778 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996528 (unix time) try "date -d @1740996528" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f30ad2e7fc2) received by PID 141269 (TID 0x7f2fc334a700) from PID 18446744072320090050 ***]

2025-03-03 18:08:53.555067 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 0, 1],"float32"), Tensor([52, 1, 1, 1],"float32"), )

W0303 18:08:58.387650 142990 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:08:58.388566 142990 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996538 (unix time) try "date -d @1740996538" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa092637fc2) received by PID 142341 (TID 0x7f9fafdc2700) from PID 18446744071870578626 ***]

2025-03-03 18:09:10.250098 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 0],"float32"), Tensor([52, 1, 1, 0],"float32"), )

W0303 18:09:16.033336 145498 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:09:16.039825 145498 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996556 (unix time) try "date -d @1740996556" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3e07a1bfc2) received by PID 145051 (TID 0x7f3d35abb700) from PID 128040898 ***]

2025-03-03 18:09:20.810078 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 0],"float32"), Tensor([52, 1, 1, 1],"float32"), )

W0303 18:09:25.445199 146708 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:09:25.446128 146708 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996565 (unix time) try "date -d @1740996565" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd6d7d97fc2) received by PID 146061 (TID 0x7fd5b7d0b700) from PID 18446744073035939778 ***]

2025-03-03 18:09:36.485642 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), )

W0303 18:09:41.045544 149025 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:09:41.046743 149025 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([0, 1, 1, 1],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:52 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:09:41.049116 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 0, 1, 1],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996581 (unix time) try "date -d @1740996581" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc752576fc2) received by PID 148296 (TID 0x7fc659dc2700) from PID 1381461954 ***]

2025-03-03 18:09:45.588064 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 1, 0, 1],"float32"), )

W0303 18:09:49.618013 150113 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:09:49.619125 150113 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 1, 0, 1],"float32"), ) 
 (InvalidArgument) All elements of the input 'repeat_times' for tile op must be positive integers, but the value received is 0.
  [Hint: Expected repeat_times_data[i] > 0, but received repeat_times_data[i]:0 <= 0:0.] (at ../paddle/phi/kernels/gpu/tile_kernel.cu:45)

2025-03-03 18:09:49.620573 test begin: paddle.einsum("blqd,bmdk->blqk", Tensor([52, 5, 1, 1],"float32"), Tensor([52, 1, 1, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996589 (unix time) try "date -d @1740996589" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd995bdffc2) received by PID 149587 (TID 0x7fd8c7b85700) from PID 18446744071926841282 ***]

2025-03-03 18:09:54.472464 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([0, 2, 16, 16],"float32"), Tensor([0, 16, 4, 1],"float32"), )

W0303 18:09:57.822307 151325 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:09:57.823284 151325 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bmtd,mdhr->bmhtr", Tensor([0, 2, 16, 16],"float32"), Tensor([0, 16, 4, 1],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `m`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:09:57.824525 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([0, 2, 16, 16],"float32"), Tensor([2, 16, 4, 1],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996597 (unix time) try "date -d @1740996597" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe575781fc2) received by PID 150678 (TID 0x7fe45b6f8700) from PID 1970806722 ***]

2025-03-03 18:10:04.094487 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 0, 16, 16],"float32"), Tensor([2, 0, 4, 1],"float32"), )

W0303 18:10:07.273936 152678 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:10:07.274866 152678 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 0, 16, 16],"float32"), Tensor([2, 0, 4, 1],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:16 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:10:07.275740 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 0, 16, 16],"float32"), Tensor([2, 16, 4, 1],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996607 (unix time) try "date -d @1740996607" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8c3563dfc2) received by PID 152027 (TID 0x7f8b556f8700) from PID 895737794 ***]

2025-03-03 18:10:11.520996 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 0, 16],"float32"), Tensor([2, 16, 0, 1],"float32"), )

W0303 18:10:14.062325 153755 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:10:14.063488 153755 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996614 (unix time) try "date -d @1740996614" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe38465cfc2) received by PID 153250 (TID 0x7fe28df48700) from PID 18446744071635849154 ***]

2025-03-03 18:10:48.610757 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 0, 16],"float32"), Tensor([2, 16, 4, 1],"float32"), )

W0303 18:10:52.223757 155243 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:10:52.224716 155243 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996652 (unix time) try "date -d @1740996652" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f46c3188fc2) received by PID 154736 (TID 0x7f458f277700) from PID 18446744072687751106 ***]

2025-03-03 18:10:56.840331 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 0],"float32"), Tensor([2, 16, 4, 0],"float32"), )

W0303 18:11:01.395867 156790 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:11:01.396855 156790 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996661 (unix time) try "date -d @1740996661" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1a756e7fc2) received by PID 155803 (TID 0x7f19976f8700) from PID 1970175938 ***]

2025-03-03 18:11:07.080197 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 0],"float32"), Tensor([2, 16, 4, 1],"float32"), )

W0303 18:11:40.046200 157883 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:11:40.047396 157883 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996700 (unix time) try "date -d @1740996700" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1a4b9e1fc2) received by PID 157250 (TID 0x7f1965abb700) from PID 1268654018 ***]

2025-03-03 18:11:44.624359 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 16],"float32"), Tensor([0, 16, 4, 1],"float32"), )

W0303 18:11:48.893471 159478 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:11:48.896237 159478 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 16],"float32"), Tensor([0, 16, 4, 1],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `m`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:11:48.899220 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 16],"float32"), Tensor([2, 0, 4, 1],"float32"), )

[paddle error] paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 16],"float32"), Tensor([2, 0, 4, 1],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `d`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:16 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:11:48.902689 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 16],"float32"), Tensor([2, 16, 0, 1],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996708 (unix time) try "date -d @1740996708" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd1c9736fc2) received by PID 158909 (TID 0x7fd0db6f8700) from PID 18446744072794369986 ***]

2025-03-03 18:11:53.843332 test begin: paddle.einsum("bmtd,mdhr->bmhtr", Tensor([13, 2, 16, 16],"float32"), Tensor([2, 16, 4, 0],"float32"), )

W0303 18:11:58.583909 160864 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:11:58.584975 160864 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996718 (unix time) try "date -d @1740996718" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fbbe6145fc2) received by PID 160142 (TID 0x7fbb07dc2700) from PID 18446744073274679234 ***]

2025-03-03 18:12:10.222144 test begin: paddle.einsum("bn,anm,bm->ba", Tensor([0, 5],"float64"), Tensor([3, 5, 2],"float64"), Tensor([2, 2],"float64"), )

[paddle error] paddle.einsum("bn,anm,bm->ba", Tensor([0, 5],"float64"), Tensor([3, 5, 2],"float64"), Tensor([2, 2],"float64"), ) 
 Size of label 'b' for operand 2 (0) does not match previous terms (2).
2025-03-03 18:12:14.244356 test begin: paddle.einsum("bn,anm,bm->ba", Tensor([2, 0],"float64"), Tensor([3, 5, 2],"float64"), Tensor([2, 2],"float64"), )

[paddle error] paddle.einsum("bn,anm,bm->ba", Tensor([2, 0],"float64"), Tensor([3, 5, 2],"float64"), Tensor([2, 2],"float64"), ) 
 Size of label 'n' for operand 1 (0) does not match previous terms (5).
2025-03-03 18:12:14.247325 test begin: paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([0, 5, 2],"float64"), Tensor([2, 2],"float64"), )

W0303 18:12:14.250542 163276 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:12:14.251555 163276 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996734 (unix time) try "date -d @1740996734" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3abd2b1c32) received by PID 162562 (TID 0x7f399934a700) from PID 18446744072588303410 ***]

2025-03-03 18:12:50.188278 test begin: paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([3, 0, 2],"float64"), Tensor([2, 2],"float64"), )

[paddle error] paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([3, 0, 2],"float64"), Tensor([2, 2],"float64"), ) 
 Size of label 'n' for operand 1 (5) does not match previous terms (0).
2025-03-03 18:12:53.383266 test begin: paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([3, 5, 0],"float64"), Tensor([2, 2],"float64"), )

[paddle error] paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([3, 5, 0],"float64"), Tensor([2, 2],"float64"), ) 
 Size of label 'm' for operand 2 (0) does not match previous terms (2).
2025-03-03 18:12:53.394430 test begin: paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([3, 5, 2],"float64"), Tensor([0, 2],"float64"), )

[paddle error] paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([3, 5, 2],"float64"), Tensor([0, 2],"float64"), ) 
 Size of label 'b' for operand 2 (2) does not match previous terms (0).
2025-03-03 18:12:53.403510 test begin: paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([3, 5, 2],"float64"), Tensor([2, 0],"float64"), )

[paddle error] paddle.einsum("bn,anm,bm->ba", Tensor([2, 5],"float64"), Tensor([3, 5, 2],"float64"), Tensor([2, 0],"float64"), ) 
 Size of label 'm' for operand 2 (2) does not match previous terms (0).
2025-03-03 18:12:53.410557 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([0, 4, 2, 2],"float32"), Tensor([0, 2, 4, 4],"float32"), )

W0303 18:12:53.413161  1740 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:12:53.414279  1740 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996773 (unix time) try "date -d @1740996773" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f03efad8fc2) received by PID 809 (TID 0x7f02e1abb700) from PID 18446744073435713474 ***]

2025-03-03 18:12:59.973181 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([0, 4, 2, 2],"float32"), Tensor([13, 2, 4, 4],"float32"), )

W0303 18:13:03.981216  2870 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:13:03.982424  2870 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996783 (unix time) try "date -d @1740996783" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7c62641fc2) received by PID 1852 (TID 0x7f7b75dc2700) from PID 1650728898 ***]

2025-03-03 18:13:45.119211 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([0, 4, 2, 4],"float32"), Tensor([0, 4, 4, 4],"float32"), )

W0303 18:13:48.303603  4867 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:13:48.304553  4867 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996828 (unix time) try "date -d @1740996828" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fbe6a0c6fc2) received by PID 4556 (TID 0x7fbd63dc2700) from PID 1779199938 ***]

2025-03-03 18:13:53.058135 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([0, 4, 2, 4],"float32"), Tensor([13, 4, 4, 4],"float32"), )

W0303 18:13:56.405449  6057 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:13:56.406554  6057 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996836 (unix time) try "date -d @1740996836" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8d6b9edfc2) received by PID 5336 (TID 0x7f8c71abb700) from PID 1805574082 ***]

2025-03-03 18:14:31.748190 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 0, 2, 2],"float32"), Tensor([13, 0, 4, 4],"float32"), )

W0303 18:14:35.634451  8336 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:14:35.635442  8336 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 0, 2, 2],"float32"), Tensor([13, 0, 4, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:14:35.636291 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 0, 2, 2],"float32"), Tensor([13, 2, 4, 4],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996875 (unix time) try "date -d @1740996875" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4e09c8dfc2) received by PID 6867 (TID 0x7f4d1fb85700) from PID 164159426 ***]

2025-03-03 18:14:40.188360 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 0, 2, 4],"float32"), Tensor([13, 0, 4, 4],"float32"), )

W0303 18:14:44.158223  9359 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:14:44.159365  9359 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 0, 2, 4],"float32"), Tensor([13, 0, 4, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:14:44.160475 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 0, 2, 4],"float32"), Tensor([13, 4, 4, 4],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996884 (unix time) try "date -d @1740996884" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8c07d9afc2) received by PID 8913 (TID 0x7f8b2dd0b700) from PID 131706818 ***]

2025-03-03 18:14:48.885699 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 0, 2],"float32"), Tensor([13, 2, 0, 4],"float32"), )

W0303 18:15:25.576263 10430 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:15:25.577939 10430 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 0, 2],"float32"), Tensor([13, 2, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:15:25.579294 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 0, 2],"float32"), Tensor([13, 2, 4, 4],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996925 (unix time) try "date -d @1740996925" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f81e0755fc2) received by PID 9850 (TID 0x7f80f7f48700) from PID 18446744073180372930 ***]

2025-03-03 18:15:29.894332 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 0, 4],"float32"), Tensor([13, 4, 0, 4],"float32"), )

W0303 18:15:32.956558 13030 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:15:32.957753 13030 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 0, 4],"float32"), Tensor([13, 4, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:15:32.958954 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 0, 4],"float32"), Tensor([13, 4, 4, 4],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996932 (unix time) try "date -d @1740996932" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6c7d488fc2) received by PID 11997 (TID 0x7f6b9b34a700) from PID 2101907394 ***]

2025-03-03 18:15:37.358959 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 0],"float32"), Tensor([13, 2, 4, 0],"float32"), )

W0303 18:16:12.407883 14096 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:16:12.408929 14096 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996972 (unix time) try "date -d @1740996972" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fda4d479fc2) received by PID 13471 (TID 0x7fd94534a700) from PID 1296539586 ***]

2025-03-03 18:16:19.343605 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 0],"float32"), Tensor([13, 2, 4, 4],"float32"), )

W0303 18:16:22.484728 16120 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:16:22.485888 16120 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740996982 (unix time) try "date -d @1740996982" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8952310fc2) received by PID 15550 (TID 0x7f8855dc2700) from PID 1378947010 ***]

2025-03-03 18:16:26.742595 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 0],"float32"), Tensor([13, 4, 4, 0],"float32"), )

W0303 18:17:00.888528 17111 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:17:00.889778 17111 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997020 (unix time) try "date -d @1740997020" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fcab58a7fc2) received by PID 16740 (TID 0x7fc9a7935700) from PID 18446744072460337090 ***]

2025-03-03 18:17:05.987956 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 0],"float32"), Tensor([13, 4, 4, 4],"float32"), )

W0303 18:17:40.743417 19007 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:17:40.744722 19007 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997060 (unix time) try "date -d @1740997060" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fefb12befc2) received by PID 18445 (TID 0x7feed334a700) from PID 18446744072387030978 ***]

2025-03-03 18:17:44.991352 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 2],"float32"), Tensor([0, 2, 4, 4],"float32"), )

W0303 18:17:50.434015 20746 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:17:50.435060 20746 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 2],"float32"), Tensor([0, 2, 4, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:17:50.451428 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 2],"float32"), Tensor([13, 0, 4, 4],"float32"), )

[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 2],"float32"), Tensor([13, 0, 4, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:17:50.457162 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 2],"float32"), Tensor([13, 2, 0, 4],"float32"), )

[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 2],"float32"), Tensor([13, 2, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:17:50.460169 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 2],"float32"), Tensor([13, 2, 4, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997070 (unix time) try "date -d @1740997070" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb2584c0fc2) received by PID 20119 (TID 0x7fb17bf48700) from PID 1481379778 ***]

2025-03-03 18:17:55.093481 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 4],"float32"), Tensor([0, 4, 4, 4],"float32"), )

W0303 18:18:00.252925 22192 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:18:00.254531 22192 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 4],"float32"), Tensor([0, 4, 4, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:13 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:18:00.258875 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 4],"float32"), Tensor([13, 0, 4, 4],"float32"), )

[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 4],"float32"), Tensor([13, 0, 4, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:18:00.265531 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 4],"float32"), Tensor([13, 4, 0, 4],"float32"), )

[paddle error] paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 4],"float32"), Tensor([13, 4, 0, 4],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:18:00.267380 test begin: paddle.einsum("bnij,bjnd->bind", Tensor([13, 4, 2, 4],"float32"), Tensor([13, 4, 4, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997080 (unix time) try "date -d @1740997080" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3db1f31fc2) received by PID 21248 (TID 0x7f3caddc2700) from PID 18446744072400084930 ***]

2025-03-03 18:18:34.639006 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([0, 4, 7, 7],"float32"), Tensor([0, 10, 4, 8],"float32"), )

W0303 18:18:38.776396 23988 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:18:38.777539 23988 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([0, 4, 7, 7],"float32"), Tensor([0, 10, 4, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:7 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:18:38.778473 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([0, 4, 7, 7],"float32"), Tensor([0, 11, 4, 8],"float32"), )

[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([0, 4, 7, 7],"float32"), Tensor([0, 11, 4, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:7 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:18:38.780684 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([0, 4, 7, 7],"float32"), Tensor([7, 10, 4, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997118 (unix time) try "date -d @1740997118" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f702bde0fc2) received by PID 23700 (TID 0x7f6f3fd0b700) from PID 735973314 ***]

2025-03-03 18:19:14.121910 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([0, 4, 7, 7],"float32"), Tensor([7, 11, 4, 8],"float32"), )

W0303 18:19:19.201854 25724 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:19:19.203125 25724 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997159 (unix time) try "date -d @1740997159" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f2247c5ffc2) received by PID 24459 (TID 0x7f2155d0b700) from PID 1204158402 ***]

2025-03-03 18:19:55.026396 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 0, 7, 7],"float32"), Tensor([7, 0, 4, 8],"float32"), )

W0303 18:19:57.965991 27894 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:19:57.967156 27894 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 0, 7, 7],"float32"), Tensor([7, 0, 4, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:19:57.968023 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 0, 7, 7],"float32"), Tensor([7, 10, 4, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997197 (unix time) try "date -d @1740997197" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdfa9efefc2) received by PID 26459 (TID 0x7fdec7dc2700) from PID 18446744072265658306 ***]

2025-03-03 18:20:03.986998 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 0, 7],"float32"), Tensor([7, 10, 0, 8],"float32"), )

W0303 18:20:08.223606 29319 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:20:08.224679 29319 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 0, 7],"float32"), Tensor([7, 10, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:20:08.225549 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 0, 7],"float32"), Tensor([7, 10, 4, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997208 (unix time) try "date -d @1740997208" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7e52f9efc2) received by PID 28681 (TID 0x7f7d7c7c3700) from PID 1392111554 ***]

2025-03-03 18:20:13.222831 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 0],"float32"), Tensor([7, 10, 4, 0],"float32"), )

W0303 18:20:16.026221 30061 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:20:16.027395 30061 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997216 (unix time) try "date -d @1740997216" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa72f84bfc2) received by PID 29857 (TID 0x7fa62d87e700) from PID 797228994 ***]

2025-03-03 18:20:20.774705 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 0],"float32"), Tensor([7, 10, 4, 8],"float32"), )

W0303 18:20:23.926067 31077 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:20:23.927067 31077 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997223 (unix time) try "date -d @1740997223" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff591bb2fc2) received by PID 30592 (TID 0x7ff49fb85700) from PID 18446744071859548098 ***]

2025-03-03 18:20:28.143384 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 7],"float32"), Tensor([0, 10, 4, 8],"float32"), )

W0303 18:20:30.856459 32389 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:20:30.857461 32389 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 7],"float32"), Tensor([0, 10, 4, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:7 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:20:30.858676 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 7],"float32"), Tensor([7, 0, 4, 8],"float32"), )

[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 7],"float32"), Tensor([7, 0, 4, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:20:30.860800 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 7],"float32"), Tensor([7, 10, 0, 8],"float32"), )

[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 7],"float32"), Tensor([7, 10, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:20:30.865648 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([10, 4, 7, 7],"float32"), Tensor([7, 10, 4, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997230 (unix time) try "date -d @1740997230" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f8a8dde3fc2) received by PID 31424 (TID 0x7f89bbdc2700) from PID 18446744071794737090 ***]

2025-03-03 18:20:36.740907 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 0, 7, 7],"float32"), Tensor([7, 0, 4, 8],"float32"), )

W0303 18:20:39.571368 33737 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:20:39.573235 33737 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 0, 7, 7],"float32"), Tensor([7, 0, 4, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:20:39.574543 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 0, 7, 7],"float32"), Tensor([7, 11, 4, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997239 (unix time) try "date -d @1740997239" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0c6a3c3fc2) received by PID 33079 (TID 0x7f0b67dc2700) from PID 1782333378 ***]

2025-03-03 18:21:15.572910 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 0, 7],"float32"), Tensor([7, 11, 0, 8],"float32"), )

W0303 18:21:20.949998 35340 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:21:20.951627 35340 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 0, 7],"float32"), Tensor([7, 11, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:21:20.952564 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 0, 7],"float32"), Tensor([7, 11, 4, 8],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997280 (unix time) try "date -d @1740997280" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f54fcc06fc2) received by PID 34146 (TID 0x7f53fe949700) from PID 18446744073655054274 ***]

2025-03-03 18:21:56.597837 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 0],"float32"), Tensor([7, 11, 4, 0],"float32"), )

W0303 18:22:00.274374 37536 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:22:00.275321 37536 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997320 (unix time) try "date -d @1740997320" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0905649fc2) received by PID 35999 (TID 0x7f07e96f8700) from PID 90480578 ***]

2025-03-03 18:22:04.926668 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 0],"float32"), Tensor([7, 11, 4, 8],"float32"), )

W0303 18:22:07.415113 38987 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:22:07.416078 38987 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997327 (unix time) try "date -d @1740997327" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fde5faa1fc2) received by PID 38266 (TID 0x7fdd69abb700) from PID 1604984770 ***]

2025-03-03 18:22:11.763568 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 7],"float32"), Tensor([0, 11, 4, 8],"float32"), )

W0303 18:22:16.990245 39651 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:22:16.991309 39651 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 7],"float32"), Tensor([0, 11, 4, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `j`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:7 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:22:16.999019 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 7],"float32"), Tensor([7, 0, 4, 8],"float32"), )

[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 7],"float32"), Tensor([7, 0, 4, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `b`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:22:17.008504 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 7],"float32"), Tensor([7, 11, 0, 8],"float32"), )

[paddle error] paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 7],"float32"), Tensor([7, 11, 0, 8],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `n`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:4 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:22:17.020614 test begin: paddle.einsum("bnij,jbnd->ibnd", Tensor([11, 4, 7, 7],"float32"), Tensor([7, 11, 4, 0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997337 (unix time) try "date -d @1740997337" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd84e4befc2) received by PID 39225 (TID 0x7fd73bdc2700) from PID 1313599426 ***]

2025-03-03 18:22:21.479322 test begin: paddle.einsum("i , j -> i j", Tensor([0],"float32"), Tensor([0],"float32"), )

W0303 18:22:24.057368 41007 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:22:24.058346 41007 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997344 (unix time) try "date -d @1740997344" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe2a1c72fc2) received by PID 40188 (TID 0x7fe1c7b85700) from PID 18446744072128769986 ***]

2025-03-03 18:22:28.088539 test begin: paddle.einsum("i , j -> i j", Tensor([0],"float32"), Tensor([2],"float32"), )

W0303 18:22:31.079784 42126 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:22:31.081089 42126 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997351 (unix time) try "date -d @1740997351" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f31aca67fc2) received by PID 41608 (TID 0x7f30dc949700) from PID 18446744072311177154 ***]

2025-03-03 18:22:35.814738 test begin: paddle.einsum("i , j -> i j", Tensor([10],"float32"), Tensor([0],"float32"), )

W0303 18:22:40.434046 43304 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:22:40.434943 43304 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997360 (unix time) try "date -d @1740997360" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9f6c89dfc2) received by PID 42654 (TID 0x7f9ea0949700) from PID 1820975042 ***]

2025-03-03 18:23:14.312989 test begin: paddle.einsum("i , j -> i j", Tensor([1],"float32"), Tensor([0],"float32"), )

W0303 18:23:18.624991 45062 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:23:18.626761 45062 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997398 (unix time) try "date -d @1740997398" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe52d9fffc2) received by PID 43862 (TID 0x7fe441935700) from PID 765460418 ***]

2025-03-03 18:23:23.702326 test begin: paddle.einsum("i, i", Tensor([0],"float64"), Tensor([0],"float64"), )

W0303 18:23:26.428043 46268 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:23:26.429080 46268 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997406 (unix time) try "date -d @1740997406" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f9aef48ec32) received by PID 45625 (TID 0x7f99eb4f4700) from PID 18446744073429118002 ***]

2025-03-03 18:23:32.757757 test begin: paddle.einsum("i, i", Tensor([0],"float64"), Tensor([1],"float64"), )

W0303 18:24:04.795593 47365 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:24:04.796555 47365 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997444 (unix time) try "date -d @1740997444" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb5a8aedc32) received by PID 46617 (TID 0x7fb4c0949700) from PID 18446744072244616242 ***]

2025-03-03 18:24:09.317168 test begin: paddle.einsum("i, i", Tensor([2],"float64"), Tensor([0],"float64"), )

W0303 18:24:17.199704 49199 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:24:17.200657 49199 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("i, i", Tensor([2],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:24:17.202108 test begin: paddle.einsum("i,d->id", Tensor([0],"float32"), Tensor([0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997457 (unix time) try "date -d @1740997457" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fe749695fc2) received by PID 48570 (TID 0x7fe64d6f8700) from PID 1231642562 ***]

2025-03-03 18:24:52.919900 test begin: paddle.einsum("i,d->id", Tensor([0],"float32"), Tensor([16],"float32"), )

W0303 18:24:56.047856 51163 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:24:56.049008 51163 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997496 (unix time) try "date -d @1740997496" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb0ce425fc2) received by PID 49840 (TID 0x7fafb1dc2700) from PID 18446744072875040706 ***]

2025-03-03 18:25:00.430259 test begin: paddle.einsum("i,d->id", Tensor([14],"float32"), Tensor([0],"float32"), )

W0303 18:25:03.728780 52952 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:25:03.729815 52952 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997503 (unix time) try "date -d @1740997503" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f71e01f0fc2) received by PID 51932 (TID 0x7f70f5f48700) from PID 18446744073174716354 ***]

2025-03-03 18:25:08.136090 test begin: paddle.einsum("i,d->id", Tensor([16],"float32"), Tensor([0],"float32"), )

W0303 18:25:41.754302 53604 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:25:41.755425 53604 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997541 (unix time) try "date -d @1740997541" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f580a499fc2) received by PID 53135 (TID 0x7f573bdc2700) from PID 172597186 ***]

2025-03-03 18:25:46.187680 test begin: paddle.einsum("i,i", Tensor([0],"float64"), Tensor([0],"float64"), )

W0303 18:25:50.307875 55320 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:25:50.309101 55320 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997550 (unix time) try "date -d @1740997550" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3afe64ec32) received by PID 54733 (TID 0x7f3a25dc2700) from PID 18446744073682611250 ***]

2025-03-03 18:25:55.035694 test begin: paddle.einsum("i,i", Tensor([0],"float64"), Tensor([10],"float64"), )

W0303 18:26:28.186033 56742 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:26:28.188143 56742 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997588 (unix time) try "date -d @1740997588" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fa2b49e1c32) received by PID 55949 (TID 0x7fa18a949700) from PID 18446744072444845106 ***]

2025-03-03 18:26:32.632967 test begin: paddle.einsum("i,i", Tensor([0],"float64"), Tensor([11],"float64"), )

W0303 18:26:35.745167 58400 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:26:35.746320 58400 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997595 (unix time) try "date -d @1740997595" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f5cd1bbec32) received by PID 58046 (TID 0x7f5bc1b85700) from PID 18446744072933338162 ***]

2025-03-03 18:26:41.298871 test begin: paddle.einsum("i,i", Tensor([10],"float64"), Tensor([0],"float64"), )

W0303 18:26:44.644956 59593 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:26:44.646261 59593 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("i,i", Tensor([10],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:26:44.647747 test begin: paddle.einsum("i,i", Tensor([11],"float64"), Tensor([0],"float64"), )

[paddle error] paddle.einsum("i,i", Tensor([11],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:11 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:26:44.726112 test begin: paddle.einsum("i,i->", Tensor([0],"float32"), Tensor([0],"float32"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997604 (unix time) try "date -d @1740997604" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd9b0115fc2) received by PID 58933 (TID 0x7fd900680700) from PID 18446744072368512962 ***]

2025-03-03 18:26:49.088380 test begin: paddle.einsum("i,i->", Tensor([0],"float32"), Tensor([5],"float32"), )

W0303 18:26:51.977648 60589 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:26:51.978595 60589 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997611 (unix time) try "date -d @1740997611" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fcf80301fc2) received by PID 60164 (TID 0x7fce69f48700) from PID 18446744071565221826 ***]

2025-03-03 18:27:27.765756 test begin: paddle.einsum("i,i->", Tensor([0],"float64"), Tensor([0],"float64"), )

W0303 18:27:31.384943 62448 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:27:31.385994 62448 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997651 (unix time) try "date -d @1740997651" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6bb51f2c32) received by PID 60831 (TID 0x7f6aa8949700) from PID 18446744072453303346 ***]

2025-03-03 18:27:35.801465 test begin: paddle.einsum("i,i->", Tensor([0],"float64"), Tensor([10],"float64"), )

W0303 18:27:38.396893 64130 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:27:38.397872 64130 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997658 (unix time) try "date -d @1740997658" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f39afbdfc32) received by PID 63205 (TID 0x7f38d5abb700) from PID 18446744072363047986 ***]

2025-03-03 18:27:44.282072 test begin: paddle.einsum("i,i->", Tensor([10],"float64"), Tensor([0],"float64"), )

W0303 18:28:14.984994 64806 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:28:14.985929 64806 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("i,i->", Tensor([10],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:10 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:28:14.987572 test begin: paddle.einsum("i,i->", Tensor([5],"float32"), Tensor([0],"float32"), )

[paddle error] paddle.einsum("i,i->", Tensor([5],"float32"), Tensor([0],"float32"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:28:14.990713 test begin: paddle.einsum("i,i->i", Tensor([0],"float64"), Tensor([0],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997694 (unix time) try "date -d @1740997694" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f1d15bc5c32) received by PID 64257 (TID 0x7f1bf7b85700) from PID 364665906 ***]

2025-03-03 18:28:19.213908 test begin: paddle.einsum("i,i->i", Tensor([0],"float64"), Tensor([5],"float64"), )

W0303 18:28:21.962055 66367 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:28:21.963879 66367 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997701 (unix time) try "date -d @1740997701" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f66be230c32) received by PID 65804 (TID 0x7f65a3dc2700) from PID 18446744072604552242 ***]

2025-03-03 18:28:26.237293 test begin: paddle.einsum("i,i->i", Tensor([5],"float64"), Tensor([0],"float64"), )

W0303 18:28:29.195428 67421 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:28:29.196375 67421 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("i,i->i", Tensor([5],"float64"), Tensor([0],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:5 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:28:29.197742 test begin: paddle.einsum("i,ij->", Tensor([0],"float64"), Tensor([2, 2],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997709 (unix time) try "date -d @1740997709" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f983e4cfc32) received by PID 66561 (TID 0x7f973bdc2700) from PID 1045232690 ***]

2025-03-03 18:29:04.425941 test begin: paddle.einsum("i,ij->", Tensor([2],"float64"), Tensor([0, 2],"float64"), )

W0303 18:29:07.169750 69331 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:29:07.170961 69331 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.einsum("i,ij->", Tensor([2],"float64"), Tensor([0, 2],"float64"), ) 
 (InvalidArgument) Same label have different shapes for label: `i`
  [Hint: Expected (*labelshape)[c] == op_dim[dim_ptr], but received (*labelshape)[c]:2 != op_dim[dim_ptr]:0.] (at ../paddle/phi/kernels/impl/einsum_impl.h:221)

2025-03-03 18:29:07.172419 test begin: paddle.einsum("i,ij->", Tensor([2],"float64"), Tensor([2, 0],"float64"), )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997747 (unix time) try "date -d @1740997747" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f432a08cc32) received by PID 68203 (TID 0x7f422fdc2700) from PID 705219634 ***]

2025-03-03 18:29:11.619099 test begin: paddle.einsum("i,j", Tensor([0],"float64"), Tensor([0],"float64"), )

W0303 18:29:14.757892 70221 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:29:14.758973 70221 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997754 (unix time) try "date -d @1740997754" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f48e4138c32) received by PID 69864 (TID 0x7f47fff48700) from PID 18446744073241070642 ***]

2025-03-03 18:29:19.410350 test begin: paddle.einsum("i,j", Tensor([0],"float64"), Tensor([10],"float64"), )

W0303 18:29:52.889874 71171 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:29:52.890805 71171 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997792 (unix time) try "date -d @1740997792" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff194519c32) received by PID 70615 (TID 0x7ff073f48700) from PID 18446744071902960690 ***]

2025-03-03 18:29:57.381667 test begin: paddle.einsum("i,j", Tensor([0],"float64"), Tensor([11],"float64"), )

W0303 18:30:00.378436 73375 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:30:00.379444 73375 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997800 (unix time) try "date -d @1740997800" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f53dff25c32) received by PID 72299 (TID 0x7f52c7f48700) from PID 18446744073171786802 ***]

2025-03-03 18:30:05.279137 test begin: paddle.einsum("i,j", Tensor([3],"float64"), Tensor([0],"float64"), )

W0303 18:30:38.414664 74800 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:30:38.416227 74800 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997838 (unix time) try "date -d @1740997838" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7efe888bdc32) received by PID 74247 (TID 0x7efd7a949700) from PID 18446744071705451570 ***]

2025-03-03 18:30:51.073716 test begin: paddle.einsum("i,j->ii", Tensor([0],"float64"), Tensor([0],"float64"), )

W0303 18:30:54.137109 77205 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:30:54.138293 77205 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997854 (unix time) try "date -d @1740997854" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6736473c32) received by PID 76630 (TID 0x7f6623dc2700) from PID 910638130 ***]

2025-03-03 18:30:58.537230 test begin: paddle.einsum("i,j->ii", Tensor([0],"float64"), Tensor([2],"float64"), )

W0303 18:31:01.941430 78368 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:31:01.942411 78368 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997861 (unix time) try "date -d @1740997861" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4ac822cc32) received by PID 77635 (TID 0x7f49f3f48700) from PID 18446744072772308018 ***]

2025-03-03 18:31:06.578212 test begin: paddle.einsum("i,j->ii", Tensor([2],"float64"), Tensor([0],"float64"), )

W0303 18:31:09.999616 79297 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:31:10.001250 79297 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997870 (unix time) try "date -d @1740997870" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fdea361ec32) received by PID 78712 (TID 0x7fddc34f4700) from PID 18446744072155687986 ***]

2025-03-03 18:31:14.557503 test begin: paddle.einsum("i,j->ij", Tensor([0],"float32"), Tensor([0],"float32"), )

W0303 18:31:17.770331 80212 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:31:17.771366 80212 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997877 (unix time) try "date -d @1740997877" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0f645aafc2) received by PID 79790 (TID 0x7f0e77f48700) from PID 1683664834 ***]

2025-03-03 18:31:22.014928 test begin: paddle.einsum("i,j->ij", Tensor([0],"float32"), Tensor([128],"float32"), )

W0303 18:31:55.566412 80671 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:31:55.567480 80671 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997915 (unix time) try "date -d @1740997915" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f77b0d59fc2) received by PID 80346 (TID 0x7f76a4949700) from PID 18446744072381374402 ***]

2025-03-03 18:32:00.031907 test begin: paddle.einsum("i,j->ij", Tensor([0],"float32"), Tensor([2],"float32"), )

W0303 18:32:03.223667 82327 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:32:03.224627 82327 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997923 (unix time) try "date -d @1740997923" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7ff89896efc2) received by PID 81844 (TID 0x7ff7ca949700) from PID 18446744071974612930 ***]

2025-03-03 18:32:07.556907 test begin: paddle.einsum("i,j->ij", Tensor([10],"float32"), Tensor([0],"float32"), )

W0303 18:32:10.957346 82790 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:32:10.958334 82790 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<float, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<float, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997930 (unix time) try "date -d @1740997930" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7faf1de7afc2) received by PID 82656 (TID 0x7fae49dc2700) from PID 501723074 ***]

2025-03-03 18:32:15.336651 test begin: paddle.einsum("i->", Tensor([0],"float64"), )

W0303 18:32:48.417940 83188 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:32:48.418921 83188 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997968 (unix time) try "date -d @1740997968" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0a42716c32) received by PID 83065 (TID 0x7f09527c3700) from PID 1114729522 ***]

2025-03-03 18:32:53.037552 test begin: paddle.einsum("i->ii", Tensor([0],"float64"), )

W0303 18:32:55.516767 84048 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:32:55.517772 84048 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740997975 (unix time) try "date -d @1740997975" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f65b29acc32) received by PID 83865 (TID 0x7f64a87c3700) from PID 18446744072411073586 ***]

2025-03-03 18:33:31.102909 test begin: paddle.einsum("i..., i...", Tensor([0, 3, 2],"float64"), Tensor([10],"float64"), )

W0303 18:33:34.506542 85458 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:33:34.507745 85458 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998014 (unix time) try "date -d @1740998014" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f54644abc32) received by PID 84516 (TID 0x7f538ff48700) from PID 1682619442 ***]

2025-03-03 18:33:38.997833 test begin: paddle.einsum("i..., i...", Tensor([0, 3, 2],"float64"), Tensor([1],"float64"), )

W0303 18:33:42.781682 85773 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:33:42.782573 85773 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998022 (unix time) try "date -d @1740998022" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb06715fc32) received by PID 85565 (TID 0x7faf5f237700) from PID 1729494066 ***]

2025-03-03 18:34:17.814360 test begin: paddle.einsum("i..., i...", Tensor([1, 0, 2],"float64"), Tensor([1],"float64"), )

W0303 18:34:21.764247 86527 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:34:21.765488 86527 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998061 (unix time) try "date -d @1740998061" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc7ae572c32) received by PID 85878 (TID 0x7fc6abdc2700) from PID 18446744072339532850 ***]

2025-03-03 18:34:26.145218 test begin: paddle.einsum("i..., i...", Tensor([1, 3, 0],"float64"), Tensor([1],"float64"), )

W0303 18:34:29.031076 87106 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:34:29.032153 87106 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_einsum(_object*, _object*, _object*)
1   einsum_ad_func(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string)
2   paddle::experimental::einsum(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::string const&)
3   phi::KernelImpl<void (*)(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >), &(void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >))>::VariadicCompute(phi::DeviceContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
4   void phi::EinsumKernel<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >)
5   void phi::EinsumKernelImpl<double, phi::GPUContext>(phi::GPUContext const&, std::vector<char, std::allocator<char> > const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::string const&, phi::DenseTensor*, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
6   phi::DenseTensor phi::PerformContraction<double, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, std::vector<std::string, std::allocator<std::string > > const&, std::vector<phi::LabelMap, std::allocator<phi::LabelMap> > const&, std::vector<char, std::allocator<char> > const&, phi::LabelMap const&, phi::LabelMap const&, std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > const&, std::vector<phi::DenseTensor*, std::allocator<phi::DenseTensor*> >, bool)
7   phi::DenseTensor phi::PerformDiagonalAndReduction<double, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::string const&, phi::LabelMap const&, std::vector<char, std::allocator<char> > const&, std::vector<int, std::allocator<int> > const&, phi::LabelMap const&)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998069 (unix time) try "date -d @1740998069" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f807c224c32) received by PID 86710 (TID 0x7f7f59f48700) from PID 2082622514 ***]

2025-03-03 18:34:34.769518 test begin: paddle.einsum("i..., i...", Tensor([1, 3, 2],"float64"), Tensor([0],"float64"), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
W0303 18:35:05.178735 87533 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:35:05.180148 87533 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
2025-03-03 18:36:23.043726 test begin: paddle.einsum("i..., i...", Tensor([10, 0, 2],"float64"), Tensor([10],"float64"), )

[Skip]
2025-03-03 18:36:23.044589 test begin: paddle.einsum("i..., i...", Tensor([10, 3, 0],"float64"), Tensor([10],"float64"), )

[Skip]
2025-03-03 18:36:23.044978 test begin: paddle.einsum("i..., i...", Tensor([10, 3, 2],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.045180 test begin: paddle.einsum("i...->...", Tensor([0, 10, 3, 3],"float64"), )

[Skip]
2025-03-03 18:36:23.045332 test begin: paddle.einsum("i...->...", Tensor([0, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.045471 test begin: paddle.einsum("i...->...", Tensor([0, 11],"float64"), )

[Skip]
2025-03-03 18:36:23.045618 test begin: paddle.einsum("i...->...", Tensor([0, 3, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.045762 test begin: paddle.einsum("i...->...", Tensor([0, 3, 11],"float64"), )

[Skip]
2025-03-03 18:36:23.045893 test begin: paddle.einsum("i...->...", Tensor([2, 0, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.046023 test begin: paddle.einsum("i...->...", Tensor([2, 0, 11],"float64"), )

[Skip]
2025-03-03 18:36:23.046161 test begin: paddle.einsum("i...->...", Tensor([2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.046293 test begin: paddle.einsum("i...->...", Tensor([2, 3, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.046421 test begin: paddle.einsum("i...->...", Tensor([5, 0, 3, 3],"float64"), )

[Skip]
2025-03-03 18:36:23.046549 test begin: paddle.einsum("i...->...", Tensor([5, 10, 0, 3],"float64"), )

[Skip]
2025-03-03 18:36:23.046695 test begin: paddle.einsum("i...->...", Tensor([5, 10, 3, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.046828 test begin: paddle.einsum("i...j, i...j->...", Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.046985 test begin: paddle.einsum("i...j, i...j->...", Tensor([0, 2],"float32"), Tensor([2, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.047143 test begin: paddle.einsum("i...j, i...j->...", Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.047289 test begin: paddle.einsum("i...j, i...j->...", Tensor([2, 0],"float32"), Tensor([2, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.047436 test begin: paddle.einsum("i...j, i...j->...", Tensor([2, 2],"float32"), Tensor([0, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.047586 test begin: paddle.einsum("i...j, i...j->...", Tensor([2, 2],"float32"), Tensor([2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.047735 test begin: paddle.einsum("ibm,hm->ibh", Tensor([0, 10, 32],"float32"), Tensor([32, 32],"float32"), )

[Skip]
2025-03-03 18:36:23.047889 test begin: paddle.einsum("ibm,hm->ibh", Tensor([0, 14, 32],"float32"), Tensor([32, 32],"float32"), )

[Skip]
2025-03-03 18:36:23.048041 test begin: paddle.einsum("ibm,hm->ibh", Tensor([1, 0, 32],"float32"), Tensor([32, 32],"float32"), )

[Skip]
2025-03-03 18:36:23.048189 test begin: paddle.einsum("ibm,hm->ibh", Tensor([1, 14, 0],"float32"), Tensor([32, 32],"float32"), )

[Skip]
2025-03-03 18:36:23.048341 test begin: paddle.einsum("ibm,hm->ibh", Tensor([1, 14, 32],"float32"), Tensor([0, 32],"float32"), )

[Skip]
2025-03-03 18:36:23.048492 test begin: paddle.einsum("ibm,hm->ibh", Tensor([1, 14, 32],"float32"), Tensor([32, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.048648 test begin: paddle.einsum("ibm,hm->ibh", Tensor([7, 0, 32],"float32"), Tensor([32, 32],"float32"), )

[Skip]
2025-03-03 18:36:23.048802 test begin: paddle.einsum("ibm,hm->ibh", Tensor([7, 10, 0],"float32"), Tensor([32, 32],"float32"), )

[Skip]
2025-03-03 18:36:23.048952 test begin: paddle.einsum("ibm,hm->ibh", Tensor([7, 10, 32],"float32"), Tensor([0, 32],"float32"), )

[Skip]
2025-03-03 18:36:23.049100 test begin: paddle.einsum("ibm,hm->ibh", Tensor([7, 10, 32],"float32"), Tensor([32, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.049244 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([0, 10, 4, 8],"float32"), Tensor([0, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.049413 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([0, 10, 4, 8],"float32"), Tensor([14, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.049575 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([0, 10, 4, 8],"float32"), Tensor([7, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.049757 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 0, 4, 8],"float32"), Tensor([14, 0, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.049915 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 0, 4, 8],"float32"), Tensor([14, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.050063 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 0, 4, 8],"float32"), Tensor([7, 0, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.050210 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 0, 4, 8],"float32"), Tensor([7, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.050358 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 0, 8],"float32"), Tensor([14, 10, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.050507 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 0, 8],"float32"), Tensor([14, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.050716 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 0, 8],"float32"), Tensor([7, 10, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.050889 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 0, 8],"float32"), Tensor([7, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.051052 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 0],"float32"), Tensor([14, 10, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.051213 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 0],"float32"), Tensor([14, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.051374 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 0],"float32"), Tensor([7, 10, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.051522 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 0],"float32"), Tensor([7, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.051705 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 8],"float32"), Tensor([0, 10, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.051862 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 8],"float32"), Tensor([14, 0, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.052045 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 8],"float32"), Tensor([14, 10, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.052210 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 8],"float32"), Tensor([14, 10, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.052375 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 8],"float32"), Tensor([7, 0, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.052528 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 8],"float32"), Tensor([7, 10, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.052744 test begin: paddle.einsum("ibnd,jbnd->bnij", Tensor([7, 10, 4, 8],"float32"), Tensor([7, 10, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.052893 test begin: paddle.einsum("ibnd,snd->ibns", Tensor([0, 14, 4, 8],"float32"), Tensor([2, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.053043 test begin: paddle.einsum("ibnd,snd->ibns", Tensor([7, 0, 4, 8],"float32"), Tensor([2, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.053191 test begin: paddle.einsum("ibnd,snd->ibns", Tensor([7, 14, 0, 8],"float32"), Tensor([2, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.053340 test begin: paddle.einsum("ibnd,snd->ibns", Tensor([7, 14, 4, 0],"float32"), Tensor([2, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.053484 test begin: paddle.einsum("ibnd,snd->ibns", Tensor([7, 14, 4, 8],"float32"), Tensor([0, 4, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.053643 test begin: paddle.einsum("ibnd,snd->ibns", Tensor([7, 14, 4, 8],"float32"), Tensor([2, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.053820 test begin: paddle.einsum("ibnd,snd->ibns", Tensor([7, 14, 4, 8],"float32"), Tensor([2, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.053974 test begin: paddle.einsum("ii->", Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.054103 test begin: paddle.einsum("ii->", Tensor([5, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.054227 test begin: paddle.einsum("ij, j", Tensor([0, 10],"float64"), Tensor([10],"float64"), )

[Skip]
2025-03-03 18:36:23.054377 test begin: paddle.einsum("ij, j", Tensor([0, 11],"float64"), Tensor([11],"float64"), )

[Skip]
2025-03-03 18:36:23.054527 test begin: paddle.einsum("ij, j", Tensor([4, 0],"float64"), Tensor([10],"float64"), )

[Skip]
2025-03-03 18:36:23.054687 test begin: paddle.einsum("ij, j", Tensor([4, 0],"float64"), Tensor([11],"float64"), )

[Skip]
2025-03-03 18:36:23.054840 test begin: paddle.einsum("ij, j", Tensor([4, 10],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.054986 test begin: paddle.einsum("ij, j", Tensor([4, 11],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.055127 test begin: paddle.einsum("ij,...i->j...", Tensor([0, 5],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.055272 test begin: paddle.einsum("ij,...i->j...", Tensor([4, 0],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.055418 test begin: paddle.einsum("ij,...i->j...", Tensor([4, 5],"float64"), Tensor([0, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.055563 test begin: paddle.einsum("ij,...i->j...", Tensor([4, 5],"float64"), Tensor([3, 0, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.055730 test begin: paddle.einsum("ij,...i->j...", Tensor([4, 5],"float64"), Tensor([3, 2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.055876 test begin: paddle.einsum("ij,i->", Tensor([0, 2],"float64"), Tensor([2],"float64"), )

[Skip]
2025-03-03 18:36:23.056019 test begin: paddle.einsum("ij,i->", Tensor([2, 0],"float64"), Tensor([2],"float64"), )

[Skip]
2025-03-03 18:36:23.056162 test begin: paddle.einsum("ij,i->", Tensor([2, 2],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.056304 test begin: paddle.einsum("ij,ij->", Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.056447 test begin: paddle.einsum("ij,ij->", Tensor([0, 2],"float32"), Tensor([2, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.056597 test begin: paddle.einsum("ij,ij->", Tensor([2, 0],"float32"), Tensor([2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.056761 test begin: paddle.einsum("ij,ij->", Tensor([2, 0],"float32"), Tensor([2, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.056911 test begin: paddle.einsum("ij,ij->", Tensor([2, 2],"float32"), Tensor([0, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.057055 test begin: paddle.einsum("ij,ij->", Tensor([2, 2],"float32"), Tensor([2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.057201 test begin: paddle.einsum("ij,ij->ij", Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.057346 test begin: paddle.einsum("ij,ij->ij", Tensor([0, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.057491 test begin: paddle.einsum("ij,ij->ij", Tensor([4, 0],"float64"), Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.057674 test begin: paddle.einsum("ij,ij->ij", Tensor([4, 0],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.057823 test begin: paddle.einsum("ij,ij->ij", Tensor([4, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.057966 test begin: paddle.einsum("ij,ij->ij", Tensor([4, 5],"float64"), Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.058106 test begin: paddle.einsum("ij,ij->j", Tensor([0, 2],"float64"), Tensor([0, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.058248 test begin: paddle.einsum("ij,ij->j", Tensor([0, 2],"float64"), Tensor([1, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.058387 test begin: paddle.einsum("ij,ij->j", Tensor([10, 0],"float64"), Tensor([1, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.058529 test begin: paddle.einsum("ij,ij->j", Tensor([10, 0],"float64"), Tensor([1, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.058699 test begin: paddle.einsum("ij,ij->j", Tensor([10, 2],"float64"), Tensor([0, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.058849 test begin: paddle.einsum("ij,ij->j", Tensor([10, 2],"float64"), Tensor([1, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.058993 test begin: paddle.einsum("ij,j", Tensor([0, 1],"float64"), Tensor([1],"float64"), )

[Skip]
2025-03-03 18:36:23.059137 test begin: paddle.einsum("ij,j", Tensor([1, 0],"float64"), Tensor([1],"float64"), )

[Skip]
2025-03-03 18:36:23.059277 test begin: paddle.einsum("ij,j", Tensor([1, 1],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.059413 test begin: paddle.einsum("ij,j->i", Tensor([0, 5],"float64"), Tensor([5],"float64"), )

[Skip]
2025-03-03 18:36:23.059550 test begin: paddle.einsum("ij,j->i", Tensor([4, 0],"float64"), Tensor([5],"float64"), )

[Skip]
2025-03-03 18:36:23.059713 test begin: paddle.einsum("ij,j->i", Tensor([4, 5],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.059853 test begin: paddle.einsum("ij,jk", Tensor([0, 10],"float64"), Tensor([0, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.060000 test begin: paddle.einsum("ij,jk", Tensor([0, 10],"float64"), Tensor([10, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.060143 test begin: paddle.einsum("ij,jk", Tensor([0, 11],"float64"), Tensor([0, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.060285 test begin: paddle.einsum("ij,jk", Tensor([0, 11],"float64"), Tensor([11, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.060422 test begin: paddle.einsum("ij,jk", Tensor([4, 0],"float64"), Tensor([10, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.060561 test begin: paddle.einsum("ij,jk", Tensor([4, 0],"float64"), Tensor([10, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.060719 test begin: paddle.einsum("ij,jk", Tensor([4, 0],"float64"), Tensor([11, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.060869 test begin: paddle.einsum("ij,jk", Tensor([4, 0],"float64"), Tensor([11, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.061009 test begin: paddle.einsum("ij,jk", Tensor([4, 10],"float64"), Tensor([0, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.061157 test begin: paddle.einsum("ij,jk", Tensor([4, 10],"float64"), Tensor([10, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.061303 test begin: paddle.einsum("ij,jk", Tensor([4, 11],"float64"), Tensor([0, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.061443 test begin: paddle.einsum("ij,jk", Tensor([4, 11],"float64"), Tensor([11, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.061580 test begin: paddle.einsum("ij,jk,kl", Tensor([0, 4],"float64"), Tensor([0, 5],"float64"), Tensor([0, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.061756 test begin: paddle.einsum("ij,jk,kl", Tensor([0, 4],"float64"), Tensor([4, 5],"float64"), Tensor([5, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.061920 test begin: paddle.einsum("ij,jk,kl", Tensor([3, 0],"float64"), Tensor([4, 0],"float64"), Tensor([5, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.062088 test begin: paddle.einsum("ij,jk,kl", Tensor([3, 0],"float64"), Tensor([4, 5],"float64"), Tensor([5, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.062252 test begin: paddle.einsum("ij,jk,kl", Tensor([3, 4],"float64"), Tensor([0, 5],"float64"), Tensor([5, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.062416 test begin: paddle.einsum("ij,jk,kl", Tensor([3, 4],"float64"), Tensor([4, 0],"float64"), Tensor([5, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.062578 test begin: paddle.einsum("ij,jk,kl", Tensor([3, 4],"float64"), Tensor([4, 5],"float64"), Tensor([0, 6],"float64"), )

[Skip]
2025-03-03 18:36:23.062754 test begin: paddle.einsum("ij,jk,kl", Tensor([3, 4],"float64"), Tensor([4, 5],"float64"), Tensor([5, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.062977 test begin: paddle.einsum("ij,k->ijk", Tensor([0, 128],"float32"), Tensor([32],"float32"), )

[Skip]
2025-03-03 18:36:23.063163 test begin: paddle.einsum("ij,k->ijk", Tensor([0, 4096],"float32"), Tensor([32],"float32"), )

[Skip]
2025-03-03 18:36:23.063310 test begin: paddle.einsum("ij,k->ijk", Tensor([1, 0],"float32"), Tensor([32],"float32"), )

[Skip]
2025-03-03 18:36:23.063463 test begin: paddle.einsum("ij,k->ijk", Tensor([1, 128],"float32"), Tensor([0],"float32"), )

[Skip]
2025-03-03 18:36:23.063621 test begin: paddle.einsum("ij,k->ijk", Tensor([1, 4096],"float32"), Tensor([0],"float32"), )

[Skip]
2025-03-03 18:36:23.063772 test begin: paddle.einsum("ij,kj->ik", Tensor([0, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.065015 test begin: paddle.einsum("ij,kj->ik", Tensor([0, 5],"float64"), Tensor([2, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.066297 test begin: paddle.einsum("ij,kj->ik", Tensor([4, 0],"float64"), Tensor([2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.066589 test begin: paddle.einsum("ij,kj->ik", Tensor([4, 0],"float64"), Tensor([2, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.066809 test begin: paddle.einsum("ij,kj->ik", Tensor([4, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.066983 test begin: paddle.einsum("ij,kj->ik", Tensor([4, 5],"float64"), Tensor([2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.067147 test begin: paddle.einsum("ij,kl->ijkl", Tensor([0, 5],"float64"), Tensor([0, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.067330 test begin: paddle.einsum("ij,kl->ijkl", Tensor([0, 5],"float64"), Tensor([3, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.067486 test begin: paddle.einsum("ij,kl->ijkl", Tensor([4, 0],"float64"), Tensor([3, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.067714 test begin: paddle.einsum("ij,kl->ijkl", Tensor([4, 0],"float64"), Tensor([3, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.067875 test begin: paddle.einsum("ij,kl->ijkl", Tensor([4, 5],"float64"), Tensor([0, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.068024 test begin: paddle.einsum("ij,kl->ijkl", Tensor([4, 5],"float64"), Tensor([3, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.068211 test begin: paddle.einsum("ij->", Tensor([0, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.068343 test begin: paddle.einsum("ij->", Tensor([2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.068472 test begin: paddle.einsum("ij->i", Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.068611 test begin: paddle.einsum("ij->i", Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.068744 test begin: paddle.einsum("ij->j", Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.069429 test begin: paddle.einsum("ij->j", Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.069679 test begin: paddle.einsum("ij->ji", Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.069817 test begin: paddle.einsum("ij->ji", Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.069947 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([0, 7, 14, 2],"float32"), Tensor([0, 14, 4, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.070106 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([0, 7, 14, 2],"float32"), Tensor([7, 14, 4, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.070263 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 0, 14, 2],"float32"), Tensor([7, 0, 4, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.070448 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 0, 14, 2],"float32"), Tensor([7, 14, 4, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.070728 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 0, 2],"float32"), Tensor([7, 14, 0, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.070914 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 0, 2],"float32"), Tensor([7, 14, 4, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.071084 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 14, 0],"float32"), Tensor([7, 14, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.071246 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 14, 0],"float32"), Tensor([7, 14, 4, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.071407 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 14, 2],"float32"), Tensor([0, 14, 4, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.071566 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 14, 2],"float32"), Tensor([7, 0, 4, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.071749 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 14, 2],"float32"), Tensor([7, 14, 0, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.071908 test begin: paddle.einsum("ijbs,ibns->bnij", Tensor([7, 7, 14, 2],"float32"), Tensor([7, 14, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.072063 test begin: paddle.einsum("iji->j", Tensor([0, 10, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.072201 test begin: paddle.einsum("iji->j", Tensor([5, 0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.072335 test begin: paddle.einsum("iji->j", Tensor([5, 10, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.072472 test begin: paddle.einsum("ijk, ik->jk", Tensor([0, 2, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.072638 test begin: paddle.einsum("ijk, ik->jk", Tensor([4, 0, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.072803 test begin: paddle.einsum("ijk, ik->jk", Tensor([4, 2, 0],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.072960 test begin: paddle.einsum("ijk, ik->jk", Tensor([4, 2, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.073115 test begin: paddle.einsum("ijk, ik->jk", Tensor([4, 2, 5],"float64"), Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.073269 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([0, 10, 3],"float64"), Tensor([0, 3, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.073424 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([0, 10, 3],"float64"), Tensor([3, 3, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.073606 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([3, 0, 3],"float64"), Tensor([3, 0, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.073771 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([3, 0, 3],"float64"), Tensor([3, 3, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.073928 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([3, 10, 0],"float64"), Tensor([3, 3, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.074082 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([3, 10, 0],"float64"), Tensor([3, 3, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.074238 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([3, 10, 3],"float64"), Tensor([0, 3, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.074394 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([3, 10, 3],"float64"), Tensor([3, 0, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.074546 test begin: paddle.einsum("ijk, ikl->ijl", Tensor([3, 10, 3],"float64"), Tensor([3, 3, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.074723 test begin: paddle.einsum("ijk, jil -> kl", Tensor([0, 4, 5],"float64"), Tensor([0, 3, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.074879 test begin: paddle.einsum("ijk, jil -> kl", Tensor([0, 4, 5],"float64"), Tensor([4, 3, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.075032 test begin: paddle.einsum("ijk, jil -> kl", Tensor([3, 0, 5],"float64"), Tensor([4, 0, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.075190 test begin: paddle.einsum("ijk, jil -> kl", Tensor([3, 0, 5],"float64"), Tensor([4, 3, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.075342 test begin: paddle.einsum("ijk, jil -> kl", Tensor([3, 4, 0],"float64"), Tensor([4, 3, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.075493 test begin: paddle.einsum("ijk, jil -> kl", Tensor([3, 4, 0],"float64"), Tensor([4, 3, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.075657 test begin: paddle.einsum("ijk, jil -> kl", Tensor([3, 4, 5],"float64"), Tensor([0, 3, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.075813 test begin: paddle.einsum("ijk, jil -> kl", Tensor([3, 4, 5],"float64"), Tensor([4, 0, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.075966 test begin: paddle.einsum("ijk, jil -> kl", Tensor([3, 4, 5],"float64"), Tensor([4, 3, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.076118 test begin: paddle.einsum("ijk,jk->i", Tensor([0, 4, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.076266 test begin: paddle.einsum("ijk,jk->i", Tensor([3, 0, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.076416 test begin: paddle.einsum("ijk,jk->i", Tensor([3, 4, 0],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.076573 test begin: paddle.einsum("ijk,jk->i", Tensor([3, 4, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.076741 test begin: paddle.einsum("ijk,jk->i", Tensor([3, 4, 5],"float64"), Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.076892 test begin: paddle.einsum("ijk,jk->ij", Tensor([0, 4, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.077045 test begin: paddle.einsum("ijk,jk->ij", Tensor([3, 0, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.077194 test begin: paddle.einsum("ijk,jk->ij", Tensor([3, 4, 0],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.077342 test begin: paddle.einsum("ijk,jk->ij", Tensor([3, 4, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.077491 test begin: paddle.einsum("ijk,jk->ij", Tensor([3, 4, 5],"float64"), Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.077671 test begin: paddle.einsum("ijk,jk->ik", Tensor([0, 4, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.077824 test begin: paddle.einsum("ijk,jk->ik", Tensor([3, 0, 5],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.077971 test begin: paddle.einsum("ijk,jk->ik", Tensor([3, 4, 0],"float64"), Tensor([4, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.078119 test begin: paddle.einsum("ijk,jk->ik", Tensor([3, 4, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.078268 test begin: paddle.einsum("ijk,jk->ik", Tensor([3, 4, 5],"float64"), Tensor([4, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.080812 test begin: paddle.einsum("ijk,lk->ijl", Tensor([0, 4, 5],"float64"), Tensor([2, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.081002 test begin: paddle.einsum("ijk,lk->ijl", Tensor([3, 0, 5],"float64"), Tensor([2, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.081174 test begin: paddle.einsum("ijk,lk->ijl", Tensor([3, 4, 0],"float64"), Tensor([2, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.081328 test begin: paddle.einsum("ijk,lk->ijl", Tensor([3, 4, 5],"float64"), Tensor([0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.081482 test begin: paddle.einsum("ijk,lk->ijl", Tensor([3, 4, 5],"float64"), Tensor([2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.081647 test begin: paddle.einsum("ijk->kji", Tensor([0, 10, 3],"float64"), )

[Skip]
2025-03-03 18:36:23.081799 test begin: paddle.einsum("ijk->kji", Tensor([3, 0, 3],"float64"), )

[Skip]
2025-03-03 18:36:23.081928 test begin: paddle.einsum("ijk->kji", Tensor([3, 10, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.082056 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([0, 5, 7, 3],"float64"), Tensor([0, 7, 5, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.082208 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([0, 5, 7, 3],"float64"), Tensor([5, 7, 5, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.082357 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 0, 7, 3],"float64"), Tensor([5, 0, 5, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.082509 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 0, 7, 3],"float64"), Tensor([5, 7, 5, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.082673 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 5, 0, 3],"float64"), Tensor([5, 7, 0, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.082846 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 5, 0, 3],"float64"), Tensor([5, 7, 5, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.083004 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 5, 7, 0],"float64"), Tensor([5, 7, 5, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.083159 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 5, 7, 0],"float64"), Tensor([5, 7, 5, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.083323 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 5, 7, 3],"float64"), Tensor([0, 7, 5, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.083480 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 5, 7, 3],"float64"), Tensor([5, 0, 5, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.083656 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 5, 7, 3],"float64"), Tensor([5, 7, 0, 7],"float64"), )

[Skip]
2025-03-03 18:36:23.083817 test begin: paddle.einsum("ijki,jkjk->ik", Tensor([3, 5, 7, 3],"float64"), Tensor([5, 7, 5, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.083975 test begin: paddle.einsum("ijkl, lmn->ijn", Tensor([0, 4, 5, 3],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.084131 test begin: paddle.einsum("ijkl, lmn->ijn", Tensor([2, 0, 5, 3],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.084284 test begin: paddle.einsum("ijkl, lmn->ijn", Tensor([2, 4, 0, 3],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.084437 test begin: paddle.einsum("ijkl, lmn->ijn", Tensor([2, 4, 5, 0],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.084610 test begin: paddle.einsum("ijkl, lmn->ijn", Tensor([2, 4, 5, 3],"float64"), Tensor([0, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.084772 test begin: paddle.einsum("ijkl, lmn->ijn", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 0, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.084933 test begin: paddle.einsum("ijkl, lmn->ijn", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.085093 test begin: paddle.einsum("ijkl, lmn->kmn", Tensor([0, 4, 5, 3],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.085243 test begin: paddle.einsum("ijkl, lmn->kmn", Tensor([2, 0, 5, 3],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.085390 test begin: paddle.einsum("ijkl, lmn->kmn", Tensor([2, 4, 0, 3],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.085535 test begin: paddle.einsum("ijkl, lmn->kmn", Tensor([2, 4, 5, 0],"float64"), Tensor([3, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.085696 test begin: paddle.einsum("ijkl, lmn->kmn", Tensor([2, 4, 5, 3],"float64"), Tensor([0, 2, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.085846 test begin: paddle.einsum("ijkl, lmn->kmn", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 0, 4],"float64"), )

[Skip]
2025-03-03 18:36:23.085993 test begin: paddle.einsum("ijkl, lmn->kmn", Tensor([2, 4, 5, 3],"float64"), Tensor([3, 2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.086139 test begin: paddle.einsum("ik, ijk->j", Tensor([0, 5],"float64"), Tensor([4, 2, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.086290 test begin: paddle.einsum("ik, ijk->j", Tensor([4, 0],"float64"), Tensor([4, 2, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.086434 test begin: paddle.einsum("ik, ijk->j", Tensor([4, 5],"float64"), Tensor([0, 2, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.086579 test begin: paddle.einsum("ik, ijk->j", Tensor([4, 5],"float64"), Tensor([4, 0, 5],"float64"), )

[Skip]
2025-03-03 18:36:23.086750 test begin: paddle.einsum("ik, ijk->j", Tensor([4, 5],"float64"), Tensor([4, 2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.086897 test begin: paddle.einsum("iox,ojx->ijx", Tensor([0, 3, 1],"complex64"), Tensor([0, 2, 1],"complex64"), )

[Skip]
2025-03-03 18:36:23.087046 test begin: paddle.einsum("iox,ojx->ijx", Tensor([0, 3, 1],"complex64"), Tensor([3, 2, 1],"complex64"), )

[Skip]
2025-03-03 18:36:23.087206 test begin: paddle.einsum("iox,ojx->ijx", Tensor([2, 0, 1],"complex64"), Tensor([3, 0, 1],"complex64"), )

[Skip]
2025-03-03 18:36:23.087361 test begin: paddle.einsum("iox,ojx->ijx", Tensor([2, 0, 1],"complex64"), Tensor([3, 2, 1],"complex64"), )

[Skip]
2025-03-03 18:36:23.087515 test begin: paddle.einsum("iox,ojx->ijx", Tensor([2, 3, 0],"complex64"), Tensor([3, 2, 0],"complex64"), )

[Skip]
2025-03-03 18:36:23.087683 test begin: paddle.einsum("iox,ojx->ijx", Tensor([2, 3, 0],"complex64"), Tensor([3, 2, 1],"complex64"), )

[Skip]
2025-03-03 18:36:23.087838 test begin: paddle.einsum("iox,ojx->ijx", Tensor([2, 3, 1],"complex64"), Tensor([0, 2, 1],"complex64"), )

[Skip]
2025-03-03 18:36:23.087994 test begin: paddle.einsum("iox,ojx->ijx", Tensor([2, 3, 1],"complex64"), Tensor([3, 0, 1],"complex64"), )

[Skip]
2025-03-03 18:36:23.088145 test begin: paddle.einsum("iox,ojx->ijx", Tensor([2, 3, 1],"complex64"), Tensor([3, 2, 0],"complex64"), )

[Skip]
2025-03-03 18:36:23.088305 test begin: paddle.einsum("ji,i->", Tensor([0, 2],"float64"), Tensor([2],"float64"), )

[Skip]
2025-03-03 18:36:23.088458 test begin: paddle.einsum("ji,i->", Tensor([2, 0],"float64"), Tensor([2],"float64"), )

[Skip]
2025-03-03 18:36:23.088614 test begin: paddle.einsum("ji,i->", Tensor([2, 2],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.088770 test begin: paddle.einsum("ji,j", Tensor([0, 4],"float64"), Tensor([10],"float64"), )

[Skip]
2025-03-03 18:36:23.088922 test begin: paddle.einsum("ji,j", Tensor([0, 4],"float64"), Tensor([1],"float64"), )

[Skip]
2025-03-03 18:36:23.089075 test begin: paddle.einsum("ji,j", Tensor([1, 0],"float64"), Tensor([1],"float64"), )

[Skip]
2025-03-03 18:36:23.089224 test begin: paddle.einsum("ji,j", Tensor([1, 4],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.089369 test begin: paddle.einsum("ji,j", Tensor([10, 0],"float64"), Tensor([10],"float64"), )

[Skip]
2025-03-03 18:36:23.089516 test begin: paddle.einsum("ji,j", Tensor([10, 4],"float64"), Tensor([0],"float64"), )

[Skip]
2025-03-03 18:36:23.089672 test begin: paddle.einsum("k...,...jk->...k", Tensor([0, 3, 2, 3, 4],"float64"), Tensor([12, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.089835 test begin: paddle.einsum("k...,...jk->...k", Tensor([10, 0, 2, 3, 4],"float64"), Tensor([12, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.089995 test begin: paddle.einsum("k...,...jk->...k", Tensor([10, 3, 0, 3, 4],"float64"), Tensor([12, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.090152 test begin: paddle.einsum("k...,...jk->...k", Tensor([10, 3, 2, 0, 4],"float64"), Tensor([12, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.090309 test begin: paddle.einsum("k...,...jk->...k", Tensor([10, 3, 2, 3, 0],"float64"), Tensor([12, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.090462 test begin: paddle.einsum("k...,...jk->...k", Tensor([10, 3, 2, 3, 4],"float64"), Tensor([0, 10],"float64"), )

[Skip]
2025-03-03 18:36:23.090628 test begin: paddle.einsum("k...,...jk->...k", Tensor([10, 3, 2, 3, 4],"float64"), Tensor([12, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.090787 test begin: paddle.einsum("k...,jk", Tensor([0, 4, 5, 3],"float64"), Tensor([2, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.090936 test begin: paddle.einsum("k...,jk", Tensor([2, 0, 5, 3],"float64"), Tensor([2, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.091088 test begin: paddle.einsum("k...,jk", Tensor([2, 4, 0, 3],"float64"), Tensor([2, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.091233 test begin: paddle.einsum("k...,jk", Tensor([2, 4, 5, 0],"float64"), Tensor([2, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.091376 test begin: paddle.einsum("k...,jk", Tensor([2, 4, 5, 3],"float64"), Tensor([0, 2],"float64"), )

[Skip]
2025-03-03 18:36:23.091525 test begin: paddle.einsum("k...,jk", Tensor([2, 4, 5, 3],"float64"), Tensor([2, 0],"float64"), )

[Skip]
2025-03-03 18:36:23.091683 test begin: paddle.einsum("lbnd,mlb->mbnd", Tensor([0, 14, 4, 8],"float32"), Tensor([1, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.091836 test begin: paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 0, 4, 8],"float32"), Tensor([1, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.091995 test begin: paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 14, 0, 8],"float32"), Tensor([1, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.092144 test begin: paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 14, 4, 0],"float32"), Tensor([1, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.092295 test begin: paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 14, 4, 8],"float32"), Tensor([0, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.092446 test begin: paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 14, 4, 8],"float32"), Tensor([1, 0, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.092600 test begin: paddle.einsum("lbnd,mlb->mbnd", Tensor([8, 14, 4, 8],"float32"), Tensor([1, 8, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.092755 test begin: paddle.einsum("m,d->md", Tensor([0],"float32"), Tensor([0],"float32"), )

[Skip]
2025-03-03 18:36:23.092909 test begin: paddle.einsum("m,d->md", Tensor([0],"float32"), Tensor([192],"float32"), )

[Skip]
2025-03-03 18:36:23.093059 test begin: paddle.einsum("m,d->md", Tensor([196],"float32"), Tensor([0],"float32"), )

[Skip]
2025-03-03 18:36:23.093202 test begin: paddle.einsum("mbnd,mlb->lbnd", Tensor([0, 14, 4, 8],"float32"), Tensor([1, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.093360 test begin: paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 0, 4, 8],"float32"), Tensor([1, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.093515 test begin: paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 0, 8],"float32"), Tensor([1, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.093712 test begin: paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 4, 0],"float32"), Tensor([1, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.093865 test begin: paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 4, 8],"float32"), Tensor([0, 8, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.094014 test begin: paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 4, 8],"float32"), Tensor([1, 0, 14],"float32"), )

[Skip]
2025-03-03 18:36:23.094161 test begin: paddle.einsum("mbnd,mlb->lbnd", Tensor([1, 14, 4, 8],"float32"), Tensor([1, 8, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.094309 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([0, 3, 4, 2],"float32"), Tensor([2, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.094475 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([0, 3, 5, 6],"float16"), Tensor([6, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.094642 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 0, 4, 2],"float32"), Tensor([2, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.094800 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 0, 5, 6],"float16"), Tensor([6, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.094952 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 0, 2],"float32"), Tensor([2, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.095108 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 0, 6],"float16"), Tensor([6, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.095261 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 4, 0],"float32"), Tensor([2, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.095412 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 4, 2],"float32"), Tensor([0, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.095562 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 4, 2],"float32"), Tensor([2, 0, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.095730 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 4, 2],"float32"), Tensor([2, 2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.095889 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 5, 0],"float16"), Tensor([6, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.096034 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 5, 6],"float16"), Tensor([0, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.096183 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 5, 6],"float16"), Tensor([6, 0, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.096329 test begin: paddle.einsum("nbka,ahc->nbkhc", Tensor([1, 3, 5, 6],"float16"), Tensor([6, 2, 0],"float16"), )

[Skip]
2025-03-03 18:36:23.096475 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([0, 3, 5, 6],"float16"), Tensor([6, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.096631 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([0, 3, 5, 6],"float32"), Tensor([6, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.096787 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 0, 5, 6],"float16"), Tensor([6, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.096943 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 0, 5, 6],"float32"), Tensor([6, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.097097 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 0, 6],"float16"), Tensor([6, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.097243 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 0, 6],"float32"), Tensor([6, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.097398 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 0],"float16"), Tensor([6, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.097548 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 0],"float32"), Tensor([6, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.097707 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float16"), Tensor([0, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.097858 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float16"), Tensor([6, 0, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.098009 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float16"), Tensor([6, 2, 0],"float16"), )

[Skip]
2025-03-03 18:36:23.098157 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float32"), Tensor([0, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.098311 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float32"), Tensor([6, 0, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.098459 test begin: paddle.einsum("nbqa,ahc->nbqhc", Tensor([1, 3, 5, 6],"float32"), Tensor([6, 2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.098624 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([0, 3, 5, 2, 4],"float16"), Tensor([0, 3, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.098788 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([0, 3, 5, 2, 4],"float16"), Tensor([1, 3, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.098943 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([0, 3, 5, 2, 4],"float32"), Tensor([0, 3, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.099095 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([0, 3, 5, 2, 4],"float32"), Tensor([1, 3, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.099248 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 0, 5, 2, 4],"float16"), Tensor([1, 0, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.099400 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 0, 5, 2, 4],"float16"), Tensor([1, 3, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.099553 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 0, 5, 2, 4],"float32"), Tensor([1, 0, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.099721 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 0, 5, 2, 4],"float32"), Tensor([1, 3, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.099878 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 0, 2, 4],"float16"), Tensor([1, 3, 0, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.100034 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 0, 2, 4],"float16"), Tensor([1, 3, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.100191 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 0, 2, 4],"float32"), Tensor([1, 3, 0, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.100355 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 0, 2, 4],"float32"), Tensor([1, 3, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.100513 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 0, 4],"float16"), Tensor([1, 3, 5, 0, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.100700 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 0, 4],"float16"), Tensor([1, 3, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.100861 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 0, 4],"float32"), Tensor([1, 3, 4, 0, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.101018 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 0, 4],"float32"), Tensor([1, 3, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.101174 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 0],"float16"), Tensor([1, 3, 5, 2, 0],"float16"), )

[Skip]
2025-03-03 18:36:23.101336 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 0],"float16"), Tensor([1, 3, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.101494 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 0],"float32"), Tensor([1, 3, 4, 2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.101677 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 0],"float32"), Tensor([1, 3, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.101833 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float16"), Tensor([0, 3, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.101994 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float16"), Tensor([1, 0, 5, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.102147 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float16"), Tensor([1, 3, 0, 2, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.102301 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float16"), Tensor([1, 3, 5, 0, 4],"float16"), )

[Skip]
2025-03-03 18:36:23.102455 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float16"), Tensor([1, 3, 5, 2, 0],"float16"), )

[Skip]
2025-03-03 18:36:23.102619 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float32"), Tensor([0, 3, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.102790 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float32"), Tensor([1, 0, 4, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.102951 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float32"), Tensor([1, 3, 0, 2, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.103109 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float32"), Tensor([1, 3, 4, 0, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.103274 test begin: paddle.einsum("nbqhc,nbkhc->nbhqk", Tensor([1, 3, 5, 2, 4],"float32"), Tensor([1, 3, 4, 2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.103430 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([0, 52, 7, 8],"float32"), Tensor([0, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.103596 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([0, 52, 7, 8],"float32"), Tensor([2, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.103762 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 0, 7, 8],"float32"), Tensor([2, 0, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.103919 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 0, 7, 8],"float32"), Tensor([2, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.104071 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 0, 8],"float32"), Tensor([2, 52, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.104226 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 0, 8],"float32"), Tensor([2, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.104377 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 7, 0],"float32"), Tensor([2, 52, 14, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.104530 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 7, 0],"float32"), Tensor([2, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.104713 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 7, 8],"float32"), Tensor([0, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.104875 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 7, 8],"float32"), Tensor([2, 0, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.105037 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 7, 8],"float32"), Tensor([2, 52, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.105195 test begin: paddle.einsum("nbtc,nbsc->nbts", Tensor([2, 52, 7, 8],"float32"), Tensor([2, 52, 14, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.105352 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([0, 52, 7, 14],"float32"), Tensor([0, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.105505 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([0, 52, 7, 14],"float32"), Tensor([2, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.105742 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 0, 7, 14],"float32"), Tensor([2, 0, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.105901 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 0, 7, 14],"float32"), Tensor([2, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.106067 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 0, 14],"float32"), Tensor([2, 52, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.106222 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 0, 14],"float32"), Tensor([2, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.106376 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 7, 0],"float32"), Tensor([2, 52, 14, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.106532 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 7, 0],"float32"), Tensor([2, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.106708 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 7, 14],"float32"), Tensor([0, 52, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.106870 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 7, 14],"float32"), Tensor([2, 0, 14, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.107026 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 7, 14],"float32"), Tensor([2, 52, 0, 8],"float32"), )

[Skip]
2025-03-03 18:36:23.107172 test begin: paddle.einsum("nbts,nbsc->nbtc", Tensor([2, 52, 7, 14],"float32"), Tensor([2, 52, 14, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.107318 test begin: paddle.einsum("se,sec->sec", Tensor([0, 60],"float32"), Tensor([10, 60, 10],"float32"), )

[Skip]
2025-03-03 18:36:23.107466 test begin: paddle.einsum("se,sec->sec", Tensor([0, 60],"float32"), Tensor([2, 60, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.107629 test begin: paddle.einsum("se,sec->sec", Tensor([10, 0],"float32"), Tensor([10, 60, 10],"float32"), )

[Skip]
2025-03-03 18:36:23.107782 test begin: paddle.einsum("se,sec->sec", Tensor([10, 60],"float32"), Tensor([0, 60, 10],"float32"), )

[Skip]
2025-03-03 18:36:23.107933 test begin: paddle.einsum("se,sec->sec", Tensor([10, 60],"float32"), Tensor([10, 0, 10],"float32"), )

[Skip]
2025-03-03 18:36:23.108085 test begin: paddle.einsum("se,sec->sec", Tensor([10, 60],"float32"), Tensor([10, 60, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.108237 test begin: paddle.einsum("se,sec->sec", Tensor([2, 0],"float32"), Tensor([2, 60, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.108384 test begin: paddle.einsum("se,sec->sec", Tensor([2, 60],"float32"), Tensor([0, 60, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.108529 test begin: paddle.einsum("se,sec->sec", Tensor([2, 60],"float32"), Tensor([2, 0, 2],"float32"), )

[Skip]
2025-03-03 18:36:23.108689 test begin: paddle.einsum("se,sec->sec", Tensor([2, 60],"float32"), Tensor([2, 60, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.108838 test begin: paddle.einsum("sec,ecm->sm", Tensor([0, 60, 10],"float32"), Tensor([0, 10, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.108988 test begin: paddle.einsum("sec,ecm->sm", Tensor([0, 60, 10],"float32"), Tensor([60, 10, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.109139 test begin: paddle.einsum("sec,ecm->sm", Tensor([0, 60, 2],"float32"), Tensor([0, 2, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.109287 test begin: paddle.einsum("sec,ecm->sm", Tensor([0, 60, 2],"float32"), Tensor([60, 2, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.109440 test begin: paddle.einsum("sec,ecm->sm", Tensor([10, 0, 10],"float32"), Tensor([60, 0, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.109599 test begin: paddle.einsum("sec,ecm->sm", Tensor([10, 0, 10],"float32"), Tensor([60, 10, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.109757 test begin: paddle.einsum("sec,ecm->sm", Tensor([10, 60, 0],"float32"), Tensor([60, 10, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.109902 test begin: paddle.einsum("sec,ecm->sm", Tensor([10, 60, 0],"float32"), Tensor([60, 10, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.110046 test begin: paddle.einsum("sec,ecm->sm", Tensor([10, 60, 10],"float32"), Tensor([0, 10, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.110193 test begin: paddle.einsum("sec,ecm->sm", Tensor([10, 60, 10],"float32"), Tensor([60, 0, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.110343 test begin: paddle.einsum("sec,ecm->sm", Tensor([10, 60, 10],"float32"), Tensor([60, 10, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.110503 test begin: paddle.einsum("sec,ecm->sm", Tensor([2, 0, 2],"float32"), Tensor([60, 0, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.110676 test begin: paddle.einsum("sec,ecm->sm", Tensor([2, 0, 2],"float32"), Tensor([60, 2, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.110829 test begin: paddle.einsum("sec,ecm->sm", Tensor([2, 60, 0],"float32"), Tensor([60, 2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.110978 test begin: paddle.einsum("sec,ecm->sm", Tensor([2, 60, 0],"float32"), Tensor([60, 2, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.111126 test begin: paddle.einsum("sec,ecm->sm", Tensor([2, 60, 2],"float32"), Tensor([0, 2, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.111282 test begin: paddle.einsum("sec,ecm->sm", Tensor([2, 60, 2],"float32"), Tensor([60, 0, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.111432 test begin: paddle.einsum("sec,ecm->sm", Tensor([2, 60, 2],"float32"), Tensor([60, 2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.111600 test begin: paddle.einsum("sec,sm->ecm", Tensor([0, 60, 10],"float32"), Tensor([10, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.111771 test begin: paddle.einsum("sec,sm->ecm", Tensor([0, 60, 2],"float32"), Tensor([2, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.111921 test begin: paddle.einsum("sec,sm->ecm", Tensor([10, 0, 10],"float32"), Tensor([10, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.112068 test begin: paddle.einsum("sec,sm->ecm", Tensor([10, 60, 0],"float32"), Tensor([10, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.112215 test begin: paddle.einsum("sec,sm->ecm", Tensor([10, 60, 10],"float32"), Tensor([0, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.112364 test begin: paddle.einsum("sec,sm->ecm", Tensor([10, 60, 10],"float32"), Tensor([10, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.112513 test begin: paddle.einsum("sec,sm->ecm", Tensor([2, 0, 2],"float32"), Tensor([2, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.112702 test begin: paddle.einsum("sec,sm->ecm", Tensor([2, 60, 0],"float32"), Tensor([2, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.112858 test begin: paddle.einsum("sec,sm->ecm", Tensor([2, 60, 2],"float32"), Tensor([0, 64],"float32"), )

[Skip]
2025-03-03 18:36:23.113005 test begin: paddle.einsum("sec,sm->ecm", Tensor([2, 60, 2],"float32"), Tensor([2, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.113150 test begin: paddle.einsum("td,dnh->tnh", Tensor([0, 32],"float32"), Tensor([32, 4, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.113293 test begin: paddle.einsum("td,dnh->tnh", Tensor([14, 0],"float32"), Tensor([32, 4, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.113438 test begin: paddle.einsum("td,dnh->tnh", Tensor([14, 32],"float32"), Tensor([0, 4, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.113616 test begin: paddle.einsum("td,dnh->tnh", Tensor([14, 32],"float32"), Tensor([32, 0, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.113812 test begin: paddle.einsum("td,dnh->tnh", Tensor([14, 32],"float32"), Tensor([32, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.113966 test begin: paddle.einsum("td,dnh->tnh", Tensor([15, 0],"float32"), Tensor([32, 4, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.114114 test begin: paddle.einsum("td,dnh->tnh", Tensor([15, 32],"float32"), Tensor([0, 4, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.114273 test begin: paddle.einsum("td,dnh->tnh", Tensor([15, 32],"float32"), Tensor([32, 0, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.114509 test begin: paddle.einsum("td,dnh->tnh", Tensor([15, 32],"float32"), Tensor([32, 4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.114684 test begin: paddle.einsum("xy,yz->xz", Tensor([0, 4],"complex64"), Tensor([0, 4],"complex64"), )

[Skip]
2025-03-03 18:36:23.114842 test begin: paddle.einsum("xy,yz->xz", Tensor([0, 4],"complex64"), Tensor([4, 4],"complex64"), )

[Skip]
2025-03-03 18:36:23.114988 test begin: paddle.einsum("xy,yz->xz", Tensor([0, 4],"float32"), Tensor([0, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.115138 test begin: paddle.einsum("xy,yz->xz", Tensor([0, 4],"float32"), Tensor([4, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.115301 test begin: paddle.einsum("xy,yz->xz", Tensor([4, 0],"complex64"), Tensor([4, 0],"complex64"), )

[Skip]
2025-03-03 18:36:23.115453 test begin: paddle.einsum("xy,yz->xz", Tensor([4, 0],"complex64"), Tensor([4, 4],"complex64"), )

[Skip]
2025-03-03 18:36:23.115608 test begin: paddle.einsum("xy,yz->xz", Tensor([4, 0],"float32"), Tensor([4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.115758 test begin: paddle.einsum("xy,yz->xz", Tensor([4, 0],"float32"), Tensor([4, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.115907 test begin: paddle.einsum("xy,yz->xz", Tensor([4, 4],"complex64"), Tensor([0, 4],"complex64"), )

[Skip]
2025-03-03 18:36:23.116050 test begin: paddle.einsum("xy,yz->xz", Tensor([4, 4],"complex64"), Tensor([4, 0],"complex64"), )

[Skip]
2025-03-03 18:36:23.116197 test begin: paddle.einsum("xy,yz->xz", Tensor([4, 4],"float32"), Tensor([0, 4],"float32"), )

[Skip]
2025-03-03 18:36:23.116340 test begin: paddle.einsum("xy,yz->xz", Tensor([4, 4],"float32"), Tensor([4, 0],"float32"), )

[Skip]
2025-03-03 18:36:23.116482 test begin: paddle.equal_all(Tensor([0, 1, 2],"int64"), Tensor([0, 1, 2],"int64"), )

W0303 18:36:26.033516 90858 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:36:26.034824 90858 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.equal_all(Tensor([0, 1, 2],"int64"), Tensor([0, 1, 2],"int64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.040370 test begin: paddle.equal_all(Tensor([0, 1, 2],"int64"), Tensor([2, 1, 2],"int64"), )

[Pass] paddle.equal_all(Tensor([0, 1, 2],"int64"), Tensor([2, 1, 2],"int64"), )
2025-03-03 18:36:26.048830 test begin: paddle.equal_all(Tensor([0, 1, 4],"int64"), Tensor([0, 1, 4],"int64"), )

[paddle error] paddle.equal_all(Tensor([0, 1, 4],"int64"), Tensor([0, 1, 4],"int64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.050769 test begin: paddle.equal_all(Tensor([0, 1, 4],"int64"), Tensor([2, 1, 4],"int64"), )

[Pass] paddle.equal_all(Tensor([0, 1, 4],"int64"), Tensor([2, 1, 4],"int64"), )
2025-03-03 18:36:26.052305 test begin: paddle.equal_all(Tensor([0, 10],"float32"), Tensor([0, 10],"float32"), )

[paddle error] paddle.equal_all(Tensor([0, 10],"float32"), Tensor([0, 10],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.053549 test begin: paddle.equal_all(Tensor([0, 10],"float32"), Tensor([1, 10],"float32"), )

[Pass] paddle.equal_all(Tensor([0, 10],"float32"), Tensor([1, 10],"float32"), )
2025-03-03 18:36:26.054800 test begin: paddle.equal_all(Tensor([0, 2, 10, 16],"bool"), Tensor([0, 2, 10, 16],"bool"), )

[paddle error] paddle.equal_all(Tensor([0, 2, 10, 16],"bool"), Tensor([0, 2, 10, 16],"bool"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.055898 test begin: paddle.equal_all(Tensor([0, 2, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )

[Pass] paddle.equal_all(Tensor([0, 2, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )
2025-03-03 18:36:26.056984 test begin: paddle.equal_all(Tensor([0, 2, 3, 4],"float32"), Tensor([0, 2, 3, 4],"float32"), )

[paddle error] paddle.equal_all(Tensor([0, 2, 3, 4],"float32"), Tensor([0, 2, 3, 4],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.057977 test begin: paddle.equal_all(Tensor([0, 2, 3, 4],"float32"), Tensor([1, 2, 3, 4],"float32"), )

[Pass] paddle.equal_all(Tensor([0, 2, 3, 4],"float32"), Tensor([1, 2, 3, 4],"float32"), )
2025-03-03 18:36:26.059748 test begin: paddle.equal_all(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), )

[paddle error] paddle.equal_all(Tensor([0, 2],"float32"), Tensor([0, 2],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.060781 test begin: paddle.equal_all(Tensor([0, 2],"float32"), Tensor([1, 2],"float32"), )

[Pass] paddle.equal_all(Tensor([0, 2],"float32"), Tensor([1, 2],"float32"), )
2025-03-03 18:36:26.061847 test begin: paddle.equal_all(Tensor([0],"float32"), Tensor([0],"float32"), )

[paddle error] paddle.equal_all(Tensor([0],"float32"), Tensor([0],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.062853 test begin: paddle.equal_all(Tensor([0],"float32"), Tensor([128],"float32"), )

[Pass] paddle.equal_all(Tensor([0],"float32"), Tensor([128],"float32"), )
2025-03-03 18:36:26.063923 test begin: paddle.equal_all(Tensor([0],"float32"), Tensor([16],"float32"), )

[Pass] paddle.equal_all(Tensor([0],"float32"), Tensor([16],"float32"), )
2025-03-03 18:36:26.064918 test begin: paddle.equal_all(Tensor([1, 0, 10, 16],"bool"), Tensor([1, 0, 10, 16],"bool"), )

[paddle error] paddle.equal_all(Tensor([1, 0, 10, 16],"bool"), Tensor([1, 0, 10, 16],"bool"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.065846 test begin: paddle.equal_all(Tensor([1, 0, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )

[Pass] paddle.equal_all(Tensor([1, 0, 10, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )
2025-03-03 18:36:26.066933 test begin: paddle.equal_all(Tensor([1, 0, 3, 4],"float32"), Tensor([1, 0, 3, 4],"float32"), )

[paddle error] paddle.equal_all(Tensor([1, 0, 3, 4],"float32"), Tensor([1, 0, 3, 4],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.069285 test begin: paddle.equal_all(Tensor([1, 0, 3, 4],"float32"), Tensor([1, 2, 3, 4],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 0, 3, 4],"float32"), Tensor([1, 2, 3, 4],"float32"), )
2025-03-03 18:36:26.070908 test begin: paddle.equal_all(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), )

[paddle error] paddle.equal_all(Tensor([1, 0],"float32"), Tensor([1, 0],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.071826 test begin: paddle.equal_all(Tensor([1, 0],"float32"), Tensor([1, 10],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 0],"float32"), Tensor([1, 10],"float32"), )
2025-03-03 18:36:26.072954 test begin: paddle.equal_all(Tensor([1, 0],"float32"), Tensor([1, 2],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 0],"float32"), Tensor([1, 2],"float32"), )
2025-03-03 18:36:26.074152 test begin: paddle.equal_all(Tensor([1, 10],"float32"), Tensor([0, 10],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 10],"float32"), Tensor([0, 10],"float32"), )
2025-03-03 18:36:26.075183 test begin: paddle.equal_all(Tensor([1, 10],"float32"), Tensor([1, 0],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 10],"float32"), Tensor([1, 0],"float32"), )
2025-03-03 18:36:26.076288 test begin: paddle.equal_all(Tensor([1, 2, 0, 16],"bool"), Tensor([1, 2, 0, 16],"bool"), )

[paddle error] paddle.equal_all(Tensor([1, 2, 0, 16],"bool"), Tensor([1, 2, 0, 16],"bool"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.077272 test begin: paddle.equal_all(Tensor([1, 2, 0, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )

[Pass] paddle.equal_all(Tensor([1, 2, 0, 16],"bool"), Tensor([1, 2, 10, 16],"bool"), )
2025-03-03 18:36:26.078344 test begin: paddle.equal_all(Tensor([1, 2, 0, 4],"float32"), Tensor([1, 2, 0, 4],"float32"), )

[paddle error] paddle.equal_all(Tensor([1, 2, 0, 4],"float32"), Tensor([1, 2, 0, 4],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.079179 test begin: paddle.equal_all(Tensor([1, 2, 0, 4],"float32"), Tensor([1, 2, 3, 4],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 2, 0, 4],"float32"), Tensor([1, 2, 3, 4],"float32"), )
2025-03-03 18:36:26.080209 test begin: paddle.equal_all(Tensor([1, 2, 10, 0],"bool"), Tensor([1, 2, 10, 0],"bool"), )

[paddle error] paddle.equal_all(Tensor([1, 2, 10, 0],"bool"), Tensor([1, 2, 10, 0],"bool"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.081048 test begin: paddle.equal_all(Tensor([1, 2, 10, 0],"bool"), Tensor([1, 2, 10, 16],"bool"), )

[Pass] paddle.equal_all(Tensor([1, 2, 10, 0],"bool"), Tensor([1, 2, 10, 16],"bool"), )
2025-03-03 18:36:26.082072 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([0, 2, 10, 16],"bool"), )

[Pass] paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([0, 2, 10, 16],"bool"), )
2025-03-03 18:36:26.083058 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 0, 10, 16],"bool"), )

[Pass] paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 0, 10, 16],"bool"), )
2025-03-03 18:36:26.084066 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 0, 16],"bool"), )

[Pass] paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 0, 16],"bool"), )
2025-03-03 18:36:26.085054 test begin: paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 10, 0],"bool"), )

[Pass] paddle.equal_all(Tensor([1, 2, 10, 16],"bool"), Tensor([1, 2, 10, 0],"bool"), )
2025-03-03 18:36:26.086081 test begin: paddle.equal_all(Tensor([1, 2, 3, 0],"float32"), Tensor([1, 2, 3, 0],"float32"), )

[paddle error] paddle.equal_all(Tensor([1, 2, 3, 0],"float32"), Tensor([1, 2, 3, 0],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.086983 test begin: paddle.equal_all(Tensor([1, 2, 3, 0],"float32"), Tensor([1, 2, 3, 4],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 2, 3, 0],"float32"), Tensor([1, 2, 3, 4],"float32"), )
2025-03-03 18:36:26.088106 test begin: paddle.equal_all(Tensor([1, 2, 3, 4],"float32"), Tensor([0, 2, 3, 4],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 2, 3, 4],"float32"), Tensor([0, 2, 3, 4],"float32"), )
2025-03-03 18:36:26.089218 test begin: paddle.equal_all(Tensor([1, 2, 3, 4],"float32"), Tensor([1, 0, 3, 4],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 2, 3, 4],"float32"), Tensor([1, 0, 3, 4],"float32"), )
2025-03-03 18:36:26.090206 test begin: paddle.equal_all(Tensor([1, 2, 3, 4],"float32"), Tensor([1, 2, 0, 4],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 2, 3, 4],"float32"), Tensor([1, 2, 0, 4],"float32"), )
2025-03-03 18:36:26.091240 test begin: paddle.equal_all(Tensor([1, 2, 3, 4],"float32"), Tensor([1, 2, 3, 0],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 2, 3, 4],"float32"), Tensor([1, 2, 3, 0],"float32"), )
2025-03-03 18:36:26.092224 test begin: paddle.equal_all(Tensor([1, 2],"float32"), Tensor([0, 2],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 2],"float32"), Tensor([0, 2],"float32"), )
2025-03-03 18:36:26.093311 test begin: paddle.equal_all(Tensor([1, 2],"float32"), Tensor([1, 0],"float32"), )

[Pass] paddle.equal_all(Tensor([1, 2],"float32"), Tensor([1, 0],"float32"), )
2025-03-03 18:36:26.094294 test begin: paddle.equal_all(Tensor([128],"float32"), Tensor([0],"float32"), )

[Pass] paddle.equal_all(Tensor([128],"float32"), Tensor([0],"float32"), )
2025-03-03 18:36:26.095355 test begin: paddle.equal_all(Tensor([16],"float32"), Tensor([0],"float32"), )

[Pass] paddle.equal_all(Tensor([16],"float32"), Tensor([0],"float32"), )
2025-03-03 18:36:26.096419 test begin: paddle.equal_all(Tensor([2, 0, 2],"int64"), Tensor([2, 0, 2],"int64"), )

[paddle error] paddle.equal_all(Tensor([2, 0, 2],"int64"), Tensor([2, 0, 2],"int64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.097494 test begin: paddle.equal_all(Tensor([2, 0, 2],"int64"), Tensor([2, 1, 2],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 0, 2],"int64"), Tensor([2, 1, 2],"int64"), )
2025-03-03 18:36:26.098526 test begin: paddle.equal_all(Tensor([2, 0, 4],"int64"), Tensor([2, 0, 4],"int64"), )

[paddle error] paddle.equal_all(Tensor([2, 0, 4],"int64"), Tensor([2, 0, 4],"int64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.099587 test begin: paddle.equal_all(Tensor([2, 0, 4],"int64"), Tensor([2, 1, 4],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 0, 4],"int64"), Tensor([2, 1, 4],"int64"), )
2025-03-03 18:36:26.100593 test begin: paddle.equal_all(Tensor([2, 1, 0],"int64"), Tensor([2, 1, 0],"int64"), )

[paddle error] paddle.equal_all(Tensor([2, 1, 0],"int64"), Tensor([2, 1, 0],"int64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.101452 test begin: paddle.equal_all(Tensor([2, 1, 0],"int64"), Tensor([2, 1, 2],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 1, 0],"int64"), Tensor([2, 1, 2],"int64"), )
2025-03-03 18:36:26.102482 test begin: paddle.equal_all(Tensor([2, 1, 0],"int64"), Tensor([2, 1, 4],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 1, 0],"int64"), Tensor([2, 1, 4],"int64"), )
2025-03-03 18:36:26.103689 test begin: paddle.equal_all(Tensor([2, 1, 2],"int64"), Tensor([0, 1, 2],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 1, 2],"int64"), Tensor([0, 1, 2],"int64"), )
2025-03-03 18:36:26.104876 test begin: paddle.equal_all(Tensor([2, 1, 2],"int64"), Tensor([2, 0, 2],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 1, 2],"int64"), Tensor([2, 0, 2],"int64"), )
2025-03-03 18:36:26.105919 test begin: paddle.equal_all(Tensor([2, 1, 2],"int64"), Tensor([2, 1, 0],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 1, 2],"int64"), Tensor([2, 1, 0],"int64"), )
2025-03-03 18:36:26.106983 test begin: paddle.equal_all(Tensor([2, 1, 4],"int64"), Tensor([0, 1, 4],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 1, 4],"int64"), Tensor([0, 1, 4],"int64"), )
2025-03-03 18:36:26.108059 test begin: paddle.equal_all(Tensor([2, 1, 4],"int64"), Tensor([2, 0, 4],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 1, 4],"int64"), Tensor([2, 0, 4],"int64"), )
2025-03-03 18:36:26.109044 test begin: paddle.equal_all(Tensor([2, 1, 4],"int64"), Tensor([2, 1, 0],"int64"), )

[Pass] paddle.equal_all(Tensor([2, 1, 4],"int64"), Tensor([2, 1, 0],"int64"), )
2025-03-03 18:36:26.110148 test begin: paddle.equal_all(x=Tensor([0, 1, 1, 2, 3],"float64"), y=Tensor([0, 1, 1, 2, 3],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([0, 1, 1, 2, 3],"float64"), y=Tensor([0, 1, 1, 2, 3],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.112284 test begin: paddle.equal_all(x=Tensor([0, 1, 1, 2, 3],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([0, 1, 1, 2, 3],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )
2025-03-03 18:36:26.114175 test begin: paddle.equal_all(x=Tensor([0, 2, 3],"float64"), y=Tensor([2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([0, 2, 3],"float64"), y=Tensor([2, 3],"float64"), )
2025-03-03 18:36:26.115792 test begin: paddle.equal_all(x=Tensor([0, 3, 3],"float32"), y=Tensor([0, 3, 3],"float32"), )

[paddle error] paddle.equal_all(x=Tensor([0, 3, 3],"float32"), y=Tensor([0, 3, 3],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.116943 test begin: paddle.equal_all(x=Tensor([0, 3, 3],"float32"), y=Tensor([3, 3, 3],"float32"), )

[Pass] paddle.equal_all(x=Tensor([0, 3, 3],"float32"), y=Tensor([3, 3, 3],"float32"), )
2025-03-03 18:36:26.118192 test begin: paddle.equal_all(x=Tensor([0, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([0, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.119364 test begin: paddle.equal_all(x=Tensor([0, 3, 3],"float64"), y=Tensor([3, 3, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([0, 3, 3],"float64"), y=Tensor([3, 3, 3],"float64"), )
2025-03-03 18:36:26.120899 test begin: paddle.equal_all(x=Tensor([0, 3, 3],"float64"), y=Tensor([3, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([0, 3, 3],"float64"), y=Tensor([3, 3],"float64"), )
2025-03-03 18:36:26.122474 test begin: paddle.equal_all(x=Tensor([0, 3],"float64"), y=Tensor([0, 3],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([0, 3],"float64"), y=Tensor([0, 3],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.124000 test begin: paddle.equal_all(x=Tensor([0, 3],"float64"), y=Tensor([1, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([0, 3],"float64"), y=Tensor([1, 3],"float64"), )
2025-03-03 18:36:26.125285 test begin: paddle.equal_all(x=Tensor([1, 0, 1, 2, 3],"float64"), y=Tensor([1, 0, 1, 2, 3],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([1, 0, 1, 2, 3],"float64"), y=Tensor([1, 0, 1, 2, 3],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.126523 test begin: paddle.equal_all(x=Tensor([1, 0, 1, 2, 3],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 0, 1, 2, 3],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )
2025-03-03 18:36:26.127753 test begin: paddle.equal_all(x=Tensor([1, 0, 3],"float64"), y=Tensor([2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 0, 3],"float64"), y=Tensor([2, 3],"float64"), )
2025-03-03 18:36:26.129254 test begin: paddle.equal_all(x=Tensor([1, 1, 0, 2, 3],"float64"), y=Tensor([1, 1, 0, 2, 3],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([1, 1, 0, 2, 3],"float64"), y=Tensor([1, 1, 0, 2, 3],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.130422 test begin: paddle.equal_all(x=Tensor([1, 1, 0, 2, 3],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 1, 0, 2, 3],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )
2025-03-03 18:36:26.131758 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 0, 3],"float64"), y=Tensor([1, 1, 1, 0, 3],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([1, 1, 1, 0, 3],"float64"), y=Tensor([1, 1, 1, 0, 3],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.132845 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 0, 3],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 1, 1, 0, 3],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )
2025-03-03 18:36:26.134028 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 2, 0],"float64"), y=Tensor([1, 1, 1, 2, 0],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([1, 1, 1, 2, 0],"float64"), y=Tensor([1, 1, 1, 2, 0],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.135063 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 2, 0],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 1, 1, 2, 0],"float64"), y=Tensor([1, 1, 1, 2, 3],"float64"), )
2025-03-03 18:36:26.136075 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([0, 1, 1, 2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([0, 1, 1, 2, 3],"float64"), )
2025-03-03 18:36:26.137194 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([1, 0, 1, 2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([1, 0, 1, 2, 3],"float64"), )
2025-03-03 18:36:26.138301 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([1, 1, 0, 2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([1, 1, 0, 2, 3],"float64"), )
2025-03-03 18:36:26.139372 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([1, 1, 1, 0, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([1, 1, 1, 0, 3],"float64"), )
2025-03-03 18:36:26.140369 test begin: paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([1, 1, 1, 2, 0],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 1, 1, 2, 3],"float64"), y=Tensor([1, 1, 1, 2, 0],"float64"), )
2025-03-03 18:36:26.141444 test begin: paddle.equal_all(x=Tensor([1, 2, 0],"float64"), y=Tensor([2, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 2, 0],"float64"), y=Tensor([2, 3],"float64"), )
2025-03-03 18:36:26.142464 test begin: paddle.equal_all(x=Tensor([1, 2, 3],"float64"), y=Tensor([0, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 2, 3],"float64"), y=Tensor([0, 3],"float64"), )
2025-03-03 18:36:26.143481 test begin: paddle.equal_all(x=Tensor([1, 2, 3],"float64"), y=Tensor([2, 0],"float64"), )

[Pass] paddle.equal_all(x=Tensor([1, 2, 3],"float64"), y=Tensor([2, 0],"float64"), )
2025-03-03 18:36:26.144761 test begin: paddle.equal_all(x=Tensor([2, 0],"float64"), y=Tensor([1, 0],"float64"), )

[Pass] paddle.equal_all(x=Tensor([2, 0],"float64"), y=Tensor([1, 0],"float64"), )
2025-03-03 18:36:26.145659 test begin: paddle.equal_all(x=Tensor([2, 0],"float64"), y=Tensor([1, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([2, 0],"float64"), y=Tensor([1, 3],"float64"), )
2025-03-03 18:36:26.146695 test begin: paddle.equal_all(x=Tensor([2, 3],"float64"), y=Tensor([0, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([2, 3],"float64"), y=Tensor([0, 3],"float64"), )
2025-03-03 18:36:26.147941 test begin: paddle.equal_all(x=Tensor([2, 3],"float64"), y=Tensor([1, 0],"float64"), )

[Pass] paddle.equal_all(x=Tensor([2, 3],"float64"), y=Tensor([1, 0],"float64"), )
2025-03-03 18:36:26.149418 test begin: paddle.equal_all(x=Tensor([3, 0, 3],"float32"), y=Tensor([3, 0, 3],"float32"), )

[paddle error] paddle.equal_all(x=Tensor([3, 0, 3],"float32"), y=Tensor([3, 0, 3],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.150577 test begin: paddle.equal_all(x=Tensor([3, 0, 3],"float32"), y=Tensor([3, 3, 3],"float32"), )

[Pass] paddle.equal_all(x=Tensor([3, 0, 3],"float32"), y=Tensor([3, 3, 3],"float32"), )
2025-03-03 18:36:26.151598 test begin: paddle.equal_all(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 0, 3],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 0, 3],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.152490 test begin: paddle.equal_all(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 3, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 3, 3],"float64"), )
2025-03-03 18:36:26.153517 test begin: paddle.equal_all(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 0, 3],"float64"), y=Tensor([3, 3],"float64"), )
2025-03-03 18:36:26.154921 test begin: paddle.equal_all(x=Tensor([3, 3, 0],"float32"), y=Tensor([3, 3, 0],"float32"), )

[paddle error] paddle.equal_all(x=Tensor([3, 3, 0],"float32"), y=Tensor([3, 3, 0],"float32"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.156023 test begin: paddle.equal_all(x=Tensor([3, 3, 0],"float32"), y=Tensor([3, 3, 3],"float32"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 0],"float32"), y=Tensor([3, 3, 3],"float32"), )
2025-03-03 18:36:26.157453 test begin: paddle.equal_all(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3, 0],"float64"), )

[paddle error] paddle.equal_all(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3, 0],"float64"), ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:26.158524 test begin: paddle.equal_all(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3, 3],"float64"), )
2025-03-03 18:36:26.159807 test begin: paddle.equal_all(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 0],"float64"), y=Tensor([3, 3],"float64"), )
2025-03-03 18:36:26.161049 test begin: paddle.equal_all(x=Tensor([3, 3, 3],"float32"), y=Tensor([0, 3, 3],"float32"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 3],"float32"), y=Tensor([0, 3, 3],"float32"), )
2025-03-03 18:36:26.162416 test begin: paddle.equal_all(x=Tensor([3, 3, 3],"float32"), y=Tensor([3, 0, 3],"float32"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 3],"float32"), y=Tensor([3, 0, 3],"float32"), )
2025-03-03 18:36:26.163736 test begin: paddle.equal_all(x=Tensor([3, 3, 3],"float32"), y=Tensor([3, 3, 0],"float32"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 3],"float32"), y=Tensor([3, 3, 0],"float32"), )
2025-03-03 18:36:26.164982 test begin: paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([0, 3, 3],"float64"), )
2025-03-03 18:36:26.166274 test begin: paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([0, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([0, 3],"float64"), )
2025-03-03 18:36:26.167701 test begin: paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 0, 3],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 0, 3],"float64"), )
2025-03-03 18:36:26.169763 test begin: paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 0],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 0],"float64"), )
2025-03-03 18:36:26.171150 test begin: paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 3, 0],"float64"), )

[Pass] paddle.equal_all(x=Tensor([3, 3, 3],"float64"), y=Tensor([3, 3, 0],"float64"), )
2025-03-03 18:36:26.172524 test begin: paddle.expand(Tensor([0, 1, 10, 1],"float32"), tuple(10,10,10,10,), )

[paddle error] paddle.expand(Tensor([0, 1, 10, 1],"float32"), tuple(10,10,10,10,), ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.176035 test begin: paddle.expand(Tensor([0, 1, 100, 136],"float32"), shape=list[1,1,-1,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 100, 136],"float32"), shape=list[1,1,-1,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.176714 test begin: paddle.expand(Tensor([0, 1, 100, 136],"float32"), shape=list[4,1,-1,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 100, 136],"float32"), shape=list[4,1,-1,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.177281 test begin: paddle.expand(Tensor([0, 1, 100, 140],"float32"), shape=list[4,1,-1,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 100, 140],"float32"), shape=list[4,1,-1,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.177839 test begin: paddle.expand(Tensor([0, 1, 100, 152],"float32"), shape=list[1,1,-1,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 100, 152],"float32"), shape=list[1,1,-1,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.178389 test begin: paddle.expand(Tensor([0, 1, 16],"float32"), shape=tuple(13,7,16,), )

[paddle error] paddle.expand(Tensor([0, 1, 16],"float32"), shape=tuple(13,7,16,), ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.179331 test begin: paddle.expand(Tensor([0, 1, 1],"int32"), list[13,2,32,], )

[paddle error] paddle.expand(Tensor([0, 1, 1],"int32"), list[13,2,32,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.180497 test begin: paddle.expand(Tensor([0, 1, 2, 2],"bool"), list[13,4,2,2,], name=None, )

[paddle error] paddle.expand(Tensor([0, 1, 2, 2],"bool"), list[13,4,2,2,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.181412 test begin: paddle.expand(Tensor([0, 1, 2, 4],"bool"), list[13,4,2,4,], name=None, )

[paddle error] paddle.expand(Tensor([0, 1, 2, 4],"bool"), list[13,4,2,4,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.182228 test begin: paddle.expand(Tensor([0, 1, 32],"int32"), list[13,2,32,], )

[paddle error] paddle.expand(Tensor([0, 1, 32],"int32"), list[13,2,32,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.183010 test begin: paddle.expand(Tensor([0, 1, 4, 4],"bool"), list[13,4,4,4,], name=None, )

[paddle error] paddle.expand(Tensor([0, 1, 4, 4],"bool"), list[13,4,4,4,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.183588 test begin: paddle.expand(Tensor([0, 1, 4, 7],"bool"), list[13,4,4,7,], name=None, )

[paddle error] paddle.expand(Tensor([0, 1, 4, 7],"bool"), list[13,4,4,7,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.184142 test begin: paddle.expand(Tensor([0, 1, 5, 1, 5],"float32"), list[13,4,5,1,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 5, 1, 5],"float32"), list[13,4,5,1,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.184706 test begin: paddle.expand(Tensor([0, 1, 5, 1, 5],"float32"), list[52,4,5,1,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 5, 1, 5],"float32"), list[52,4,5,1,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (52) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.185357 test begin: paddle.expand(Tensor([0, 1, 7, 14],"float32"), shape=list[2,13,7,14,], )

[paddle error] paddle.expand(Tensor([0, 1, 7, 14],"float32"), shape=list[2,13,7,14,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.185987 test begin: paddle.expand(Tensor([0, 1, 7, 1],"float32"), list[13,4,7,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 7, 1],"float32"), list[13,4,7,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.186531 test begin: paddle.expand(Tensor([0, 1, 7, 1],"float32"), list[52,4,7,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 7, 1],"float32"), list[52,4,7,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (52) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.187103 test begin: paddle.expand(Tensor([0, 1, 7, 7],"bool"), list[13,4,7,7,], name=None, )

[paddle error] paddle.expand(Tensor([0, 1, 7, 7],"bool"), list[13,4,7,7,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.187667 test begin: paddle.expand(Tensor([0, 1, 7, 7],"int64"), list[13,4,7,7,], )

[paddle error] paddle.expand(Tensor([0, 1, 7, 7],"int64"), list[13,4,7,7,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.188282 test begin: paddle.expand(Tensor([0, 1, 7, 7],"int64"), list[52,4,7,7,], )

[paddle error] paddle.expand(Tensor([0, 1, 7, 7],"int64"), list[52,4,7,7,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (52) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.188868 test begin: paddle.expand(Tensor([0, 1, 7],"int64"), shape=list[14,2,-1,], )

[paddle error] paddle.expand(Tensor([0, 1, 7],"int64"), shape=list[14,2,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.189421 test begin: paddle.expand(Tensor([0, 1, 8],"float32"), list[-1,8,-1,], )

[Pass] paddle.expand(Tensor([0, 1, 8],"float32"), list[-1,8,-1,], )
2025-03-03 18:36:26.194719 test begin: paddle.expand(Tensor([0, 10, 1, 1],"float32"), list[-1,-1,256,256,], )

[Pass] paddle.expand(Tensor([0, 10, 1, 1],"float32"), list[-1,-1,256,256,], )
2025-03-03 18:36:26.196551 test begin: paddle.expand(Tensor([0, 10, 1, 1],"float32"), list[-1,-1,32,32,], )

[Pass] paddle.expand(Tensor([0, 10, 1, 1],"float32"), list[-1,-1,32,32,], )
2025-03-03 18:36:26.198625 test begin: paddle.expand(Tensor([0, 10, 1],"float32"), tuple(10,10,10,10,), )

[paddle error] paddle.expand(Tensor([0, 10, 1],"float32"), tuple(10,10,10,10,), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.199339 test begin: paddle.expand(Tensor([0, 1024],"int64"), list[1,1024,], )

[paddle error] paddle.expand(Tensor([0, 1024],"int64"), list[1,1024,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.200050 test begin: paddle.expand(Tensor([0, 10],"float32"), tuple(10,10,), )

[paddle error] paddle.expand(Tensor([0, 10],"float32"), tuple(10,10,), ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.200649 test begin: paddle.expand(Tensor([0, 10],"int64"), list[14,10,], )

[paddle error] paddle.expand(Tensor([0, 10],"int64"), list[14,10,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.201323 test begin: paddle.expand(Tensor([0, 11],"int64"), list[1,11,], )

[paddle error] paddle.expand(Tensor([0, 11],"int64"), list[1,11,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.201974 test begin: paddle.expand(Tensor([0, 128],"float32"), list[16,32,128,], )

[paddle error] paddle.expand(Tensor([0, 128],"float32"), list[16,32,128,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.202596 test begin: paddle.expand(Tensor([0, 128],"float32"), list[32,64,128,], )

[paddle error] paddle.expand(Tensor([0, 128],"float32"), list[32,64,128,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (64) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.203268 test begin: paddle.expand(Tensor([0, 1],"float32"), list[2,3072,], )

[paddle error] paddle.expand(Tensor([0, 1],"float32"), list[2,3072,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.203900 test begin: paddle.expand(Tensor([0, 1],"int32"), list[-1,1,], )

[Pass] paddle.expand(Tensor([0, 1],"int32"), list[-1,1,], )
2025-03-03 18:36:26.205212 test begin: paddle.expand(Tensor([0, 1],"int64"), list[1,1,], )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), list[1,1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.205788 test begin: paddle.expand(Tensor([0, 1],"int64"), list[14,1,], )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), list[14,1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.206314 test begin: paddle.expand(Tensor([0, 1],"int64"), list[2,1,], )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), list[2,1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.206834 test begin: paddle.expand(Tensor([0, 1],"int64"), list[4,1,], )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), list[4,1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.207354 test begin: paddle.expand(Tensor([0, 1],"int64"), list[6,1,], )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), list[6,1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (6) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.207869 test begin: paddle.expand(Tensor([0, 1],"int64"), tuple(128,128,), )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), tuple(128,128,), ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (128) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.208452 test begin: paddle.expand(Tensor([0, 1],"int64"), tuple(14,32,), name=None, )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), tuple(14,32,), name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.209380 test begin: paddle.expand(Tensor([0, 1],"int64"), tuple(15,32,), name=None, )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), tuple(15,32,), name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (15) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.210635 test begin: paddle.expand(Tensor([0, 1],"int64"), tuple(4,32,), name=None, )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), tuple(4,32,), name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.211619 test begin: paddle.expand(Tensor([0, 1],"int64"), tuple(8,32,), name=None, )

[paddle error] paddle.expand(Tensor([0, 1],"int64"), tuple(8,32,), name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (8) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.212268 test begin: paddle.expand(Tensor([0, 2, 1, 8, 128, 96],"float16"), list[2,2,1,8,128,96,], )

[paddle error] paddle.expand(Tensor([0, 2, 1, 8, 128, 96],"float16"), list[2,2,1,8,128,96,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.213013 test begin: paddle.expand(Tensor([0, 2, 1, 8, 128, 96],"float16"), list[2,2,2,8,128,96,], )

[paddle error] paddle.expand(Tensor([0, 2, 1, 8, 128, 96],"float16"), list[2,2,2,8,128,96,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.213749 test begin: paddle.expand(Tensor([0, 20],"int64"), list[1,20,], )

[paddle error] paddle.expand(Tensor([0, 20],"int64"), list[1,20,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.214455 test begin: paddle.expand(Tensor([0, 2],"float16"), shape=list[512,3,2,], )

[paddle error] paddle.expand(Tensor([0, 2],"float16"), shape=list[512,3,2,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.215243 test begin: paddle.expand(Tensor([0, 2],"float32"), shape=list[512,3,2,], )

[paddle error] paddle.expand(Tensor([0, 2],"float32"), shape=list[512,3,2,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.215991 test begin: paddle.expand(Tensor([0, 3, 1, 1, 1, 1],"float32"), list[1,3,4,4,1,1,], )

[paddle error] paddle.expand(Tensor([0, 3, 1, 1, 1, 1],"float32"), list[1,3,4,4,1,1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.216590 test begin: paddle.expand(Tensor([0, 32, 128],"float32"), list[8,16,32,128,], )

[paddle error] paddle.expand(Tensor([0, 32, 128],"float32"), list[8,16,32,128,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (16) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.217087 test begin: paddle.expand(Tensor([0, 32],"float32"), list[128,32,32,], )

[paddle error] paddle.expand(Tensor([0, 32],"float32"), list[128,32,32,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.217534 test begin: paddle.expand(Tensor([0, 32],"float32"), list[96,32,32,], )

[paddle error] paddle.expand(Tensor([0, 32],"float32"), list[96,32,32,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.217981 test begin: paddle.expand(Tensor([0, 32],"int64"), list[14,32,], )

[paddle error] paddle.expand(Tensor([0, 32],"int64"), list[14,32,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.218428 test begin: paddle.expand(Tensor([0, 32],"int64"), list[15,32,], )

[paddle error] paddle.expand(Tensor([0, 32],"int64"), list[15,32,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (15) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.218872 test begin: paddle.expand(Tensor([0, 32],"int64"), list[4,32,], )

[paddle error] paddle.expand(Tensor([0, 32],"int64"), list[4,32,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.219364 test begin: paddle.expand(Tensor([0, 32],"int64"), list[8,32,], )

[paddle error] paddle.expand(Tensor([0, 32],"int64"), list[8,32,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (8) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.219830 test begin: paddle.expand(Tensor([0, 3],"int64"), list[14,3,], )

[paddle error] paddle.expand(Tensor([0, 3],"int64"), list[14,3,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.220326 test begin: paddle.expand(Tensor([0, 3],"int64"), list[2,3,], )

[paddle error] paddle.expand(Tensor([0, 3],"int64"), list[2,3,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.220786 test begin: paddle.expand(Tensor([0, 3],"int64"), list[4,3,], )

[paddle error] paddle.expand(Tensor([0, 3],"int64"), list[4,3,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.221223 test begin: paddle.expand(Tensor([0, 3],"int64"), list[6,3,], )

[paddle error] paddle.expand(Tensor([0, 3],"int64"), list[6,3,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (6) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.221659 test begin: paddle.expand(Tensor([0, 4, 1, 1, 1, 8],"float32"), list[13,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([0, 4, 1, 1, 1, 8],"float32"), list[13,4,3,1,1,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.222153 test begin: paddle.expand(Tensor([0, 4, 1, 1, 1, 8],"float32"), list[52,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([0, 4, 1, 1, 1, 8],"float32"), list[52,4,3,1,1,-1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (52) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.222626 test begin: paddle.expand(Tensor([0, 4, 2, 1],"float32"), list[13,4,2,2,], name=None, )

[paddle error] paddle.expand(Tensor([0, 4, 2, 1],"float32"), list[13,4,2,2,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.223091 test begin: paddle.expand(Tensor([0, 4, 2, 1],"float32"), list[13,4,2,4,], name=None, )

[paddle error] paddle.expand(Tensor([0, 4, 2, 1],"float32"), list[13,4,2,4,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.223543 test begin: paddle.expand(Tensor([0, 4, 4, 1],"float32"), list[13,4,4,4,], name=None, )

[paddle error] paddle.expand(Tensor([0, 4, 4, 1],"float32"), list[13,4,4,4,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.224031 test begin: paddle.expand(Tensor([0, 4, 4, 1],"float32"), list[13,4,4,7,], name=None, )

[paddle error] paddle.expand(Tensor([0, 4, 4, 1],"float32"), list[13,4,4,7,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.224474 test begin: paddle.expand(Tensor([0, 4, 7, 1],"float32"), list[13,4,7,7,], name=None, )

[paddle error] paddle.expand(Tensor([0, 4, 7, 1],"float32"), list[13,4,7,7,], name=None, ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.224979 test begin: paddle.expand(Tensor([0, 64],"float32"), list[16,32,64,], )

[paddle error] paddle.expand(Tensor([0, 64],"float32"), list[16,32,64,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.225419 test begin: paddle.expand(Tensor([0, 7, 7],"float32"), shape=list[13,7,7,], )

[paddle error] paddle.expand(Tensor([0, 7, 7],"float32"), shape=list[13,7,7,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.225869 test begin: paddle.expand(Tensor([0, 7],"int64"), list[14,7,], )

[paddle error] paddle.expand(Tensor([0, 7],"int64"), list[14,7,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.226381 test begin: paddle.expand(Tensor([0, 8],"int64"), list[14,8,], )

[paddle error] paddle.expand(Tensor([0, 8],"int64"), list[14,8,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.226859 test begin: paddle.expand(Tensor([0],"float32"), shape=list[128,96,], )

[paddle error] paddle.expand(Tensor([0],"float32"), shape=list[128,96,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (96) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.227304 test begin: paddle.expand(Tensor([0],"int64"), list[13,7,4,], )

[paddle error] paddle.expand(Tensor([0],"int64"), list[13,7,4,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.227812 test begin: paddle.expand(Tensor([0],"int64"), shape=list[1,-1,], )

[Pass] paddle.expand(Tensor([0],"int64"), shape=list[1,-1,], )
2025-03-03 18:36:26.229328 test begin: paddle.expand(Tensor([1, 0, 1, 1, 1, 1],"float32"), list[1,3,4,4,1,1,], )

[paddle error] paddle.expand(Tensor([1, 0, 1, 1, 1, 1],"float32"), list[1,3,4,4,1,1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.230411 test begin: paddle.expand(Tensor([1, 0, 1, 1],"float32"), list[-1,-1,256,256,], )

[Pass] paddle.expand(Tensor([1, 0, 1, 1],"float32"), list[-1,-1,256,256,], )
2025-03-03 18:36:26.232231 test begin: paddle.expand(Tensor([1, 0, 1, 1],"float32"), list[-1,-1,32,32,], )

[Pass] paddle.expand(Tensor([1, 0, 1, 1],"float32"), list[-1,-1,32,32,], )
2025-03-03 18:36:26.233569 test begin: paddle.expand(Tensor([1, 0, 100, 136],"float32"), shape=list[1,1,-1,-1,], )

[paddle error] paddle.expand(Tensor([1, 0, 100, 136],"float32"), shape=list[1,1,-1,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.234200 test begin: paddle.expand(Tensor([1, 0, 100, 136],"float32"), shape=list[4,1,-1,-1,], )

[paddle error] paddle.expand(Tensor([1, 0, 100, 136],"float32"), shape=list[4,1,-1,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.234784 test begin: paddle.expand(Tensor([1, 0, 100, 140],"float32"), shape=list[4,1,-1,-1,], )

[paddle error] paddle.expand(Tensor([1, 0, 100, 140],"float32"), shape=list[4,1,-1,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.235342 test begin: paddle.expand(Tensor([1, 0, 100, 152],"float32"), shape=list[1,1,-1,-1,], )

[paddle error] paddle.expand(Tensor([1, 0, 100, 152],"float32"), shape=list[1,1,-1,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.235923 test begin: paddle.expand(Tensor([1, 0, 1],"float32"), tuple(10,10,10,10,), )

[paddle error] paddle.expand(Tensor([1, 0, 1],"float32"), tuple(10,10,10,10,), ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.236552 test begin: paddle.expand(Tensor([1, 0, 32],"int32"), list[13,2,32,], )

[paddle error] paddle.expand(Tensor([1, 0, 32],"int32"), list[13,2,32,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.237131 test begin: paddle.expand(Tensor([1, 0, 7, 7],"int64"), list[13,4,7,7,], )

[paddle error] paddle.expand(Tensor([1, 0, 7, 7],"int64"), list[13,4,7,7,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.237682 test begin: paddle.expand(Tensor([1, 0, 7, 7],"int64"), list[52,4,7,7,], )

[paddle error] paddle.expand(Tensor([1, 0, 7, 7],"int64"), list[52,4,7,7,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.238149 test begin: paddle.expand(Tensor([1, 0, 7],"float32"), shape=list[13,7,7,], )

[paddle error] paddle.expand(Tensor([1, 0, 7],"float32"), shape=list[13,7,7,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.238594 test begin: paddle.expand(Tensor([1, 0, 7],"int64"), shape=list[14,2,-1,], )

[paddle error] paddle.expand(Tensor([1, 0, 7],"int64"), shape=list[14,2,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.239046 test begin: paddle.expand(Tensor([1, 0],"float32"), tuple(10,10,), )

[paddle error] paddle.expand(Tensor([1, 0],"float32"), tuple(10,10,), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.239468 test begin: paddle.expand(Tensor([1, 0],"int64"), list[1,1,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[1,1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.239906 test begin: paddle.expand(Tensor([1, 0],"int64"), list[1,1024,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[1,1024,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1024) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.240355 test begin: paddle.expand(Tensor([1, 0],"int64"), list[1,11,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[1,11,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (11) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.240979 test begin: paddle.expand(Tensor([1, 0],"int64"), list[1,20,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[1,20,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (20) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.241519 test begin: paddle.expand(Tensor([1, 0],"int64"), list[14,1,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[14,1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.242060 test begin: paddle.expand(Tensor([1, 0],"int64"), list[14,10,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[14,10,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.242605 test begin: paddle.expand(Tensor([1, 0],"int64"), list[14,3,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[14,3,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.243141 test begin: paddle.expand(Tensor([1, 0],"int64"), list[14,32,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[14,32,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.243696 test begin: paddle.expand(Tensor([1, 0],"int64"), list[14,7,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[14,7,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.244131 test begin: paddle.expand(Tensor([1, 0],"int64"), list[14,8,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[14,8,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (8) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.244552 test begin: paddle.expand(Tensor([1, 0],"int64"), list[15,32,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[15,32,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.244986 test begin: paddle.expand(Tensor([1, 0],"int64"), list[2,1,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[2,1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.245406 test begin: paddle.expand(Tensor([1, 0],"int64"), list[2,3,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[2,3,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.246007 test begin: paddle.expand(Tensor([1, 0],"int64"), list[4,1,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[4,1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.246453 test begin: paddle.expand(Tensor([1, 0],"int64"), list[4,3,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[4,3,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.246883 test begin: paddle.expand(Tensor([1, 0],"int64"), list[4,32,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[4,32,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.247569 test begin: paddle.expand(Tensor([1, 0],"int64"), list[6,1,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[6,1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.248247 test begin: paddle.expand(Tensor([1, 0],"int64"), list[6,3,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[6,3,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.248709 test begin: paddle.expand(Tensor([1, 0],"int64"), list[8,32,], )

[paddle error] paddle.expand(Tensor([1, 0],"int64"), list[8,32,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.249207 test begin: paddle.expand(Tensor([1, 1, 0, 136],"float32"), shape=list[1,1,-1,-1,], )

[Pass] paddle.expand(Tensor([1, 1, 0, 136],"float32"), shape=list[1,1,-1,-1,], )
2025-03-03 18:36:26.251109 test begin: paddle.expand(Tensor([1, 1, 0, 136],"float32"), shape=list[4,1,-1,-1,], )

[Pass] paddle.expand(Tensor([1, 1, 0, 136],"float32"), shape=list[4,1,-1,-1,], )
2025-03-03 18:36:26.254141 test begin: paddle.expand(Tensor([1, 1, 0, 140],"float32"), shape=list[4,1,-1,-1,], )

[Pass] paddle.expand(Tensor([1, 1, 0, 140],"float32"), shape=list[4,1,-1,-1,], )
2025-03-03 18:36:26.257778 test begin: paddle.expand(Tensor([1, 1, 0, 152],"float32"), shape=list[1,1,-1,-1,], )

[Pass] paddle.expand(Tensor([1, 1, 0, 152],"float32"), shape=list[1,1,-1,-1,], )
2025-03-03 18:36:26.259477 test begin: paddle.expand(Tensor([1, 1, 0, 7],"int64"), list[13,4,7,7,], )

[paddle error] paddle.expand(Tensor([1, 1, 0, 7],"int64"), list[13,4,7,7,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.260039 test begin: paddle.expand(Tensor([1, 1, 0, 7],"int64"), list[52,4,7,7,], )

[paddle error] paddle.expand(Tensor([1, 1, 0, 7],"int64"), list[52,4,7,7,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.260533 test begin: paddle.expand(Tensor([1, 1, 0],"int32"), list[13,2,32,], )

[paddle error] paddle.expand(Tensor([1, 1, 0],"int32"), list[13,2,32,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.261081 test begin: paddle.expand(Tensor([1, 1, 0],"int64"), shape=list[14,2,-1,], )

[Pass] paddle.expand(Tensor([1, 1, 0],"int64"), shape=list[14,2,-1,], )
2025-03-03 18:36:26.263156 test begin: paddle.expand(Tensor([1, 1, 100, 0],"float32"), shape=list[1,1,-1,-1,], )

[Pass] paddle.expand(Tensor([1, 1, 100, 0],"float32"), shape=list[1,1,-1,-1,], )
2025-03-03 18:36:26.266920 test begin: paddle.expand(Tensor([1, 1, 100, 0],"float32"), shape=list[4,1,-1,-1,], )

[Pass] paddle.expand(Tensor([1, 1, 100, 0],"float32"), shape=list[4,1,-1,-1,], )
2025-03-03 18:36:26.269525 test begin: paddle.expand(Tensor([1, 1, 7, 0],"int64"), list[13,4,7,7,], )

[paddle error] paddle.expand(Tensor([1, 1, 7, 0],"int64"), list[13,4,7,7,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.270348 test begin: paddle.expand(Tensor([1, 1, 7, 0],"int64"), list[52,4,7,7,], )

[paddle error] paddle.expand(Tensor([1, 1, 7, 0],"int64"), list[52,4,7,7,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.271010 test begin: paddle.expand(Tensor([1, 10, 0, 1],"float32"), list[-1,-1,256,256,], )

[paddle error] paddle.expand(Tensor([1, 10, 0, 1],"float32"), list[-1,-1,256,256,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (256) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.271573 test begin: paddle.expand(Tensor([1, 10, 0, 1],"float32"), list[-1,-1,32,32,], )

[paddle error] paddle.expand(Tensor([1, 10, 0, 1],"float32"), list[-1,-1,32,32,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.272126 test begin: paddle.expand(Tensor([1, 10, 0],"float32"), tuple(10,10,10,10,), )

[paddle error] paddle.expand(Tensor([1, 10, 0],"float32"), tuple(10,10,10,10,), ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.272745 test begin: paddle.expand(Tensor([1, 10, 1, 0],"float32"), list[-1,-1,256,256,], )

[paddle error] paddle.expand(Tensor([1, 10, 1, 0],"float32"), list[-1,-1,256,256,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (256) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.273292 test begin: paddle.expand(Tensor([1, 10, 1, 0],"float32"), list[-1,-1,32,32,], )

[paddle error] paddle.expand(Tensor([1, 10, 1, 0],"float32"), list[-1,-1,32,32,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.273931 test begin: paddle.expand(Tensor([1, 3, 0, 1, 1, 1],"float32"), list[1,3,4,4,1,1,], )

[paddle error] paddle.expand(Tensor([1, 3, 0, 1, 1, 1],"float32"), list[1,3,4,4,1,1,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.274931 test begin: paddle.expand(Tensor([1, 3, 1, 0, 1, 1],"float32"), list[1,3,4,4,1,1,], )

[paddle error] paddle.expand(Tensor([1, 3, 1, 0, 1, 1],"float32"), list[1,3,4,4,1,1,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.276171 test begin: paddle.expand(Tensor([1, 3, 1, 1, 0, 1],"float32"), list[1,3,4,4,1,1,], )

[paddle error] paddle.expand(Tensor([1, 3, 1, 1, 0, 1],"float32"), list[1,3,4,4,1,1,], ) 
 (InvalidArgument) The 4-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.276711 test begin: paddle.expand(Tensor([1, 3, 1, 1, 1, 0],"float32"), list[1,3,4,4,1,1,], )

[paddle error] paddle.expand(Tensor([1, 3, 1, 1, 1, 0],"float32"), list[1,3,4,4,1,1,], ) 
 (InvalidArgument) The 5-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.277451 test begin: paddle.expand(Tensor([1, 7, 0],"float32"), shape=list[13,7,7,], )

[paddle error] paddle.expand(Tensor([1, 7, 0],"float32"), shape=list[13,7,7,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.277947 test begin: paddle.expand(Tensor([10, 0, 10, 1],"float32"), tuple(10,10,10,10,), )

[paddle error] paddle.expand(Tensor([10, 0, 10, 1],"float32"), tuple(10,10,10,10,), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.278403 test begin: paddle.expand(Tensor([10, 0],"float32"), tuple(10,10,), )

[paddle error] paddle.expand(Tensor([10, 0],"float32"), tuple(10,10,), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.278841 test begin: paddle.expand(Tensor([10, 1, 0, 1],"float32"), tuple(10,10,10,10,), )

[paddle error] paddle.expand(Tensor([10, 1, 0, 1],"float32"), tuple(10,10,10,10,), ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.279295 test begin: paddle.expand(Tensor([10, 1, 10, 0],"float32"), tuple(10,10,10,10,), )

[paddle error] paddle.expand(Tensor([10, 1, 10, 0],"float32"), tuple(10,10,10,10,), ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (10) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.279742 test begin: paddle.expand(Tensor([108, 0],"int32"), list[-1,1,], )

[paddle error] paddle.expand(Tensor([108, 0],"int32"), list[-1,1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.280183 test begin: paddle.expand(Tensor([117, 0],"int32"), list[-1,1,], )

[paddle error] paddle.expand(Tensor([117, 0],"int32"), list[-1,1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.280672 test begin: paddle.expand(Tensor([128, 0],"int64"), tuple(128,128,), )

[paddle error] paddle.expand(Tensor([128, 0],"int64"), tuple(128,128,), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (128) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.281116 test begin: paddle.expand(Tensor([13, 0, 1, 1, 1, 8],"float32"), list[13,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([13, 0, 1, 1, 1, 8],"float32"), list[13,4,3,1,1,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.281602 test begin: paddle.expand(Tensor([13, 0, 16],"float32"), shape=tuple(13,7,16,), )

[paddle error] paddle.expand(Tensor([13, 0, 16],"float32"), shape=tuple(13,7,16,), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.282039 test begin: paddle.expand(Tensor([13, 0, 1],"int32"), list[13,2,32,], )

[paddle error] paddle.expand(Tensor([13, 0, 1],"int32"), list[13,2,32,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.282474 test begin: paddle.expand(Tensor([13, 0, 2, 1],"float32"), list[13,4,2,2,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 2, 1],"float32"), list[13,4,2,2,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.282943 test begin: paddle.expand(Tensor([13, 0, 2, 1],"float32"), list[13,4,2,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 2, 1],"float32"), list[13,4,2,4,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.283380 test begin: paddle.expand(Tensor([13, 0, 2, 2],"bool"), list[13,4,2,2,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 2, 2],"bool"), list[13,4,2,2,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.283845 test begin: paddle.expand(Tensor([13, 0, 2, 4],"bool"), list[13,4,2,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 2, 4],"bool"), list[13,4,2,4,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.284296 test begin: paddle.expand(Tensor([13, 0, 4, 1],"float32"), list[13,4,4,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 4, 1],"float32"), list[13,4,4,4,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.284756 test begin: paddle.expand(Tensor([13, 0, 4, 1],"float32"), list[13,4,4,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 4, 1],"float32"), list[13,4,4,7,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.285204 test begin: paddle.expand(Tensor([13, 0, 4, 4],"bool"), list[13,4,4,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 4, 4],"bool"), list[13,4,4,4,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.285690 test begin: paddle.expand(Tensor([13, 0, 4, 7],"bool"), list[13,4,4,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 4, 7],"bool"), list[13,4,4,7,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.286160 test begin: paddle.expand(Tensor([13, 0, 5, 1, 5],"float32"), list[13,4,5,1,-1,], )

[paddle error] paddle.expand(Tensor([13, 0, 5, 1, 5],"float32"), list[13,4,5,1,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.286628 test begin: paddle.expand(Tensor([13, 0, 7, 1],"float32"), list[13,4,7,-1,], )

[paddle error] paddle.expand(Tensor([13, 0, 7, 1],"float32"), list[13,4,7,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.287065 test begin: paddle.expand(Tensor([13, 0, 7, 1],"float32"), list[13,4,7,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 7, 1],"float32"), list[13,4,7,7,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.287505 test begin: paddle.expand(Tensor([13, 0, 7, 7],"bool"), list[13,4,7,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 0, 7, 7],"bool"), list[13,4,7,7,], name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.287958 test begin: paddle.expand(Tensor([13, 1, 0, 1, 5],"float32"), list[13,4,5,1,-1,], )

[paddle error] paddle.expand(Tensor([13, 1, 0, 1, 5],"float32"), list[13,4,5,1,-1,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (5) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.288436 test begin: paddle.expand(Tensor([13, 1, 0, 1],"float32"), list[13,4,7,-1,], )

[paddle error] paddle.expand(Tensor([13, 1, 0, 1],"float32"), list[13,4,7,-1,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.288910 test begin: paddle.expand(Tensor([13, 1, 0, 2],"bool"), list[13,4,2,2,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 0, 2],"bool"), list[13,4,2,2,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.289407 test begin: paddle.expand(Tensor([13, 1, 0, 4],"bool"), list[13,4,2,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 0, 4],"bool"), list[13,4,2,4,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.289876 test begin: paddle.expand(Tensor([13, 1, 0, 4],"bool"), list[13,4,4,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 0, 4],"bool"), list[13,4,4,4,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.290318 test begin: paddle.expand(Tensor([13, 1, 0, 7],"bool"), list[13,4,4,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 0, 7],"bool"), list[13,4,4,7,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.290764 test begin: paddle.expand(Tensor([13, 1, 0, 7],"bool"), list[13,4,7,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 0, 7],"bool"), list[13,4,7,7,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.291201 test begin: paddle.expand(Tensor([13, 1, 0],"float32"), shape=tuple(13,7,16,), )

[paddle error] paddle.expand(Tensor([13, 1, 0],"float32"), shape=tuple(13,7,16,), ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (16) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.291630 test begin: paddle.expand(Tensor([13, 1, 0],"int32"), list[13,2,32,], )

[paddle error] paddle.expand(Tensor([13, 1, 0],"int32"), list[13,2,32,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.292072 test begin: paddle.expand(Tensor([13, 1, 2, 0],"bool"), list[13,4,2,2,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 2, 0],"bool"), list[13,4,2,2,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.292505 test begin: paddle.expand(Tensor([13, 1, 2, 0],"bool"), list[13,4,2,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 2, 0],"bool"), list[13,4,2,4,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.292954 test begin: paddle.expand(Tensor([13, 1, 4, 0],"bool"), list[13,4,4,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 4, 0],"bool"), list[13,4,4,4,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.293386 test begin: paddle.expand(Tensor([13, 1, 4, 0],"bool"), list[13,4,4,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 4, 0],"bool"), list[13,4,4,7,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.293823 test begin: paddle.expand(Tensor([13, 1, 5, 0, 5],"float32"), list[13,4,5,1,-1,], )

[paddle error] paddle.expand(Tensor([13, 1, 5, 0, 5],"float32"), list[13,4,5,1,-1,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.294311 test begin: paddle.expand(Tensor([13, 1, 5, 1, 0],"float32"), list[13,4,5,1,-1,], )

[Pass] paddle.expand(Tensor([13, 1, 5, 1, 0],"float32"), list[13,4,5,1,-1,], )
2025-03-03 18:36:26.295984 test begin: paddle.expand(Tensor([13, 1, 7, 0],"bool"), list[13,4,7,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 1, 7, 0],"bool"), list[13,4,7,7,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.296505 test begin: paddle.expand(Tensor([13, 1, 7, 0],"float32"), list[13,4,7,-1,], )

[Pass] paddle.expand(Tensor([13, 1, 7, 0],"float32"), list[13,4,7,-1,], )
2025-03-03 18:36:26.299395 test begin: paddle.expand(Tensor([13, 4, 0, 1, 1, 8],"float32"), list[13,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([13, 4, 0, 1, 1, 8],"float32"), list[13,4,3,1,1,-1,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.299897 test begin: paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,2,2,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,2,2,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.300387 test begin: paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,2,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,2,4,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.300914 test begin: paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,4,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,4,4,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.301380 test begin: paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,4,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,4,7,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.301840 test begin: paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,7,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 0, 1],"float32"), list[13,4,7,7,], name=None, ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.302779 test begin: paddle.expand(Tensor([13, 4, 1, 0, 1, 8],"float32"), list[13,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([13, 4, 1, 0, 1, 8],"float32"), list[13,4,3,1,1,-1,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.303335 test begin: paddle.expand(Tensor([13, 4, 1, 1, 0, 8],"float32"), list[13,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([13, 4, 1, 1, 0, 8],"float32"), list[13,4,3,1,1,-1,], ) 
 (InvalidArgument) The 4-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.303797 test begin: paddle.expand(Tensor([13, 4, 1, 1, 1, 0],"float32"), list[13,4,3,1,1,-1,], )

[Pass] paddle.expand(Tensor([13, 4, 1, 1, 1, 0],"float32"), list[13,4,3,1,1,-1,], )
2025-03-03 18:36:26.312588 test begin: paddle.expand(Tensor([13, 4, 2, 0],"float32"), list[13,4,2,2,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 2, 0],"float32"), list[13,4,2,2,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.313376 test begin: paddle.expand(Tensor([13, 4, 2, 0],"float32"), list[13,4,2,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 2, 0],"float32"), list[13,4,2,4,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.314103 test begin: paddle.expand(Tensor([13, 4, 4, 0],"float32"), list[13,4,4,4,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 4, 0],"float32"), list[13,4,4,4,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.314738 test begin: paddle.expand(Tensor([13, 4, 4, 0],"float32"), list[13,4,4,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 4, 0],"float32"), list[13,4,4,7,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.315332 test begin: paddle.expand(Tensor([13, 4, 7, 0],"float32"), list[13,4,7,7,], name=None, )

[paddle error] paddle.expand(Tensor([13, 4, 7, 0],"float32"), list[13,4,7,7,], name=None, ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.315910 test begin: paddle.expand(Tensor([14, 0],"int64"), tuple(14,32,), name=None, )

[paddle error] paddle.expand(Tensor([14, 0],"int64"), tuple(14,32,), name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.316427 test begin: paddle.expand(Tensor([15, 0],"int64"), tuple(15,32,), name=None, )

[paddle error] paddle.expand(Tensor([15, 0],"int64"), tuple(15,32,), name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.316896 test begin: paddle.expand(Tensor([16, 0, 128],"float32"), list[8,16,32,128,], )

[paddle error] paddle.expand(Tensor([16, 0, 128],"float32"), list[8,16,32,128,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.317336 test begin: paddle.expand(Tensor([16, 32, 0],"float32"), list[8,16,32,128,], )

[paddle error] paddle.expand(Tensor([16, 32, 0],"float32"), list[8,16,32,128,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (128) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.317915 test begin: paddle.expand(Tensor([2, 0, 1, 8, 128, 96],"float16"), list[2,2,1,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 0, 1, 8, 128, 96],"float16"), list[2,2,1,8,128,96,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.318503 test begin: paddle.expand(Tensor([2, 0, 1, 8, 128, 96],"float16"), list[2,2,2,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 0, 1, 8, 128, 96],"float16"), list[2,2,2,8,128,96,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.319055 test begin: paddle.expand(Tensor([2, 0, 7, 14],"float32"), shape=list[2,13,7,14,], )

[paddle error] paddle.expand(Tensor([2, 0, 7, 14],"float32"), shape=list[2,13,7,14,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (13) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.319518 test begin: paddle.expand(Tensor([2, 0, 8],"float32"), list[-1,8,-1,], )

[paddle error] paddle.expand(Tensor([2, 0, 8],"float32"), list[-1,8,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (8) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.319953 test begin: paddle.expand(Tensor([2, 0],"float32"), list[2,3072,], )

[paddle error] paddle.expand(Tensor([2, 0],"float32"), list[2,3072,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3072) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.320361 test begin: paddle.expand(Tensor([2, 1, 0, 14],"float32"), shape=list[2,13,7,14,], )

[paddle error] paddle.expand(Tensor([2, 1, 0, 14],"float32"), shape=list[2,13,7,14,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.320799 test begin: paddle.expand(Tensor([2, 1, 0],"float32"), list[-1,8,-1,], )

[Pass] paddle.expand(Tensor([2, 1, 0],"float32"), list[-1,8,-1,], )
2025-03-03 18:36:26.330675 test begin: paddle.expand(Tensor([2, 1, 7, 0],"float32"), shape=list[2,13,7,14,], )

[paddle error] paddle.expand(Tensor([2, 1, 7, 0],"float32"), shape=list[2,13,7,14,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (14) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.331242 test begin: paddle.expand(Tensor([2, 2, 0, 8, 128, 96],"float16"), list[2,2,1,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 2, 0, 8, 128, 96],"float16"), list[2,2,1,8,128,96,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.331804 test begin: paddle.expand(Tensor([2, 2, 0, 8, 128, 96],"float16"), list[2,2,2,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 2, 0, 8, 128, 96],"float16"), list[2,2,2,8,128,96,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.332366 test begin: paddle.expand(Tensor([2, 2, 1, 0, 128, 96],"float16"), list[2,2,1,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 2, 1, 0, 128, 96],"float16"), list[2,2,1,8,128,96,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (8) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.332842 test begin: paddle.expand(Tensor([2, 2, 1, 0, 128, 96],"float16"), list[2,2,2,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 2, 1, 0, 128, 96],"float16"), list[2,2,2,8,128,96,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (8) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.333288 test begin: paddle.expand(Tensor([2, 2, 1, 8, 0, 96],"float16"), list[2,2,1,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 2, 1, 8, 0, 96],"float16"), list[2,2,1,8,128,96,], ) 
 (InvalidArgument) The 4-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (128) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.333741 test begin: paddle.expand(Tensor([2, 2, 1, 8, 0, 96],"float16"), list[2,2,2,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 2, 1, 8, 0, 96],"float16"), list[2,2,2,8,128,96,], ) 
 (InvalidArgument) The 4-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (128) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.334182 test begin: paddle.expand(Tensor([2, 2, 1, 8, 128, 0],"float16"), list[2,2,1,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 2, 1, 8, 128, 0],"float16"), list[2,2,1,8,128,96,], ) 
 (InvalidArgument) The 5-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (96) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.334641 test begin: paddle.expand(Tensor([2, 2, 1, 8, 128, 0],"float16"), list[2,2,2,8,128,96,], )

[paddle error] paddle.expand(Tensor([2, 2, 1, 8, 128, 0],"float16"), list[2,2,2,8,128,96,], ) 
 (InvalidArgument) The 5-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (96) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.335092 test begin: paddle.expand(Tensor([3, 0],"float16"), shape=list[512,3,2,], )

[paddle error] paddle.expand(Tensor([3, 0],"float16"), shape=list[512,3,2,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.335505 test begin: paddle.expand(Tensor([3, 0],"float32"), shape=list[512,3,2,], )

[paddle error] paddle.expand(Tensor([3, 0],"float32"), shape=list[512,3,2,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.335935 test begin: paddle.expand(Tensor([32, 0],"float32"), list[128,32,32,], )

[paddle error] paddle.expand(Tensor([32, 0],"float32"), list[128,32,32,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.336344 test begin: paddle.expand(Tensor([32, 0],"float32"), list[16,32,128,], )

[paddle error] paddle.expand(Tensor([32, 0],"float32"), list[16,32,128,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (128) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.336759 test begin: paddle.expand(Tensor([32, 0],"float32"), list[16,32,64,], )

[paddle error] paddle.expand(Tensor([32, 0],"float32"), list[16,32,64,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (64) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.337165 test begin: paddle.expand(Tensor([32, 0],"float32"), list[96,32,32,], )

[paddle error] paddle.expand(Tensor([32, 0],"float32"), list[96,32,32,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.337564 test begin: paddle.expand(Tensor([4, 0],"int64"), tuple(4,32,), name=None, )

[paddle error] paddle.expand(Tensor([4, 0],"int64"), tuple(4,32,), name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.338088 test begin: paddle.expand(Tensor([52, 0, 1, 1, 1, 8],"float32"), list[52,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([52, 0, 1, 1, 1, 8],"float32"), list[52,4,3,1,1,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.338542 test begin: paddle.expand(Tensor([52, 0, 5, 1, 5],"float32"), list[52,4,5,1,-1,], )

[paddle error] paddle.expand(Tensor([52, 0, 5, 1, 5],"float32"), list[52,4,5,1,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.339072 test begin: paddle.expand(Tensor([52, 0, 7, 1],"float32"), list[52,4,7,-1,], )

[paddle error] paddle.expand(Tensor([52, 0, 7, 1],"float32"), list[52,4,7,-1,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (4) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.339538 test begin: paddle.expand(Tensor([52, 1, 0, 1, 5],"float32"), list[52,4,5,1,-1,], )

[paddle error] paddle.expand(Tensor([52, 1, 0, 1, 5],"float32"), list[52,4,5,1,-1,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (5) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.340001 test begin: paddle.expand(Tensor([52, 1, 0, 1],"float32"), list[52,4,7,-1,], )

[paddle error] paddle.expand(Tensor([52, 1, 0, 1],"float32"), list[52,4,7,-1,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (7) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.340491 test begin: paddle.expand(Tensor([52, 1, 5, 0, 5],"float32"), list[52,4,5,1,-1,], )

[paddle error] paddle.expand(Tensor([52, 1, 5, 0, 5],"float32"), list[52,4,5,1,-1,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.340989 test begin: paddle.expand(Tensor([52, 1, 5, 1, 0],"float32"), list[52,4,5,1,-1,], )

[Pass] paddle.expand(Tensor([52, 1, 5, 1, 0],"float32"), list[52,4,5,1,-1,], )
2025-03-03 18:36:26.348898 test begin: paddle.expand(Tensor([52, 1, 7, 0],"float32"), list[52,4,7,-1,], )

[Pass] paddle.expand(Tensor([52, 1, 7, 0],"float32"), list[52,4,7,-1,], )
2025-03-03 18:36:26.364442 test begin: paddle.expand(Tensor([52, 4, 0, 1, 1, 8],"float32"), list[52,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([52, 4, 0, 1, 1, 8],"float32"), list[52,4,3,1,1,-1,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.365219 test begin: paddle.expand(Tensor([52, 4, 1, 0, 1, 8],"float32"), list[52,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([52, 4, 1, 0, 1, 8],"float32"), list[52,4,3,1,1,-1,], ) 
 (InvalidArgument) The 3-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.365778 test begin: paddle.expand(Tensor([52, 4, 1, 1, 0, 8],"float32"), list[52,4,3,1,1,-1,], )

[paddle error] paddle.expand(Tensor([52, 4, 1, 1, 0, 8],"float32"), list[52,4,3,1,1,-1,], ) 
 (InvalidArgument) The 4-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.366363 test begin: paddle.expand(Tensor([52, 4, 1, 1, 1, 0],"float32"), list[52,4,3,1,1,-1,], )

[Pass] paddle.expand(Tensor([52, 4, 1, 1, 1, 0],"float32"), list[52,4,3,1,1,-1,], )
2025-03-03 18:36:26.369296 test begin: paddle.expand(Tensor([64, 0],"float32"), list[32,64,128,], )

[paddle error] paddle.expand(Tensor([64, 0],"float32"), list[32,64,128,], ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (128) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.369813 test begin: paddle.expand(Tensor([8, 0],"int64"), tuple(8,32,), name=None, )

[paddle error] paddle.expand(Tensor([8, 0],"int64"), tuple(8,32,), name=None, ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (32) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.370338 test begin: paddle.expand(x=Tensor([0, 1, 1],"int64"), shape=Tensor([3],"int32"), )

[Pass] paddle.expand(x=Tensor([0, 1, 1],"int64"), shape=Tensor([3],"int32"), )
2025-03-03 18:36:26.373541 test begin: paddle.expand(x=Tensor([0, 1],"int64"), shape=tuple(3,2,), )

[paddle error] paddle.expand(x=Tensor([0, 1],"int64"), shape=tuple(3,2,), ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.374130 test begin: paddle.expand(x=Tensor([0],"float64"), shape=list[3,3,], )

[paddle error] paddle.expand(x=Tensor([0],"float64"), shape=list[3,3,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.374630 test begin: paddle.expand(x=Tensor([0],"int32"), shape=list[1,], )

[paddle error] paddle.expand(x=Tensor([0],"int32"), shape=list[1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.375085 test begin: paddle.expand(x=Tensor([0],"int32"), shape=tuple(3,3,), )

[paddle error] paddle.expand(x=Tensor([0],"int32"), shape=tuple(3,3,), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.375650 test begin: paddle.expand(x=Tensor([0],"int64"), shape=list[1,-1,], )

[Pass] paddle.expand(x=Tensor([0],"int64"), shape=list[1,-1,], )
2025-03-03 18:36:26.377292 test begin: paddle.expand(x=Tensor([0],"int64"), shape=list[1,1,1,1,1,1,], )

[paddle error] paddle.expand(x=Tensor([0],"int64"), shape=list[1,1,1,1,1,1,], ) 
 (InvalidArgument) The 5-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.377889 test begin: paddle.expand(x=Tensor([0],"int64"), shape=list[1,], )

[paddle error] paddle.expand(x=Tensor([0],"int64"), shape=list[1,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (1) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.378341 test begin: paddle.expand(x=Tensor([0],"int64"), shape=list[2,3,], )

[paddle error] paddle.expand(x=Tensor([0],"int64"), shape=list[2,3,], ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (3) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.378830 test begin: paddle.expand(x=Tensor([0],"int64"), shape=list[2,], )

[paddle error] paddle.expand(x=Tensor([0],"int64"), shape=list[2,], ) 
 (InvalidArgument) The 0-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.379266 test begin: paddle.expand(x=Tensor([1, 0, 1],"int64"), shape=Tensor([3],"int32"), )

[paddle error] paddle.expand(x=Tensor([1, 0, 1],"int64"), shape=Tensor([3],"int32"), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (53588) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.381316 test begin: paddle.expand(x=Tensor([1, 1, 0],"int64"), shape=Tensor([3],"int32"), )

[paddle error] paddle.expand(x=Tensor([1, 1, 0],"int64"), shape=Tensor([3],"int32"), ) 
 (InvalidArgument) The 2-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (11068) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:26.382341 test begin: paddle.expand(x=Tensor([1, 1, 1],"int64"), shape=Tensor([0],"int32"), )

terminate called after throwing an instance of 'std::length_error'
  what():  vector::_M_fill_insert


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::ThrowExceptionToPython(std::__exception_ptr::exception_ptr)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998186 (unix time) try "date -d @1740998186" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16032) received by PID 90162 (TID 0x7fcdc47c3700) from PID 90162 ***]

2025-03-03 18:36:30.720042 test begin: paddle.expand(x=Tensor([3, 0],"int64"), shape=tuple(3,2,), )

W0303 18:36:33.724473 92547 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:36:33.725768 92547 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.expand(x=Tensor([3, 0],"int64"), shape=tuple(3,2,), ) 
 (InvalidArgument) The 1-th dimension of input tensor (0) must match or be broadcastable to the corresponding dimension (2) in shape.
  [Hint: Expected vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i] == true, but received vec_in_dims[i] == 1 || vec_in_dims[i] == expand_shape[i]:0 != true:1.] (at ../paddle/phi/kernels/gpu/expand_kernel.cu:71)

2025-03-03 18:36:33.726925 test begin: paddle.expand_as(Tensor([0, 128],"int64"), Tensor([0, 128],"int64"), )

[paddle error] paddle.expand_as(Tensor([0, 128],"int64"), Tensor([0, 128],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.732812 test begin: paddle.expand_as(Tensor([0, 128],"int64"), Tensor([32, 128],"int64"), )

[paddle error] paddle.expand_as(Tensor([0, 128],"int64"), Tensor([32, 128],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (32) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:32.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.739312 test begin: paddle.expand_as(Tensor([0, 128],"int64"), Tensor([7, 128],"int64"), )

[paddle error] paddle.expand_as(Tensor([0, 128],"int64"), Tensor([7, 128],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (7) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:7.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.743461 test begin: paddle.expand_as(Tensor([0, 2, 1, 1],"float32"), Tensor([0, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([0, 2, 1, 1],"float32"), Tensor([0, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.747270 test begin: paddle.expand_as(Tensor([0, 2, 1, 1],"float32"), Tensor([1, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([0, 2, 1, 1],"float32"), Tensor([1, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (1) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:1.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.751155 test begin: paddle.expand_as(Tensor([0, 2, 1, 1],"float32"), Tensor([10, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([0, 2, 1, 1],"float32"), Tensor([10, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (10) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:10.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.755494 test begin: paddle.expand_as(Tensor([0, 500, 1],"int32"), Tensor([0, 500, 128],"float16"), )

[paddle error] paddle.expand_as(Tensor([0, 500, 1],"int32"), Tensor([0, 500, 128],"float16"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.756742 test begin: paddle.expand_as(Tensor([0, 500, 1],"int32"), Tensor([0, 500, 128],"float32"), )

[paddle error] paddle.expand_as(Tensor([0, 500, 1],"int32"), Tensor([0, 500, 128],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.758559 test begin: paddle.expand_as(Tensor([0, 500, 1],"int32"), Tensor([20, 500, 128],"float16"), )

[paddle error] paddle.expand_as(Tensor([0, 500, 1],"int32"), Tensor([20, 500, 128],"float16"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (20) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:20.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.782884 test begin: paddle.expand_as(Tensor([0, 500, 1],"int32"), Tensor([20, 500, 128],"float32"), )

[paddle error] paddle.expand_as(Tensor([0, 500, 1],"int32"), Tensor([20, 500, 128],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (20) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:20.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.799276 test begin: paddle.expand_as(Tensor([0],"float32"), Tensor([1, 1],"int64"), )

[paddle error] paddle.expand_as(Tensor([0],"float32"), Tensor([1, 1],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (1) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:1.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.801371 test begin: paddle.expand_as(Tensor([0],"float32"), Tensor([1, 2],"int64"), )

[paddle error] paddle.expand_as(Tensor([0],"float32"), Tensor([1, 2],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (2) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:2.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.803007 test begin: paddle.expand_as(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 0, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 0, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.804154 test begin: paddle.expand_as(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 0, 1, 1],"float32"), Tensor([1, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (2) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:2.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.805718 test begin: paddle.expand_as(Tensor([1, 0],"int64"), Tensor([32, 0],"int64"), )

[paddle error] paddle.expand_as(Tensor([1, 0],"int64"), Tensor([32, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.806591 test begin: paddle.expand_as(Tensor([1, 0],"int64"), Tensor([32, 128],"int64"), )

[paddle error] paddle.expand_as(Tensor([1, 0],"int64"), Tensor([32, 128],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (128) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:128.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.807955 test begin: paddle.expand_as(Tensor([1, 0],"int64"), Tensor([7, 0],"int64"), )

[paddle error] paddle.expand_as(Tensor([1, 0],"int64"), Tensor([7, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.808722 test begin: paddle.expand_as(Tensor([1, 0],"int64"), Tensor([7, 128],"int64"), )

[paddle error] paddle.expand_as(Tensor([1, 0],"int64"), Tensor([7, 128],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (128) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:128.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.809884 test begin: paddle.expand_as(Tensor([1, 128],"int64"), Tensor([0, 128],"int64"), )

[paddle error] paddle.expand_as(Tensor([1, 128],"int64"), Tensor([0, 128],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.811025 test begin: paddle.expand_as(Tensor([1, 128],"int64"), Tensor([32, 0],"int64"), )

[paddle error] paddle.expand_as(Tensor([1, 128],"int64"), Tensor([32, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.812702 test begin: paddle.expand_as(Tensor([1, 128],"int64"), Tensor([7, 0],"int64"), )

[paddle error] paddle.expand_as(Tensor([1, 128],"int64"), Tensor([7, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.813802 test begin: paddle.expand_as(Tensor([1, 2, 0, 1],"float32"), Tensor([1, 2, 0, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 2, 0, 1],"float32"), Tensor([1, 2, 0, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.814558 test begin: paddle.expand_as(Tensor([1, 2, 0, 1],"float32"), Tensor([1, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 2, 0, 1],"float32"), Tensor([1, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (28) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:28.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.815791 test begin: paddle.expand_as(Tensor([1, 2, 1, 0],"float32"), Tensor([1, 2, 28, 0],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 2, 1, 0],"float32"), Tensor([1, 2, 28, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.816505 test begin: paddle.expand_as(Tensor([1, 2, 1, 0],"float32"), Tensor([1, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 2, 1, 0],"float32"), Tensor([1, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (28) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:28.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.817793 test begin: paddle.expand_as(Tensor([1, 2, 1, 1],"float32"), Tensor([0, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 2, 1, 1],"float32"), Tensor([0, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.819028 test begin: paddle.expand_as(Tensor([1, 2, 1, 1],"float32"), Tensor([1, 0, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 2, 1, 1],"float32"), Tensor([1, 0, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.820274 test begin: paddle.expand_as(Tensor([1, 2, 1, 1],"float32"), Tensor([1, 2, 0, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 2, 1, 1],"float32"), Tensor([1, 2, 0, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.821527 test begin: paddle.expand_as(Tensor([1, 2, 1, 1],"float32"), Tensor([1, 2, 28, 0],"float32"), )

[paddle error] paddle.expand_as(Tensor([1, 2, 1, 1],"float32"), Tensor([1, 2, 28, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.822714 test begin: paddle.expand_as(Tensor([10, 0, 1, 1],"float32"), Tensor([10, 0, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 0, 1, 1],"float32"), Tensor([10, 0, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.823355 test begin: paddle.expand_as(Tensor([10, 0, 1, 1],"float32"), Tensor([10, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 0, 1, 1],"float32"), Tensor([10, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (2) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:2.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.824748 test begin: paddle.expand_as(Tensor([10, 2, 0, 1],"float32"), Tensor([10, 2, 0, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 2, 0, 1],"float32"), Tensor([10, 2, 0, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.825464 test begin: paddle.expand_as(Tensor([10, 2, 0, 1],"float32"), Tensor([10, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 2, 0, 1],"float32"), Tensor([10, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (28) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:28.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.826763 test begin: paddle.expand_as(Tensor([10, 2, 1, 0],"float32"), Tensor([10, 2, 28, 0],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 2, 1, 0],"float32"), Tensor([10, 2, 28, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.827528 test begin: paddle.expand_as(Tensor([10, 2, 1, 0],"float32"), Tensor([10, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 2, 1, 0],"float32"), Tensor([10, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (28) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:28.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.831649 test begin: paddle.expand_as(Tensor([10, 2, 1, 1],"float32"), Tensor([0, 2, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 2, 1, 1],"float32"), Tensor([0, 2, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.832883 test begin: paddle.expand_as(Tensor([10, 2, 1, 1],"float32"), Tensor([10, 0, 28, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 2, 1, 1],"float32"), Tensor([10, 0, 28, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.833945 test begin: paddle.expand_as(Tensor([10, 2, 1, 1],"float32"), Tensor([10, 2, 0, 28],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 2, 1, 1],"float32"), Tensor([10, 2, 0, 28],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.836172 test begin: paddle.expand_as(Tensor([10, 2, 1, 1],"float32"), Tensor([10, 2, 28, 0],"float32"), )

[paddle error] paddle.expand_as(Tensor([10, 2, 1, 1],"float32"), Tensor([10, 2, 28, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.837330 test begin: paddle.expand_as(Tensor([1],"float32"), Tensor([0, 1],"int64"), )

[paddle error] paddle.expand_as(Tensor([1],"float32"), Tensor([0, 1],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.838429 test begin: paddle.expand_as(Tensor([1],"float32"), Tensor([1, 0],"int64"), )

[paddle error] paddle.expand_as(Tensor([1],"float32"), Tensor([1, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.840276 test begin: paddle.expand_as(Tensor([20, 0, 1],"int32"), Tensor([20, 0, 128],"float16"), )

[paddle error] paddle.expand_as(Tensor([20, 0, 1],"int32"), Tensor([20, 0, 128],"float16"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.841736 test begin: paddle.expand_as(Tensor([20, 0, 1],"int32"), Tensor([20, 0, 128],"float32"), )

[paddle error] paddle.expand_as(Tensor([20, 0, 1],"int32"), Tensor([20, 0, 128],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.842334 test begin: paddle.expand_as(Tensor([20, 0, 1],"int32"), Tensor([20, 500, 128],"float16"), )

[paddle error] paddle.expand_as(Tensor([20, 0, 1],"int32"), Tensor([20, 500, 128],"float16"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (500) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:500.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.861162 test begin: paddle.expand_as(Tensor([20, 0, 1],"int32"), Tensor([20, 500, 128],"float32"), )

[paddle error] paddle.expand_as(Tensor([20, 0, 1],"int32"), Tensor([20, 500, 128],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (500) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:500.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.873979 test begin: paddle.expand_as(Tensor([20, 500, 0],"int32"), Tensor([20, 500, 0],"float16"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 0],"int32"), Tensor([20, 500, 0],"float16"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.876858 test begin: paddle.expand_as(Tensor([20, 500, 0],"int32"), Tensor([20, 500, 0],"float32"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 0],"int32"), Tensor([20, 500, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.877655 test begin: paddle.expand_as(Tensor([20, 500, 0],"int32"), Tensor([20, 500, 128],"float16"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 0],"int32"), Tensor([20, 500, 128],"float16"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (128) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:128.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.903327 test begin: paddle.expand_as(Tensor([20, 500, 0],"int32"), Tensor([20, 500, 128],"float32"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 0],"int32"), Tensor([20, 500, 128],"float32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (128) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:128.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.923071 test begin: paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([0, 500, 128],"float16"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([0, 500, 128],"float16"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.924841 test begin: paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([0, 500, 128],"float32"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([0, 500, 128],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.927162 test begin: paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([20, 0, 128],"float16"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([20, 0, 128],"float16"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.928833 test begin: paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([20, 0, 128],"float32"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([20, 0, 128],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.930377 test begin: paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([20, 500, 0],"float16"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([20, 500, 0],"float16"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.931746 test begin: paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([20, 500, 0],"float32"), )

[paddle error] paddle.expand_as(Tensor([20, 500, 1],"int32"), Tensor([20, 500, 0],"float32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.933003 test begin: paddle.expand_as(Tensor([2],"float32"), Tensor([0, 2],"int64"), )

[paddle error] paddle.expand_as(Tensor([2],"float32"), Tensor([0, 2],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.934183 test begin: paddle.expand_as(Tensor([2],"float32"), Tensor([1, 0],"int64"), )

[paddle error] paddle.expand_as(Tensor([2],"float32"), Tensor([1, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.935869 test begin: paddle.expand_as(x=Tensor([0, 1],"int64"), y=Tensor([0, 2],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([0, 1],"int64"), y=Tensor([0, 2],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.937106 test begin: paddle.expand_as(x=Tensor([0, 1],"int64"), y=Tensor([3, 2],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([0, 1],"int64"), y=Tensor([3, 2],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (3) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:3.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.938937 test begin: paddle.expand_as(x=Tensor([0, 2],"int32"), y=Tensor([1, 2, 2],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([0, 2],"int32"), y=Tensor([1, 2, 2],"int32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (2) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:2.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.940064 test begin: paddle.expand_as(x=Tensor([0],"float64"), y=Tensor([3, 3],"float64"), )

[paddle error] paddle.expand_as(x=Tensor([0],"float64"), y=Tensor([3, 3],"float64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (3) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:3.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.941524 test begin: paddle.expand_as(x=Tensor([0],"int32"), y=Tensor([0],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([0],"int32"), y=Tensor([0],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.942204 test begin: paddle.expand_as(x=Tensor([0],"int32"), y=Tensor([1],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([0],"int32"), y=Tensor([1],"int32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (1) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:1.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.945390 test begin: paddle.expand_as(x=Tensor([0],"int32"), y=Tensor([2, 1, 2],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([0],"int32"), y=Tensor([2, 1, 2],"int32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (2) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:2.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.948169 test begin: paddle.expand_as(x=Tensor([0],"int32"), y=Tensor([3, 3],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([0],"int32"), y=Tensor([3, 3],"int32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (3) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:3.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.949802 test begin: paddle.expand_as(x=Tensor([0],"int64"), y=Tensor([1, 1, 1, 1, 1, 1],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([0],"int64"), y=Tensor([1, 1, 1, 1, 1, 1],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (1) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:1.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.951127 test begin: paddle.expand_as(x=Tensor([1],"int32"), y=Tensor([0],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([1],"int32"), y=Tensor([0],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.952141 test begin: paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([0, 1, 1, 1, 1, 1],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([0, 1, 1, 1, 1, 1],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.953141 test begin: paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 0, 1, 1, 1, 1],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 0, 1, 1, 1, 1],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.954091 test begin: paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 1, 0, 1, 1, 1],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 1, 0, 1, 1, 1],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.955071 test begin: paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 1, 1, 0, 1, 1],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 1, 1, 0, 1, 1],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.955993 test begin: paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 1, 1, 1, 0, 1],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 1, 1, 1, 0, 1],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.957008 test begin: paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 1, 1, 1, 1, 0],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([1],"int64"), y=Tensor([1, 1, 1, 1, 1, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.957975 test begin: paddle.expand_as(x=Tensor([2, 0],"int32"), y=Tensor([1, 2, 2],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([2, 0],"int32"), y=Tensor([1, 2, 2],"int32"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (2) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:2.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.958970 test begin: paddle.expand_as(x=Tensor([2, 2],"int32"), y=Tensor([0, 2, 2],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([2, 2],"int32"), y=Tensor([0, 2, 2],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.959908 test begin: paddle.expand_as(x=Tensor([2, 2],"int32"), y=Tensor([1, 0, 2],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([2, 2],"int32"), y=Tensor([1, 0, 2],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.960889 test begin: paddle.expand_as(x=Tensor([2, 2],"int32"), y=Tensor([1, 2, 0],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([2, 2],"int32"), y=Tensor([1, 2, 0],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.961842 test begin: paddle.expand_as(x=Tensor([2],"int32"), y=Tensor([0, 1, 2],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([2],"int32"), y=Tensor([0, 1, 2],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.962835 test begin: paddle.expand_as(x=Tensor([2],"int32"), y=Tensor([2, 0, 2],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([2],"int32"), y=Tensor([2, 0, 2],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.964563 test begin: paddle.expand_as(x=Tensor([2],"int32"), y=Tensor([2, 1, 0],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([2],"int32"), y=Tensor([2, 1, 0],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.965700 test begin: paddle.expand_as(x=Tensor([3, 0],"int64"), y=Tensor([3, 0],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([3, 0],"int64"), y=Tensor([3, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.966298 test begin: paddle.expand_as(x=Tensor([3, 0],"int64"), y=Tensor([3, 2],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([3, 0],"int64"), y=Tensor([3, 2],"int64"), ) 
 (InvalidArgument) The value (0) of the non-singleton dimension does not match the corresponding value (2) in shape for expand_as_v2 op.
  [Hint: Expected vec_in_dims[i] == target_shape[i], but received vec_in_dims[i]:0 != target_shape[i]:2.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:67)

2025-03-03 18:36:33.967224 test begin: paddle.expand_as(x=Tensor([3, 1],"int64"), y=Tensor([0, 2],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([3, 1],"int64"), y=Tensor([0, 2],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.968609 test begin: paddle.expand_as(x=Tensor([3, 1],"int64"), y=Tensor([3, 0],"int64"), )

[paddle error] paddle.expand_as(x=Tensor([3, 1],"int64"), y=Tensor([3, 0],"int64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.969867 test begin: paddle.expand_as(x=Tensor([3],"float64"), y=Tensor([0, 3],"float64"), )

[paddle error] paddle.expand_as(x=Tensor([3],"float64"), y=Tensor([0, 3],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.971067 test begin: paddle.expand_as(x=Tensor([3],"float64"), y=Tensor([3, 0],"float64"), )

[paddle error] paddle.expand_as(x=Tensor([3],"float64"), y=Tensor([3, 0],"float64"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.972245 test begin: paddle.expand_as(x=Tensor([3],"int32"), y=Tensor([0, 3],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([3],"int32"), y=Tensor([0, 3],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.973556 test begin: paddle.expand_as(x=Tensor([3],"int32"), y=Tensor([3, 0],"int32"), )

[paddle error] paddle.expand_as(x=Tensor([3],"int32"), y=Tensor([3, 0],"int32"), ) 
 (InvalidArgument) The value of target shape cannot be zero.
  [Hint: Expected target_shape[i] != 0, but received target_shape[i]:0 == 0:0.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:49)

2025-03-03 18:36:33.974613 test begin: paddle.fft.rfft2(x=Tensor([0, 2, 2],"float64"), s=list[1,2,], norm="ortho", axes=None, )

[paddle error] paddle.fft.rfft2(x=Tensor([0, 2, 2],"float64"), s=list[1,2,], norm="ortho", axes=None, ) 
 (External) CUFFT error(8). 
  [Hint: 'CUFFT_INVALID_SIZE'. User specified an invalid transform size] (at ../paddle/phi/kernels/funcs/cufft_util.h:127)

2025-03-03 18:36:33.993302 test begin: paddle.fft.rfft2(x=Tensor([2, 0, 2],"float64"), s=list[1,2,], norm="ortho", axes=None, )

[Pass] paddle.fft.rfft2(x=Tensor([2, 0, 2],"float64"), s=list[1,2,], norm="ortho", axes=None, )
2025-03-03 18:36:34.027456 test begin: paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], norm="ortho", axes=None, )

W0303 18:36:34.031476 92673 backward.cc:437] While running Node (SliceGradNode) raises an EnforceNotMet exception
[paddle error] paddle.fft.rfft2(x=Tensor([2, 2, 0],"float64"), s=list[1,2,], norm="ortho", axes=None, ) 
 (InvalidArgument) StridedCopyKernel's out tensor must complete mutable data before call kernel.
  [Hint: output_data should not be null.] (at ../paddle/phi/kernels/gpu/strided_copy_kernel.cu:1299)

2025-03-03 18:36:34.031840 test begin: paddle.flip(Tensor([0, 3, 112, 112],"float32"), axis=-1, )

[cuda error] paddle.flip(Tensor([0, 3, 112, 112],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.033828 test begin: paddle.flip(Tensor([0, 3],"float32"), 1, )

[cuda error] paddle.flip(Tensor([0, 3],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.035379 test begin: paddle.flip(Tensor([2, 0],"float32"), 1, )

[cuda error] paddle.flip(Tensor([2, 0],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.036791 test begin: paddle.flip(Tensor([20, 0, 112, 112],"float32"), axis=-1, )

[cuda error] paddle.flip(Tensor([20, 0, 112, 112],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.037787 test begin: paddle.flip(Tensor([20, 3, 0, 112],"float32"), axis=-1, )

[cuda error] paddle.flip(Tensor([20, 3, 0, 112],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.038698 test begin: paddle.flip(Tensor([20, 3, 112, 0],"float32"), axis=-1, )

[cuda error] paddle.flip(Tensor([20, 3, 112, 0],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.039622 test begin: paddle.flip(Tensor([32, 0, 112, 112],"float32"), axis=-1, )

[cuda error] paddle.flip(Tensor([32, 0, 112, 112],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.040475 test begin: paddle.flip(Tensor([32, 3, 0, 112],"float32"), axis=-1, )

[cuda error] paddle.flip(Tensor([32, 3, 0, 112],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.041876 test begin: paddle.flip(Tensor([32, 3, 112, 0],"float32"), axis=-1, )

[cuda error] paddle.flip(Tensor([32, 3, 112, 0],"float32"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.043213 test begin: paddle.floor_divide(Tensor([0, 1024],"int64"), Tensor([0, 1024],"int64"), )

[Pass] paddle.floor_divide(Tensor([0, 1024],"int64"), Tensor([0, 1024],"int64"), )
2025-03-03 18:36:34.044067 test begin: paddle.floor_divide(Tensor([0, 1024],"int64"), Tensor([10, 1024],"int64"), )

[paddle error] paddle.floor_divide(Tensor([0, 1024],"int64"), Tensor([10, 1024],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1024] and the shape of Y = [10, 1024]. Received [0] in X is not equal to [10] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.045041 test begin: paddle.floor_divide(Tensor([0, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )

[Pass] paddle.floor_divide(Tensor([0, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )
2025-03-03 18:36:34.050131 test begin: paddle.floor_divide(Tensor([0, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), )

[paddle error] paddle.floor_divide(Tensor([0, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 20, 1] and the shape of Y = [10, 20, 1]. Received [0] in X is not equal to [10] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.051062 test begin: paddle.floor_divide(Tensor([0, 3, 2],"bfloat16"), Tensor([0, 3, 2],"float16"), )

W0303 18:36:34.051754 92687 dygraph_functions.cc:88704] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.floor_divide(Tensor([0, 3, 2],"bfloat16"), Tensor([0, 3, 2],"float16"), )
2025-03-03 18:36:34.052032 test begin: paddle.floor_divide(Tensor([0, 3, 2],"bfloat16"), Tensor([4, 3, 2],"float16"), )

[paddle error] paddle.floor_divide(Tensor([0, 3, 2],"bfloat16"), Tensor([4, 3, 2],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 2] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.053043 test begin: paddle.floor_divide(Tensor([0, 4],"int64"), Tensor([1],"int64"), )

[Pass] paddle.floor_divide(Tensor([0, 4],"int64"), Tensor([1],"int64"), )
2025-03-03 18:36:34.053919 test begin: paddle.floor_divide(Tensor([0, 8],"int64"), Tensor([1],"int64"), )

[Pass] paddle.floor_divide(Tensor([0, 8],"int64"), Tensor([1],"int64"), )
2025-03-03 18:36:34.054744 test begin: paddle.floor_divide(Tensor([0],"bfloat16"), Tensor([0],"bfloat16"), )

[Pass] paddle.floor_divide(Tensor([0],"bfloat16"), Tensor([0],"bfloat16"), )
2025-03-03 18:36:34.055883 test begin: paddle.floor_divide(Tensor([0],"bfloat16"), Tensor([3],"bfloat16"), )

[paddle error] paddle.floor_divide(Tensor([0],"bfloat16"), Tensor([3],"bfloat16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.057767 test begin: paddle.floor_divide(Tensor([0],"float16"), Tensor([0],"float16"), )

[Pass] paddle.floor_divide(Tensor([0],"float16"), Tensor([0],"float16"), )
2025-03-03 18:36:34.058802 test begin: paddle.floor_divide(Tensor([0],"float16"), Tensor([3],"float16"), )

[paddle error] paddle.floor_divide(Tensor([0],"float16"), Tensor([3],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.059632 test begin: paddle.floor_divide(Tensor([10, 0, 1],"float32"), Tensor([10, 0, 1],"float32"), )

[Pass] paddle.floor_divide(Tensor([10, 0, 1],"float32"), Tensor([10, 0, 1],"float32"), )
2025-03-03 18:36:34.060237 test begin: paddle.floor_divide(Tensor([10, 0, 1],"float32"), Tensor([10, 20, 1],"float32"), )

[paddle error] paddle.floor_divide(Tensor([10, 0, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 0, 1] and the shape of Y = [10, 20, 1]. Received [0] in X is not equal to [20] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.060915 test begin: paddle.floor_divide(Tensor([10, 0],"int64"), Tensor([10, 0],"int64"), )

[Pass] paddle.floor_divide(Tensor([10, 0],"int64"), Tensor([10, 0],"int64"), )
2025-03-03 18:36:34.061570 test begin: paddle.floor_divide(Tensor([10, 0],"int64"), Tensor([10, 1024],"int64"), )

[paddle error] paddle.floor_divide(Tensor([10, 0],"int64"), Tensor([10, 1024],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 0] and the shape of Y = [10, 1024]. Received [0] in X is not equal to [1024] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.062277 test begin: paddle.floor_divide(Tensor([10, 1024],"int64"), Tensor([0, 1024],"int64"), )

[paddle error] paddle.floor_divide(Tensor([10, 1024],"int64"), Tensor([0, 1024],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 1024] and the shape of Y = [0, 1024]. Received [10] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.062988 test begin: paddle.floor_divide(Tensor([10, 1024],"int64"), Tensor([10, 0],"int64"), )

[paddle error] paddle.floor_divide(Tensor([10, 1024],"int64"), Tensor([10, 0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 1024] and the shape of Y = [10, 0]. Received [1024] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.063681 test begin: paddle.floor_divide(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 0],"float32"), )

[Pass] paddle.floor_divide(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 0],"float32"), )
2025-03-03 18:36:34.064206 test begin: paddle.floor_divide(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 1],"float32"), )

[Pass] paddle.floor_divide(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 1],"float32"), )
2025-03-03 18:36:34.064976 test begin: paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )

[paddle error] paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 20, 1] and the shape of Y = [0, 20, 1]. Received [10] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.065646 test begin: paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 0, 1],"float32"), )

[paddle error] paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 0, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 20, 1] and the shape of Y = [10, 0, 1]. Received [20] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.066272 test begin: paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 0],"float32"), )

[Pass] paddle.floor_divide(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 0],"float32"), )
2025-03-03 18:36:34.066927 test begin: paddle.floor_divide(Tensor([1],"bfloat16"), Tensor([0],"bfloat16"), )

[Pass] paddle.floor_divide(Tensor([1],"bfloat16"), Tensor([0],"bfloat16"), )
2025-03-03 18:36:34.067778 test begin: paddle.floor_divide(Tensor([1],"float16"), Tensor([0],"float16"), )

[Pass] paddle.floor_divide(Tensor([1],"float16"), Tensor([0],"float16"), )
2025-03-03 18:36:34.068482 test begin: paddle.floor_divide(Tensor([4, 0, 2],"bfloat16"), Tensor([4, 0, 2],"float16"), )

[Pass] paddle.floor_divide(Tensor([4, 0, 2],"bfloat16"), Tensor([4, 0, 2],"float16"), )
2025-03-03 18:36:34.069088 test begin: paddle.floor_divide(Tensor([4, 0, 2],"bfloat16"), Tensor([4, 3, 2],"float16"), )

[paddle error] paddle.floor_divide(Tensor([4, 0, 2],"bfloat16"), Tensor([4, 3, 2],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 0, 2] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.069887 test begin: paddle.floor_divide(Tensor([4, 0],"int64"), Tensor([1],"int64"), )

[Pass] paddle.floor_divide(Tensor([4, 0],"int64"), Tensor([1],"int64"), )
2025-03-03 18:36:34.070970 test begin: paddle.floor_divide(Tensor([4, 3, 0],"bfloat16"), Tensor([4, 3, 0],"float16"), )

[Pass] paddle.floor_divide(Tensor([4, 3, 0],"bfloat16"), Tensor([4, 3, 0],"float16"), )
2025-03-03 18:36:34.071840 test begin: paddle.floor_divide(Tensor([4, 3, 0],"bfloat16"), Tensor([4, 3, 2],"float16"), )

[paddle error] paddle.floor_divide(Tensor([4, 3, 0],"bfloat16"), Tensor([4, 3, 2],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 0] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.072724 test begin: paddle.floor_divide(Tensor([4, 3, 2],"bfloat16"), Tensor([0, 3, 2],"float16"), )

[paddle error] paddle.floor_divide(Tensor([4, 3, 2],"bfloat16"), Tensor([0, 3, 2],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [0, 3, 2]. Received [4] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.073663 test begin: paddle.floor_divide(Tensor([4, 3, 2],"bfloat16"), Tensor([4, 0, 2],"float16"), )

[paddle error] paddle.floor_divide(Tensor([4, 3, 2],"bfloat16"), Tensor([4, 0, 2],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [4, 0, 2]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.074483 test begin: paddle.floor_divide(Tensor([4, 3, 2],"bfloat16"), Tensor([4, 3, 0],"float16"), )

[paddle error] paddle.floor_divide(Tensor([4, 3, 2],"bfloat16"), Tensor([4, 3, 0],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [4, 3, 0]. Received [2] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.075318 test begin: paddle.floor_divide(Tensor([4, 4],"int64"), Tensor([0],"int64"), )

[paddle error] paddle.floor_divide(Tensor([4, 4],"int64"), Tensor([0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 4] and the shape of Y = [0]. Received [4] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.076009 test begin: paddle.floor_divide(Tensor([4, 8],"int64"), Tensor([0],"int64"), )

[paddle error] paddle.floor_divide(Tensor([4, 8],"int64"), Tensor([0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 8] and the shape of Y = [0]. Received [8] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.076635 test begin: paddle.floor_divide(x=Tensor([0, 1],"int32"), y=Tensor([0, 1],"int32"), name=None, )

[Pass] paddle.floor_divide(x=Tensor([0, 1],"int32"), y=Tensor([0, 1],"int32"), name=None, )
2025-03-03 18:36:34.077271 test begin: paddle.floor_divide(x=Tensor([0, 1],"int32"), y=Tensor([3, 1],"int32"), name=None, )

[paddle error] paddle.floor_divide(x=Tensor([0, 1],"int32"), y=Tensor([3, 1],"int32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 1] and the shape of Y = [3, 1]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.077904 test begin: paddle.floor_divide(x=Tensor([0, 3, 3],"int32"), y=Tensor([3],"int32"), )

[Pass] paddle.floor_divide(x=Tensor([0, 3, 3],"int32"), y=Tensor([3],"int32"), )
2025-03-03 18:36:34.078548 test begin: paddle.floor_divide(x=Tensor([0],"int32"), y=Tensor([0],"int32"), )

[Pass] paddle.floor_divide(x=Tensor([0],"int32"), y=Tensor([0],"int32"), )
2025-03-03 18:36:34.079114 test begin: paddle.floor_divide(x=Tensor([0],"int32"), y=Tensor([2],"int32"), )

[paddle error] paddle.floor_divide(x=Tensor([0],"int32"), y=Tensor([2],"int32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [2]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.079747 test begin: paddle.floor_divide(x=Tensor([0],"int32"), y=Tensor([3],"int32"), )

[paddle error] paddle.floor_divide(x=Tensor([0],"int32"), y=Tensor([3],"int32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.080340 test begin: paddle.floor_divide(x=Tensor([2],"int32"), y=Tensor([0],"int32"), )

[paddle error] paddle.floor_divide(x=Tensor([2],"int32"), y=Tensor([0],"int32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2] and the shape of Y = [0]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.080948 test begin: paddle.floor_divide(x=Tensor([3, 0, 3],"int32"), y=Tensor([3],"int32"), )

[Pass] paddle.floor_divide(x=Tensor([3, 0, 3],"int32"), y=Tensor([3],"int32"), )
2025-03-03 18:36:34.081546 test begin: paddle.floor_divide(x=Tensor([3, 0],"int32"), y=Tensor([3, 0],"int32"), name=None, )

[Pass] paddle.floor_divide(x=Tensor([3, 0],"int32"), y=Tensor([3, 0],"int32"), name=None, )
2025-03-03 18:36:34.082114 test begin: paddle.floor_divide(x=Tensor([3, 0],"int32"), y=Tensor([3, 1],"int32"), name=None, )

[Pass] paddle.floor_divide(x=Tensor([3, 0],"int32"), y=Tensor([3, 1],"int32"), name=None, )
2025-03-03 18:36:34.082758 test begin: paddle.floor_divide(x=Tensor([3, 1],"int32"), y=Tensor([0, 1],"int32"), name=None, )

[paddle error] paddle.floor_divide(x=Tensor([3, 1],"int32"), y=Tensor([0, 1],"int32"), name=None, ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 1] and the shape of Y = [0, 1]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.083405 test begin: paddle.floor_divide(x=Tensor([3, 1],"int32"), y=Tensor([3, 0],"int32"), name=None, )

[Pass] paddle.floor_divide(x=Tensor([3, 1],"int32"), y=Tensor([3, 0],"int32"), name=None, )
2025-03-03 18:36:34.084059 test begin: paddle.floor_divide(x=Tensor([3, 3, 0],"int32"), y=Tensor([3],"int32"), )

[paddle error] paddle.floor_divide(x=Tensor([3, 3, 0],"int32"), y=Tensor([3],"int32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.084761 test begin: paddle.floor_divide(x=Tensor([3, 3, 3],"int32"), y=Tensor([0],"int32"), )

[paddle error] paddle.floor_divide(x=Tensor([3, 3, 3],"int32"), y=Tensor([0],"int32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3, 3] and the shape of Y = [0]. Received [3] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.085416 test begin: paddle.floor_divide(x=Tensor([3],"int32"), y=Tensor([0],"int32"), )

[paddle error] paddle.floor_divide(x=Tensor([3],"int32"), y=Tensor([0],"int32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3] and the shape of Y = [0]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.086018 test begin: paddle.frac(Tensor([0, 3],"int32"), )

[cuda error] paddle.frac(Tensor([0, 3],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.087857 test begin: paddle.frac(Tensor([0, 3],"int64"), )

[cuda error] paddle.frac(Tensor([0, 3],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.088952 test begin: paddle.frac(Tensor([2, 0],"int32"), )

[cuda error] paddle.frac(Tensor([2, 0],"int32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.089930 test begin: paddle.frac(Tensor([2, 0],"int64"), )

[cuda error] paddle.frac(Tensor([2, 0],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.090850 test begin: paddle.frexp(Tensor([0, 12],"float32"), )

[cuda error] paddle.frexp(Tensor([0, 12],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.093958 test begin: paddle.frexp(Tensor([0, 12],"float64"), )

[cuda error] paddle.frexp(Tensor([0, 12],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.096035 test begin: paddle.frexp(Tensor([0, 5, 2],"float32"), )

[cuda error] paddle.frexp(Tensor([0, 5, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.097555 test begin: paddle.frexp(Tensor([0, 5, 2],"float64"), )

[cuda error] paddle.frexp(Tensor([0, 5, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.098981 test begin: paddle.frexp(Tensor([10, 0],"float32"), )

[cuda error] paddle.frexp(Tensor([10, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.101617 test begin: paddle.frexp(Tensor([10, 0],"float64"), )

[cuda error] paddle.frexp(Tensor([10, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.103924 test begin: paddle.frexp(Tensor([4, 0, 2],"float32"), )

[cuda error] paddle.frexp(Tensor([4, 0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.105320 test begin: paddle.frexp(Tensor([4, 0, 2],"float64"), )

[cuda error] paddle.frexp(Tensor([4, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.107016 test begin: paddle.frexp(Tensor([4, 5, 0],"float32"), )

[cuda error] paddle.frexp(Tensor([4, 5, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.108302 test begin: paddle.frexp(Tensor([4, 5, 0],"float64"), )

[cuda error] paddle.frexp(Tensor([4, 5, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.109497 test begin: paddle.full_like(Tensor([0, 1, 1, 100],"bool"), -65504.0, dtype=Dtype(float16), )

[Pass] paddle.full_like(Tensor([0, 1, 1, 100],"bool"), -65504.0, dtype=Dtype(float16), )
2025-03-03 18:36:34.110244 test begin: paddle.full_like(Tensor([0, 1, 1, 23],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0, 1, 1, 23],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )
2025-03-03 18:36:34.110808 test begin: paddle.full_like(Tensor([0, 1, 1, 24],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0, 1, 1, 24],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )
2025-03-03 18:36:34.111884 test begin: paddle.full_like(Tensor([0, 1, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )

[Pass] paddle.full_like(Tensor([0, 1, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
2025-03-03 18:36:34.112694 test begin: paddle.full_like(Tensor([0, 128],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0, 128],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.113240 test begin: paddle.full_like(Tensor([0, 128],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0, 128],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.113809 test begin: paddle.full_like(Tensor([0, 160],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0, 160],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.114286 test begin: paddle.full_like(Tensor([0, 192, 3, 3],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0, 192, 3, 3],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.114757 test begin: paddle.full_like(Tensor([0, 192, 3, 3],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0, 192, 3, 3],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.115262 test begin: paddle.full_like(Tensor([0, 192],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0, 192],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.115727 test begin: paddle.full_like(Tensor([0, 1],"int64"), True, dtype=VarType(bool), )

[Pass] paddle.full_like(Tensor([0, 1],"int64"), True, dtype=VarType(bool), )
2025-03-03 18:36:34.117096 test begin: paddle.full_like(Tensor([0, 232],"float32"), 0.0, VarType(bfloat16), None, )

[Pass] paddle.full_like(Tensor([0, 232],"float32"), 0.0, VarType(bfloat16), None, )
2025-03-03 18:36:34.117940 test begin: paddle.full_like(Tensor([0, 232],"int32"), 0.0, Dtype(int16), None, )

[Pass] paddle.full_like(Tensor([0, 232],"int32"), 0.0, Dtype(int16), None, )
2025-03-03 18:36:34.118568 test begin: paddle.full_like(Tensor([0, 232],"int32"), 0.0, VarType(float32), None, )

[Pass] paddle.full_like(Tensor([0, 232],"int32"), 0.0, VarType(float32), None, )
2025-03-03 18:36:34.119217 test begin: paddle.full_like(Tensor([0, 2],"float32"), dtype=type(numpy.float32), fill_value=1.1, )

[Pass] paddle.full_like(Tensor([0, 2],"float32"), dtype=type(numpy.float32), fill_value=1.1, )
2025-03-03 18:36:34.119767 test begin: paddle.full_like(Tensor([0, 384, 3, 3],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0, 384, 3, 3],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.120261 test begin: paddle.full_like(Tensor([0, 384, 3, 3],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0, 384, 3, 3],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.120751 test begin: paddle.full_like(Tensor([0, 4],"float32"), 0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0, 4],"float32"), 0, dtype=VarType(float32), )
2025-03-03 18:36:34.121232 test begin: paddle.full_like(Tensor([0],"float32"), -2.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), -2.0, dtype=VarType(float32), )
2025-03-03 18:36:34.121710 test begin: paddle.full_like(Tensor([0],"float32"), -4.595, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), -4.595, dtype=Dtype(float32), )
2025-03-03 18:36:34.122162 test begin: paddle.full_like(Tensor([0],"float32"), -4.59511985013459, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), -4.59511985013459, dtype=Dtype(float32), )
2025-03-03 18:36:34.122678 test begin: paddle.full_like(Tensor([0],"float32"), -4.59511985013459, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), -4.59511985013459, dtype=VarType(float32), )
2025-03-03 18:36:34.123151 test begin: paddle.full_like(Tensor([0],"float32"), 0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), 0, dtype=Dtype(float32), )
2025-03-03 18:36:34.123599 test begin: paddle.full_like(Tensor([0],"float32"), 0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), 0, dtype=VarType(float32), )
2025-03-03 18:36:34.124055 test begin: paddle.full_like(Tensor([0],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.124494 test begin: paddle.full_like(Tensor([0],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.124942 test begin: paddle.full_like(Tensor([0],"float32"), 1.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), 1.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.125382 test begin: paddle.full_like(Tensor([0],"float32"), 1.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([0],"float32"), 1.0, dtype=VarType(float32), )
2025-03-03 18:36:34.125823 test begin: paddle.full_like(Tensor([0],"int64"), False, dtype="bool", )

[Pass] paddle.full_like(Tensor([0],"int64"), False, dtype="bool", )
2025-03-03 18:36:34.126297 test begin: paddle.full_like(Tensor([1, 0, 1, 23],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([1, 0, 1, 23],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )
2025-03-03 18:36:34.126775 test begin: paddle.full_like(Tensor([1, 0, 1, 24],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([1, 0, 1, 24],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )
2025-03-03 18:36:34.127231 test begin: paddle.full_like(Tensor([1, 0, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )

[Pass] paddle.full_like(Tensor([1, 0, 2048, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
2025-03-03 18:36:34.127777 test begin: paddle.full_like(Tensor([1, 0, 3, 3],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([1, 0, 3, 3],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.128269 test begin: paddle.full_like(Tensor([1, 0],"float32"), dtype=type(numpy.float32), fill_value=1.1, )

[Pass] paddle.full_like(Tensor([1, 0],"float32"), dtype=type(numpy.float32), fill_value=1.1, )
2025-03-03 18:36:34.128808 test begin: paddle.full_like(Tensor([1, 0],"int64"), True, dtype=VarType(bool), )

[Pass] paddle.full_like(Tensor([1, 0],"int64"), True, dtype=VarType(bool), )
2025-03-03 18:36:34.129308 test begin: paddle.full_like(Tensor([1, 1, 0, 2048],"bool"), -65504.0, dtype=Dtype(float16), )

[Pass] paddle.full_like(Tensor([1, 1, 0, 2048],"bool"), -65504.0, dtype=Dtype(float16), )
2025-03-03 18:36:34.129787 test begin: paddle.full_like(Tensor([1, 1, 0, 23],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([1, 1, 0, 23],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )
2025-03-03 18:36:34.130270 test begin: paddle.full_like(Tensor([1, 1, 0, 24],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([1, 1, 0, 24],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )
2025-03-03 18:36:34.130729 test begin: paddle.full_like(Tensor([1, 1, 1, 0],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([1, 1, 1, 0],"bool"), -3.4028234663852886e+38, dtype=Dtype(float32), )
2025-03-03 18:36:34.131180 test begin: paddle.full_like(Tensor([1, 1, 2048, 0],"bool"), -65504.0, dtype=Dtype(float16), )

[Pass] paddle.full_like(Tensor([1, 1, 2048, 0],"bool"), -65504.0, dtype=Dtype(float16), )
2025-03-03 18:36:34.131628 test begin: paddle.full_like(Tensor([1, 192, 0, 3],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([1, 192, 0, 3],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.132123 test begin: paddle.full_like(Tensor([1, 192, 3, 0],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([1, 192, 3, 0],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.132638 test begin: paddle.full_like(Tensor([1, 384, 0, 3],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([1, 384, 0, 3],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.133120 test begin: paddle.full_like(Tensor([1, 384, 3, 0],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([1, 384, 3, 0],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.133602 test begin: paddle.full_like(Tensor([10, 0],"int64"), True, dtype=VarType(bool), )

[Pass] paddle.full_like(Tensor([10, 0],"int64"), True, dtype=VarType(bool), )
2025-03-03 18:36:34.134100 test begin: paddle.full_like(Tensor([2, 0, 1, 100],"bool"), -65504.0, dtype=Dtype(float16), )

[Pass] paddle.full_like(Tensor([2, 0, 1, 100],"bool"), -65504.0, dtype=Dtype(float16), )
2025-03-03 18:36:34.134578 test begin: paddle.full_like(Tensor([2, 1, 0, 100],"bool"), -65504.0, dtype=Dtype(float16), )

[Pass] paddle.full_like(Tensor([2, 1, 0, 100],"bool"), -65504.0, dtype=Dtype(float16), )
2025-03-03 18:36:34.135194 test begin: paddle.full_like(Tensor([2, 1, 1, 0],"bool"), -65504.0, dtype=Dtype(float16), )

[Pass] paddle.full_like(Tensor([2, 1, 1, 0],"bool"), -65504.0, dtype=Dtype(float16), )
2025-03-03 18:36:34.135683 test begin: paddle.full_like(Tensor([219, 0],"float32"), 0.0, VarType(bfloat16), None, )

[Pass] paddle.full_like(Tensor([219, 0],"float32"), 0.0, VarType(bfloat16), None, )
2025-03-03 18:36:34.136165 test begin: paddle.full_like(Tensor([219, 0],"int32"), 0.0, Dtype(int16), None, )

[Pass] paddle.full_like(Tensor([219, 0],"int32"), 0.0, Dtype(int16), None, )
2025-03-03 18:36:34.136650 test begin: paddle.full_like(Tensor([219, 0],"int32"), 0.0, VarType(float32), None, )

[Pass] paddle.full_like(Tensor([219, 0],"int32"), 0.0, VarType(float32), None, )
2025-03-03 18:36:34.137119 test begin: paddle.full_like(Tensor([256, 0],"float32"), 0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([256, 0],"float32"), 0, dtype=VarType(float32), )
2025-03-03 18:36:34.137556 test begin: paddle.full_like(Tensor([256, 0],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([256, 0],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.138009 test begin: paddle.full_like(Tensor([256, 0],"float32"), 0.0, dtype=VarType(float32), )

[Pass] paddle.full_like(Tensor([256, 0],"float32"), 0.0, dtype=VarType(float32), )
2025-03-03 18:36:34.138446 test begin: paddle.full_like(Tensor([68, 0, 3, 3],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([68, 0, 3, 3],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.138907 test begin: paddle.full_like(Tensor([68, 192, 0, 3],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([68, 192, 0, 3],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.139409 test begin: paddle.full_like(Tensor([68, 192, 3, 0],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([68, 192, 3, 0],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.139906 test begin: paddle.full_like(Tensor([68, 384, 0, 3],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([68, 384, 0, 3],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.140406 test begin: paddle.full_like(Tensor([68, 384, 3, 0],"float32"), 0.0, dtype=Dtype(float32), )

[Pass] paddle.full_like(Tensor([68, 384, 3, 0],"float32"), 0.0, dtype=Dtype(float32), )
2025-03-03 18:36:34.140891 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=1, dtype="int32", )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=1, dtype="int32", )
2025-03-03 18:36:34.141409 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=1, dtype="int64", )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=1, dtype="int64", )
2025-03-03 18:36:34.141911 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=1, dtype=type(numpy.int32), )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=1, dtype=type(numpy.int32), )
2025-03-03 18:36:34.142415 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=1, dtype=type(numpy.int64), )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=1, dtype=type(numpy.int64), )
2025-03-03 18:36:34.142902 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=1.0, dtype="float16", )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=1.0, dtype="float16", )
2025-03-03 18:36:34.143382 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=1.0, dtype="float32", )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=1.0, dtype="float32", )
2025-03-03 18:36:34.143846 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=1.0, dtype=type(numpy.float16), )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=1.0, dtype=type(numpy.float16), )
2025-03-03 18:36:34.144332 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=1.0, dtype=type(numpy.float32), )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=1.0, dtype=type(numpy.float32), )
2025-03-03 18:36:34.144791 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=False, dtype="bool", )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=False, dtype="bool", )
2025-03-03 18:36:34.145272 test begin: paddle.full_like(x=Tensor([0],"float64"), fill_value=True, dtype="bool", )

[Pass] paddle.full_like(x=Tensor([0],"float64"), fill_value=True, dtype="bool", )
2025-03-03 18:36:34.145746 test begin: paddle.gammaln(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.gammaln(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.147205 test begin: paddle.gammaln(Tensor([0, 3, 4, 5],"float32"), )

[cuda error] paddle.gammaln(Tensor([0, 3, 4, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.148018 test begin: paddle.gammaln(Tensor([0, 3, 4, 5],"float64"), )

[cuda error] paddle.gammaln(Tensor([0, 3, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.148880 test begin: paddle.gammaln(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.gammaln(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.149642 test begin: paddle.gammaln(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.gammaln(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.150403 test begin: paddle.gammaln(Tensor([2, 0, 4, 5],"float32"), )

[cuda error] paddle.gammaln(Tensor([2, 0, 4, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.151104 test begin: paddle.gammaln(Tensor([2, 0, 4, 5],"float64"), )

[cuda error] paddle.gammaln(Tensor([2, 0, 4, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.151816 test begin: paddle.gammaln(Tensor([2, 3, 0, 5],"float32"), )

[cuda error] paddle.gammaln(Tensor([2, 3, 0, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.152560 test begin: paddle.gammaln(Tensor([2, 3, 0, 5],"float64"), )

[cuda error] paddle.gammaln(Tensor([2, 3, 0, 5],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.153328 test begin: paddle.gammaln(Tensor([2, 3, 4, 0],"float32"), )

[cuda error] paddle.gammaln(Tensor([2, 3, 4, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.154050 test begin: paddle.gammaln(Tensor([2, 3, 4, 0],"float64"), )

[cuda error] paddle.gammaln(Tensor([2, 3, 4, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:36:34.155038 test begin: paddle.greater_equal(Tensor([0, 1, 1, 1, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([0, 1, 1, 1, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.155954 test begin: paddle.greater_equal(Tensor([0, 1, 1, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([0, 1, 1, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.156575 test begin: paddle.greater_equal(Tensor([0, 1],"int64"), Tensor([0, 1],"int64"), )

[Pass] paddle.greater_equal(Tensor([0, 1],"int64"), Tensor([0, 1],"int64"), )
2025-03-03 18:36:34.157230 test begin: paddle.greater_equal(Tensor([0, 1],"int64"), Tensor([1, 1],"int64"), )

[Pass] paddle.greater_equal(Tensor([0, 1],"int64"), Tensor([1, 1],"int64"), )
2025-03-03 18:36:34.157815 test begin: paddle.greater_equal(Tensor([0, 2, 16, 4, 1],"int64"), Tensor([0, 2, 16, 1, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([0, 2, 16, 4, 1],"int64"), Tensor([0, 2, 16, 1, 8],"int64"), )
2025-03-03 18:36:34.158345 test begin: paddle.greater_equal(Tensor([0, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([0, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2, 16, 4, 1] and the shape of Y = [13, 2, 16, 1, 8]. Received [0] in X is not equal to [13] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.159023 test begin: paddle.greater_equal(Tensor([0, 2, 8, 4, 1],"int64"), Tensor([0, 2, 8, 1, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([0, 2, 8, 4, 1],"int64"), Tensor([0, 2, 8, 1, 8],"int64"), )
2025-03-03 18:36:34.159578 test begin: paddle.greater_equal(Tensor([0, 2, 8, 4, 1],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([0, 2, 8, 4, 1],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2, 8, 4, 1] and the shape of Y = [13, 2, 8, 1, 8]. Received [0] in X is not equal to [13] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.160207 test begin: paddle.greater_equal(Tensor([0, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )

[Pass] paddle.greater_equal(Tensor([0, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )
2025-03-03 18:36:34.160717 test begin: paddle.greater_equal(Tensor([0, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), )

[paddle error] paddle.greater_equal(Tensor([0, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 20, 1] and the shape of Y = [10, 20, 1]. Received [0] in X is not equal to [10] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.161289 test begin: paddle.greater_equal(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )

W0303 18:36:34.161720 93001 dygraph_functions.cc:89309] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.greater_equal(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )
2025-03-03 18:36:34.161914 test begin: paddle.greater_equal(Tensor([0, 3, 2],"float16"), Tensor([4, 3, 2],"float32"), )

[paddle error] paddle.greater_equal(Tensor([0, 3, 2],"float16"), Tensor([4, 3, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 2] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.162514 test begin: paddle.greater_equal(Tensor([0, 3, 4, 5],"float32"), Tensor([0, 3, 4, 5],"float32"), )

[Pass] paddle.greater_equal(Tensor([0, 3, 4, 5],"float32"), Tensor([0, 3, 4, 5],"float32"), )
2025-03-03 18:36:34.163075 test begin: paddle.greater_equal(Tensor([0, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), )

[paddle error] paddle.greater_equal(Tensor([0, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 4, 5] and the shape of Y = [2, 3, 4, 5]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.163658 test begin: paddle.greater_equal(Tensor([0, 3, 4, 5],"float64"), Tensor([0, 3, 4, 5],"float64"), )

[Pass] paddle.greater_equal(Tensor([0, 3, 4, 5],"float64"), Tensor([0, 3, 4, 5],"float64"), )
2025-03-03 18:36:34.164200 test begin: paddle.greater_equal(Tensor([0, 3, 4, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), )

[paddle error] paddle.greater_equal(Tensor([0, 3, 4, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 4, 5] and the shape of Y = [2, 3, 4, 5]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.164759 test begin: paddle.greater_equal(Tensor([0, 8, 1, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([0, 8, 1, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.165343 test begin: paddle.greater_equal(Tensor([0],"float16"), Tensor([0],"float16"), )

[Pass] paddle.greater_equal(Tensor([0],"float16"), Tensor([0],"float16"), )
2025-03-03 18:36:34.165934 test begin: paddle.greater_equal(Tensor([0],"float16"), Tensor([4],"float16"), )

[paddle error] paddle.greater_equal(Tensor([0],"float16"), Tensor([4],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.168189 test begin: paddle.greater_equal(Tensor([0],"int64"), Tensor([0],"int64"), )

[Pass] paddle.greater_equal(Tensor([0],"int64"), Tensor([0],"int64"), )
2025-03-03 18:36:34.168923 test begin: paddle.greater_equal(Tensor([0],"int64"), Tensor([4],"int64"), )

[paddle error] paddle.greater_equal(Tensor([0],"int64"), Tensor([4],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.169712 test begin: paddle.greater_equal(Tensor([1, 0],"int64"), Tensor([1, 0],"int64"), )

[Pass] paddle.greater_equal(Tensor([1, 0],"int64"), Tensor([1, 0],"int64"), )
2025-03-03 18:36:34.170468 test begin: paddle.greater_equal(Tensor([1, 0],"int64"), Tensor([1, 1],"int64"), )

[Pass] paddle.greater_equal(Tensor([1, 0],"int64"), Tensor([1, 1],"int64"), )
2025-03-03 18:36:34.171263 test begin: paddle.greater_equal(Tensor([1, 1],"int64"), Tensor([0, 1],"int64"), )

[Pass] paddle.greater_equal(Tensor([1, 1],"int64"), Tensor([0, 1],"int64"), )
2025-03-03 18:36:34.172035 test begin: paddle.greater_equal(Tensor([1, 1],"int64"), Tensor([1, 0],"int64"), )

[Pass] paddle.greater_equal(Tensor([1, 1],"int64"), Tensor([1, 0],"int64"), )
2025-03-03 18:36:34.172767 test begin: paddle.greater_equal(Tensor([10, 0, 1],"float32"), Tensor([10, 0, 1],"float32"), )

[Pass] paddle.greater_equal(Tensor([10, 0, 1],"float32"), Tensor([10, 0, 1],"float32"), )
2025-03-03 18:36:34.173262 test begin: paddle.greater_equal(Tensor([10, 0, 1],"float32"), Tensor([10, 20, 1],"float32"), )

[paddle error] paddle.greater_equal(Tensor([10, 0, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 0, 1] and the shape of Y = [10, 20, 1]. Received [0] in X is not equal to [20] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.173842 test begin: paddle.greater_equal(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 0],"float32"), )

[Pass] paddle.greater_equal(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 0],"float32"), )
2025-03-03 18:36:34.174390 test begin: paddle.greater_equal(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 1],"float32"), )

[Pass] paddle.greater_equal(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 1],"float32"), )
2025-03-03 18:36:34.174975 test begin: paddle.greater_equal(Tensor([10, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )

[paddle error] paddle.greater_equal(Tensor([10, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 20, 1] and the shape of Y = [0, 20, 1]. Received [10] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.175623 test begin: paddle.greater_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 0, 1],"float32"), )

[paddle error] paddle.greater_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 0, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 20, 1] and the shape of Y = [10, 0, 1]. Received [20] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.176439 test begin: paddle.greater_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 0],"float32"), )

[Pass] paddle.greater_equal(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 0],"float32"), )
2025-03-03 18:36:34.177107 test begin: paddle.greater_equal(Tensor([13, 0, 1, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([13, 0, 1, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.177766 test begin: paddle.greater_equal(Tensor([13, 0, 16, 4, 1],"int64"), Tensor([13, 0, 16, 1, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 0, 16, 4, 1],"int64"), Tensor([13, 0, 16, 1, 8],"int64"), )
2025-03-03 18:36:34.178343 test begin: paddle.greater_equal(Tensor([13, 0, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 0, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 0, 16, 4, 1] and the shape of Y = [13, 2, 16, 1, 8]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.178964 test begin: paddle.greater_equal(Tensor([13, 0, 8, 4, 1],"int64"), Tensor([13, 0, 8, 1, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 0, 8, 4, 1],"int64"), Tensor([13, 0, 8, 1, 8],"int64"), )
2025-03-03 18:36:34.179570 test begin: paddle.greater_equal(Tensor([13, 0, 8, 4, 1],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 0, 8, 4, 1],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 0, 8, 4, 1] and the shape of Y = [13, 2, 8, 1, 8]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.180216 test begin: paddle.greater_equal(Tensor([13, 2, 0, 4, 1],"int64"), Tensor([13, 2, 0, 1, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 0, 4, 1],"int64"), Tensor([13, 2, 0, 1, 8],"int64"), )
2025-03-03 18:36:34.180797 test begin: paddle.greater_equal(Tensor([13, 2, 0, 4, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 0, 4, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 0, 4, 1] and the shape of Y = [13, 2, 16, 1, 8]. Received [0] in X is not equal to [16] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.181405 test begin: paddle.greater_equal(Tensor([13, 2, 0, 4, 1],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 0, 4, 1],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 0, 4, 1] and the shape of Y = [13, 2, 8, 1, 8]. Received [0] in X is not equal to [8] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.182008 test begin: paddle.greater_equal(Tensor([13, 2, 16, 0, 1],"int64"), Tensor([13, 2, 16, 0, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 16, 0, 1],"int64"), Tensor([13, 2, 16, 0, 8],"int64"), )
2025-03-03 18:36:34.182546 test begin: paddle.greater_equal(Tensor([13, 2, 16, 0, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 16, 0, 1],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )
2025-03-03 18:36:34.183170 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 0],"int64"), Tensor([13, 2, 16, 1, 0],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 16, 4, 0],"int64"), Tensor([13, 2, 16, 1, 0],"int64"), )
2025-03-03 18:36:34.183706 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 0],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 16, 4, 0],"int64"), Tensor([13, 2, 16, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 16, 4, 0] and the shape of Y = [13, 2, 16, 1, 8]. Received [0] in X is not equal to [8] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.184364 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([0, 2, 16, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([0, 2, 16, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 16, 4, 1] and the shape of Y = [0, 2, 16, 1, 8]. Received [13] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.185766 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 0, 16, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 0, 16, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 16, 4, 1] and the shape of Y = [13, 0, 16, 1, 8]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.186666 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 0, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 0, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 16, 4, 1] and the shape of Y = [13, 2, 0, 1, 8]. Received [16] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.187349 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 0, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 0, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 16, 4, 1] and the shape of Y = [13, 2, 16, 0, 8]. Received [4] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.188051 test begin: paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 0],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 16, 4, 1],"int64"), Tensor([13, 2, 16, 1, 0],"int64"), )
2025-03-03 18:36:34.188778 test begin: paddle.greater_equal(Tensor([13, 2, 8, 0, 1],"int64"), Tensor([13, 2, 8, 0, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 8, 0, 1],"int64"), Tensor([13, 2, 8, 0, 8],"int64"), )
2025-03-03 18:36:34.189350 test begin: paddle.greater_equal(Tensor([13, 2, 8, 0, 1],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 8, 0, 1],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), )
2025-03-03 18:36:34.189974 test begin: paddle.greater_equal(Tensor([13, 2, 8, 4, 0],"int64"), Tensor([13, 2, 8, 1, 0],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 8, 4, 0],"int64"), Tensor([13, 2, 8, 1, 0],"int64"), )
2025-03-03 18:36:34.190511 test begin: paddle.greater_equal(Tensor([13, 2, 8, 4, 0],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 8, 4, 0],"int64"), Tensor([13, 2, 8, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 8, 4, 0] and the shape of Y = [13, 2, 8, 1, 8]. Received [0] in X is not equal to [8] in Y at i:4.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.191162 test begin: paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([0, 2, 8, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([0, 2, 8, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 8, 4, 1] and the shape of Y = [0, 2, 8, 1, 8]. Received [13] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.191778 test begin: paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([13, 0, 8, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([13, 0, 8, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 8, 4, 1] and the shape of Y = [13, 0, 8, 1, 8]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.192419 test begin: paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([13, 2, 0, 1, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([13, 2, 0, 1, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 8, 4, 1] and the shape of Y = [13, 2, 0, 1, 8]. Received [8] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.193027 test begin: paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([13, 2, 8, 0, 8],"int64"), )

[paddle error] paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([13, 2, 8, 0, 8],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [13, 2, 8, 4, 1] and the shape of Y = [13, 2, 8, 0, 8]. Received [4] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.193620 test begin: paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([13, 2, 8, 1, 0],"int64"), )

[Pass] paddle.greater_equal(Tensor([13, 2, 8, 4, 1],"int64"), Tensor([13, 2, 8, 1, 0],"int64"), )
2025-03-03 18:36:34.194226 test begin: paddle.greater_equal(Tensor([13, 8, 0, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([13, 8, 0, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.195300 test begin: paddle.greater_equal(Tensor([13, 8, 1, 0],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([13, 8, 1, 0],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.196055 test begin: paddle.greater_equal(Tensor([13, 8, 1, 1],"float32"), Tensor([0],"float32"), )

[Pass] paddle.greater_equal(Tensor([13, 8, 1, 1],"float32"), Tensor([0],"float32"), )
2025-03-03 18:36:34.196694 test begin: paddle.greater_equal(Tensor([2, 0, 1, 1, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 0, 1, 1, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.197306 test begin: paddle.greater_equal(Tensor([2, 0, 1, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 0, 1, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.197898 test begin: paddle.greater_equal(Tensor([2, 0, 4, 5],"float32"), Tensor([2, 0, 4, 5],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 0, 4, 5],"float32"), Tensor([2, 0, 4, 5],"float32"), )
2025-03-03 18:36:34.198424 test begin: paddle.greater_equal(Tensor([2, 0, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), )

[paddle error] paddle.greater_equal(Tensor([2, 0, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0, 4, 5] and the shape of Y = [2, 3, 4, 5]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.199027 test begin: paddle.greater_equal(Tensor([2, 0, 4, 5],"float64"), Tensor([2, 0, 4, 5],"float64"), )

[Pass] paddle.greater_equal(Tensor([2, 0, 4, 5],"float64"), Tensor([2, 0, 4, 5],"float64"), )
2025-03-03 18:36:34.199560 test begin: paddle.greater_equal(Tensor([2, 0, 4, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), )

[paddle error] paddle.greater_equal(Tensor([2, 0, 4, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 0, 4, 5] and the shape of Y = [2, 3, 4, 5]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.200122 test begin: paddle.greater_equal(Tensor([2, 0],"int64"), Tensor([1, 0],"int64"), )

[Pass] paddle.greater_equal(Tensor([2, 0],"int64"), Tensor([1, 0],"int64"), )
2025-03-03 18:36:34.200709 test begin: paddle.greater_equal(Tensor([2, 0],"int64"), Tensor([1, 1],"int64"), )

[Pass] paddle.greater_equal(Tensor([2, 0],"int64"), Tensor([1, 1],"int64"), )
2025-03-03 18:36:34.201279 test begin: paddle.greater_equal(Tensor([2, 1, 0, 1, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 1, 0, 1, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.201849 test begin: paddle.greater_equal(Tensor([2, 1, 0, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 1, 0, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.202515 test begin: paddle.greater_equal(Tensor([2, 1, 1, 0, 1],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 1, 1, 0, 1],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.203158 test begin: paddle.greater_equal(Tensor([2, 1, 1, 0],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 1, 1, 0],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.203747 test begin: paddle.greater_equal(Tensor([2, 1, 1, 1, 0],"float32"), Tensor([1],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 1, 1, 1, 0],"float32"), Tensor([1],"float32"), )
2025-03-03 18:36:34.204331 test begin: paddle.greater_equal(Tensor([2, 1, 1, 1, 1],"float32"), Tensor([0],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 1, 1, 1, 1],"float32"), Tensor([0],"float32"), )
2025-03-03 18:36:34.204923 test begin: paddle.greater_equal(Tensor([2, 1, 1, 1],"float32"), Tensor([0],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 1, 1, 1],"float32"), Tensor([0],"float32"), )
2025-03-03 18:36:34.205588 test begin: paddle.greater_equal(Tensor([2, 1],"int64"), Tensor([0, 1],"int64"), )

[paddle error] paddle.greater_equal(Tensor([2, 1],"int64"), Tensor([0, 1],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 1] and the shape of Y = [0, 1]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.206210 test begin: paddle.greater_equal(Tensor([2, 1],"int64"), Tensor([1, 0],"int64"), )

[Pass] paddle.greater_equal(Tensor([2, 1],"int64"), Tensor([1, 0],"int64"), )
2025-03-03 18:36:34.206837 test begin: paddle.greater_equal(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), )
2025-03-03 18:36:34.207349 test begin: paddle.greater_equal(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 0, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 0, 5] and the shape of Y = [2, 3, 4, 5]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.207932 test begin: paddle.greater_equal(Tensor([2, 3, 0, 5],"float64"), Tensor([2, 3, 0, 5],"float64"), )

[Pass] paddle.greater_equal(Tensor([2, 3, 0, 5],"float64"), Tensor([2, 3, 0, 5],"float64"), )
2025-03-03 18:36:34.208483 test begin: paddle.greater_equal(Tensor([2, 3, 0, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 0, 5],"float64"), Tensor([2, 3, 4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 0, 5] and the shape of Y = [2, 3, 4, 5]. Received [0] in X is not equal to [4] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.209051 test begin: paddle.greater_equal(Tensor([2, 3, 4, 0],"float32"), Tensor([2, 3, 4, 0],"float32"), )

[Pass] paddle.greater_equal(Tensor([2, 3, 4, 0],"float32"), Tensor([2, 3, 4, 0],"float32"), )
2025-03-03 18:36:34.209595 test begin: paddle.greater_equal(Tensor([2, 3, 4, 0],"float32"), Tensor([2, 3, 4, 5],"float32"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 0],"float32"), Tensor([2, 3, 4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 0] and the shape of Y = [2, 3, 4, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.210159 test begin: paddle.greater_equal(Tensor([2, 3, 4, 0],"float64"), Tensor([2, 3, 4, 0],"float64"), )

[Pass] paddle.greater_equal(Tensor([2, 3, 4, 0],"float64"), Tensor([2, 3, 4, 0],"float64"), )
2025-03-03 18:36:34.210706 test begin: paddle.greater_equal(Tensor([2, 3, 4, 0],"float64"), Tensor([2, 3, 4, 5],"float64"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 0],"float64"), Tensor([2, 3, 4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 0] and the shape of Y = [2, 3, 4, 5]. Received [0] in X is not equal to [5] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.211291 test begin: paddle.greater_equal(Tensor([2, 3, 4, 5],"float32"), Tensor([0, 3, 4, 5],"float32"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 5],"float32"), Tensor([0, 3, 4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [0, 3, 4, 5]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.211962 test begin: paddle.greater_equal(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 0, 4, 5],"float32"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 0, 4, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [2, 0, 4, 5]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.212525 test begin: paddle.greater_equal(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 0, 5],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [2, 3, 0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.213219 test begin: paddle.greater_equal(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 0],"float32"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [2, 3, 4, 0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.213831 test begin: paddle.greater_equal(Tensor([2, 3, 4, 5],"float64"), Tensor([0, 3, 4, 5],"float64"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 5],"float64"), Tensor([0, 3, 4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [0, 3, 4, 5]. Received [2] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.214407 test begin: paddle.greater_equal(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 0, 4, 5],"float64"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 0, 4, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [2, 0, 4, 5]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.215002 test begin: paddle.greater_equal(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 3, 0, 5],"float64"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 3, 0, 5],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [2, 3, 0, 5]. Received [4] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.215620 test begin: paddle.greater_equal(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 3, 4, 0],"float64"), )

[paddle error] paddle.greater_equal(Tensor([2, 3, 4, 5],"float64"), Tensor([2, 3, 4, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [2, 3, 4, 5] and the shape of Y = [2, 3, 4, 0]. Received [5] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.216192 test begin: paddle.greater_equal(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )

[Pass] paddle.greater_equal(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )
2025-03-03 18:36:34.216816 test begin: paddle.greater_equal(Tensor([4, 0, 2],"float16"), Tensor([4, 3, 2],"float32"), )

[paddle error] paddle.greater_equal(Tensor([4, 0, 2],"float16"), Tensor([4, 3, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 0, 2] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.217415 test begin: paddle.greater_equal(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )

[Pass] paddle.greater_equal(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )
2025-03-03 18:36:34.217981 test begin: paddle.greater_equal(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 2],"float32"), )

[paddle error] paddle.greater_equal(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 0] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.218564 test begin: paddle.greater_equal(Tensor([4, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )

[paddle error] paddle.greater_equal(Tensor([4, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [0, 3, 2]. Received [4] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.219768 test begin: paddle.greater_equal(Tensor([4, 3, 2],"float16"), Tensor([4, 0, 2],"float32"), )

[paddle error] paddle.greater_equal(Tensor([4, 3, 2],"float16"), Tensor([4, 0, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [4, 0, 2]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.220674 test begin: paddle.greater_equal(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 0],"float32"), )

[paddle error] paddle.greater_equal(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [4, 3, 0]. Received [2] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.221506 test begin: paddle.greater_equal(Tensor([4],"float16"), Tensor([0],"float16"), )

[paddle error] paddle.greater_equal(Tensor([4],"float16"), Tensor([0],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4] and the shape of Y = [0]. Received [4] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.222381 test begin: paddle.greater_equal(Tensor([4],"int64"), Tensor([0],"int64"), )

[paddle error] paddle.greater_equal(Tensor([4],"int64"), Tensor([0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4] and the shape of Y = [0]. Received [4] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.223165 test begin: paddle.greater_equal(x=Tensor([0, 2, 1, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )

[Pass] paddle.greater_equal(x=Tensor([0, 2, 1, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )
2025-03-03 18:36:34.224016 test begin: paddle.greater_equal(x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), )

[Pass] paddle.greater_equal(x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), )
2025-03-03 18:36:34.224709 test begin: paddle.greater_equal(x=Tensor([0, 2],"float64"), y=Tensor([2, 2],"float64"), )

[paddle error] paddle.greater_equal(x=Tensor([0, 2],"float64"), y=Tensor([2, 2],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2] and the shape of Y = [2, 2]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.225431 test begin: paddle.greater_equal(x=Tensor([0, 3],"float32"), y=Tensor([0, 3],"float32"), )

[Pass] paddle.greater_equal(x=Tensor([0, 3],"float32"), y=Tensor([0, 3],"float32"), )
2025-03-03 18:36:34.226105 test begin: paddle.greater_equal(x=Tensor([0, 3],"float32"), y=Tensor([3, 3],"float32"), )

[paddle error] paddle.greater_equal(x=Tensor([0, 3],"float32"), y=Tensor([3, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [3, 3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.226808 test begin: paddle.greater_equal(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )

[Pass] paddle.greater_equal(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )
2025-03-03 18:36:34.227474 test begin: paddle.greater_equal(x=Tensor([0],"float64"), y=Tensor([4],"float64"), )

[paddle error] paddle.greater_equal(x=Tensor([0],"float64"), y=Tensor([4],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.228207 test begin: paddle.greater_equal(x=Tensor([0],"int64"), y=Tensor([0],"int64"), )

[Pass] paddle.greater_equal(x=Tensor([0],"int64"), y=Tensor([0],"int64"), )
2025-03-03 18:36:34.228950 test begin: paddle.greater_equal(x=Tensor([0],"int64"), y=Tensor([3],"int64"), )

[paddle error] paddle.greater_equal(x=Tensor([0],"int64"), y=Tensor([3],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.229720 test begin: paddle.greater_equal(x=Tensor([1, 0, 1, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )

[Pass] paddle.greater_equal(x=Tensor([1, 0, 1, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )
2025-03-03 18:36:34.230488 test begin: paddle.greater_equal(x=Tensor([1, 0],"float64"), y=Tensor([2, 0],"float64"), )

[Pass] paddle.greater_equal(x=Tensor([1, 0],"float64"), y=Tensor([2, 0],"float64"), )
2025-03-03 18:36:34.231158 test begin: paddle.greater_equal(x=Tensor([1, 0],"float64"), y=Tensor([2, 2],"float64"), )

[paddle error] paddle.greater_equal(x=Tensor([1, 0],"float64"), y=Tensor([2, 2],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0] and the shape of Y = [2, 2]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.231968 test begin: paddle.greater_equal(x=Tensor([1, 2, 0, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )

[paddle error] paddle.greater_equal(x=Tensor([1, 2, 0, 3],"float64"), y=Tensor([1, 2, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2, 0, 3] and the shape of Y = [1, 2, 3]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.232737 test begin: paddle.greater_equal(x=Tensor([1, 2, 1, 0],"float64"), y=Tensor([1, 2, 3],"float64"), )

[paddle error] paddle.greater_equal(x=Tensor([1, 2, 1, 0],"float64"), y=Tensor([1, 2, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2, 1, 0] and the shape of Y = [1, 2, 3]. Received [0] in X is not equal to [3] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.233502 test begin: paddle.greater_equal(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([0, 2, 3],"float64"), )

[paddle error] paddle.greater_equal(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([0, 2, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2, 1, 3] and the shape of Y = [0, 2, 3]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.234269 test begin: paddle.greater_equal(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([1, 0, 3],"float64"), )

[Pass] paddle.greater_equal(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([1, 0, 3],"float64"), )
2025-03-03 18:36:34.235098 test begin: paddle.greater_equal(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([1, 2, 0],"float64"), )

[paddle error] paddle.greater_equal(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([1, 2, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2, 1, 3] and the shape of Y = [1, 2, 0]. Received [3] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.235882 test begin: paddle.greater_equal(x=Tensor([1, 2],"float64"), y=Tensor([0, 2],"float64"), )

[Pass] paddle.greater_equal(x=Tensor([1, 2],"float64"), y=Tensor([0, 2],"float64"), )
2025-03-03 18:36:34.236691 test begin: paddle.greater_equal(x=Tensor([1, 2],"float64"), y=Tensor([2, 0],"float64"), )

[paddle error] paddle.greater_equal(x=Tensor([1, 2],"float64"), y=Tensor([2, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2] and the shape of Y = [2, 0]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.237632 test begin: paddle.greater_equal(x=Tensor([1],"float64"), y=Tensor([0],"float64"), )

[Pass] paddle.greater_equal(x=Tensor([1],"float64"), y=Tensor([0],"float64"), )
2025-03-03 18:36:34.238330 test begin: paddle.greater_equal(x=Tensor([3, 0],"float32"), y=Tensor([3, 0],"float32"), )

[Pass] paddle.greater_equal(x=Tensor([3, 0],"float32"), y=Tensor([3, 0],"float32"), )
2025-03-03 18:36:34.238875 test begin: paddle.greater_equal(x=Tensor([3, 0],"float32"), y=Tensor([3, 3],"float32"), )

[paddle error] paddle.greater_equal(x=Tensor([3, 0],"float32"), y=Tensor([3, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0] and the shape of Y = [3, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.239466 test begin: paddle.greater_equal(x=Tensor([3, 3],"float32"), y=Tensor([0, 3],"float32"), )

[paddle error] paddle.greater_equal(x=Tensor([3, 3],"float32"), y=Tensor([0, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3] and the shape of Y = [0, 3]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.240118 test begin: paddle.greater_equal(x=Tensor([3, 3],"float32"), y=Tensor([3, 0],"float32"), )

[paddle error] paddle.greater_equal(x=Tensor([3, 3],"float32"), y=Tensor([3, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3] and the shape of Y = [3, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.240732 test begin: paddle.greater_equal(x=Tensor([3],"int64"), y=Tensor([0],"int64"), )

[paddle error] paddle.greater_equal(x=Tensor([3],"int64"), y=Tensor([0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3] and the shape of Y = [0]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.241383 test begin: paddle.greater_than(Tensor([0, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )

[Pass] paddle.greater_than(Tensor([0, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )
2025-03-03 18:36:34.241962 test begin: paddle.greater_than(Tensor([0, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), )

[paddle error] paddle.greater_than(Tensor([0, 20, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 20, 1] and the shape of Y = [10, 20, 1]. Received [0] in X is not equal to [10] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.242535 test begin: paddle.greater_than(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )

W0303 18:36:34.243041 93121 dygraph_functions.cc:89536] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.greater_than(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )
2025-03-03 18:36:34.243253 test begin: paddle.greater_than(Tensor([0, 3, 2],"float16"), Tensor([4, 3, 2],"float32"), )

[paddle error] paddle.greater_than(Tensor([0, 3, 2],"float16"), Tensor([4, 3, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3, 2] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.243878 test begin: paddle.greater_than(Tensor([0],"float16"), Tensor([0],"float16"), )

[Pass] paddle.greater_than(Tensor([0],"float16"), Tensor([0],"float16"), )
2025-03-03 18:36:34.244684 test begin: paddle.greater_than(Tensor([0],"float16"), Tensor([4],"float16"), )

[paddle error] paddle.greater_than(Tensor([0],"float16"), Tensor([4],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.245483 test begin: paddle.greater_than(Tensor([0],"int64"), Tensor([0],"int64"), )

[Pass] paddle.greater_than(Tensor([0],"int64"), Tensor([0],"int64"), )
2025-03-03 18:36:34.246284 test begin: paddle.greater_than(Tensor([0],"int64"), Tensor([4],"int64"), )

[paddle error] paddle.greater_than(Tensor([0],"int64"), Tensor([4],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.247015 test begin: paddle.greater_than(Tensor([10, 0, 1],"float32"), Tensor([10, 0, 1],"float32"), )

[Pass] paddle.greater_than(Tensor([10, 0, 1],"float32"), Tensor([10, 0, 1],"float32"), )
2025-03-03 18:36:34.247686 test begin: paddle.greater_than(Tensor([10, 0, 1],"float32"), Tensor([10, 20, 1],"float32"), )

[paddle error] paddle.greater_than(Tensor([10, 0, 1],"float32"), Tensor([10, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 0, 1] and the shape of Y = [10, 20, 1]. Received [0] in X is not equal to [20] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.248408 test begin: paddle.greater_than(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 0],"float32"), )

[Pass] paddle.greater_than(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 0],"float32"), )
2025-03-03 18:36:34.249091 test begin: paddle.greater_than(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 1],"float32"), )

[Pass] paddle.greater_than(Tensor([10, 20, 0],"float32"), Tensor([10, 20, 1],"float32"), )
2025-03-03 18:36:34.249953 test begin: paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), )

[paddle error] paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([0, 20, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 20, 1] and the shape of Y = [0, 20, 1]. Received [10] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.250756 test begin: paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 0, 1],"float32"), )

[paddle error] paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 0, 1],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [10, 20, 1] and the shape of Y = [10, 0, 1]. Received [20] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.251577 test begin: paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 0],"float32"), )

[Pass] paddle.greater_than(Tensor([10, 20, 1],"float32"), Tensor([10, 20, 0],"float32"), )
2025-03-03 18:36:34.252417 test begin: paddle.greater_than(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )

[Pass] paddle.greater_than(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )
2025-03-03 18:36:34.253086 test begin: paddle.greater_than(Tensor([4, 0, 2],"float16"), Tensor([4, 3, 2],"float32"), )

[paddle error] paddle.greater_than(Tensor([4, 0, 2],"float16"), Tensor([4, 3, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 0, 2] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.253848 test begin: paddle.greater_than(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )

[Pass] paddle.greater_than(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )
2025-03-03 18:36:34.254531 test begin: paddle.greater_than(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 2],"float32"), )

[paddle error] paddle.greater_than(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 0] and the shape of Y = [4, 3, 2]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.255305 test begin: paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )

[paddle error] paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [0, 3, 2]. Received [4] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.256145 test begin: paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([4, 0, 2],"float32"), )

[paddle error] paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([4, 0, 2],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [4, 0, 2]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.256990 test begin: paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 0],"float32"), )

[paddle error] paddle.greater_than(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4, 3, 2] and the shape of Y = [4, 3, 0]. Received [2] in X is not equal to [0] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.257861 test begin: paddle.greater_than(Tensor([4],"float16"), Tensor([0],"float16"), )

[paddle error] paddle.greater_than(Tensor([4],"float16"), Tensor([0],"float16"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4] and the shape of Y = [0]. Received [4] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.258620 test begin: paddle.greater_than(Tensor([4],"int64"), Tensor([0],"int64"), )

[paddle error] paddle.greater_than(Tensor([4],"int64"), Tensor([0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [4] and the shape of Y = [0]. Received [4] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.259376 test begin: paddle.greater_than(x=Tensor([0, 2, 1, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )

[Pass] paddle.greater_than(x=Tensor([0, 2, 1, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )
2025-03-03 18:36:34.260172 test begin: paddle.greater_than(x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), )

[Pass] paddle.greater_than(x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), )
2025-03-03 18:36:34.260855 test begin: paddle.greater_than(x=Tensor([0, 2],"float64"), y=Tensor([2, 2],"float64"), )

[paddle error] paddle.greater_than(x=Tensor([0, 2],"float64"), y=Tensor([2, 2],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 2] and the shape of Y = [2, 2]. Received [0] in X is not equal to [2] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.261559 test begin: paddle.greater_than(x=Tensor([0, 3],"float32"), y=Tensor([0, 3],"float32"), )

[Pass] paddle.greater_than(x=Tensor([0, 3],"float32"), y=Tensor([0, 3],"float32"), )
2025-03-03 18:36:34.262266 test begin: paddle.greater_than(x=Tensor([0, 3],"float32"), y=Tensor([3, 3],"float32"), )

[paddle error] paddle.greater_than(x=Tensor([0, 3],"float32"), y=Tensor([3, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0, 3] and the shape of Y = [3, 3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.263041 test begin: paddle.greater_than(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )

[Pass] paddle.greater_than(x=Tensor([0],"float64"), y=Tensor([0],"float64"), )
2025-03-03 18:36:34.263834 test begin: paddle.greater_than(x=Tensor([0],"float64"), y=Tensor([4],"float64"), )

[paddle error] paddle.greater_than(x=Tensor([0],"float64"), y=Tensor([4],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [4]. Received [0] in X is not equal to [4] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.264875 test begin: paddle.greater_than(x=Tensor([0],"int64"), y=Tensor([0],"int64"), )

[Pass] paddle.greater_than(x=Tensor([0],"int64"), y=Tensor([0],"int64"), )
2025-03-03 18:36:34.265628 test begin: paddle.greater_than(x=Tensor([0],"int64"), y=Tensor([3],"int64"), )

[paddle error] paddle.greater_than(x=Tensor([0],"int64"), y=Tensor([3],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [0] and the shape of Y = [3]. Received [0] in X is not equal to [3] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.266295 test begin: paddle.greater_than(x=Tensor([1, 0, 1, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )

[Pass] paddle.greater_than(x=Tensor([1, 0, 1, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )
2025-03-03 18:36:34.266950 test begin: paddle.greater_than(x=Tensor([1, 0],"float64"), y=Tensor([2, 0],"float64"), )

[Pass] paddle.greater_than(x=Tensor([1, 0],"float64"), y=Tensor([2, 0],"float64"), )
2025-03-03 18:36:34.267510 test begin: paddle.greater_than(x=Tensor([1, 0],"float64"), y=Tensor([2, 2],"float64"), )

[paddle error] paddle.greater_than(x=Tensor([1, 0],"float64"), y=Tensor([2, 2],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 0] and the shape of Y = [2, 2]. Received [0] in X is not equal to [2] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.268161 test begin: paddle.greater_than(x=Tensor([1, 2, 0, 3],"float64"), y=Tensor([1, 2, 3],"float64"), )

[paddle error] paddle.greater_than(x=Tensor([1, 2, 0, 3],"float64"), y=Tensor([1, 2, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2, 0, 3] and the shape of Y = [1, 2, 3]. Received [0] in X is not equal to [2] in Y at i:2.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.268911 test begin: paddle.greater_than(x=Tensor([1, 2, 1, 0],"float64"), y=Tensor([1, 2, 3],"float64"), )

[paddle error] paddle.greater_than(x=Tensor([1, 2, 1, 0],"float64"), y=Tensor([1, 2, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2, 1, 0] and the shape of Y = [1, 2, 3]. Received [0] in X is not equal to [3] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.269547 test begin: paddle.greater_than(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([0, 2, 3],"float64"), )

[paddle error] paddle.greater_than(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([0, 2, 3],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2, 1, 3] and the shape of Y = [0, 2, 3]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.270168 test begin: paddle.greater_than(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([1, 0, 3],"float64"), )

[Pass] paddle.greater_than(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([1, 0, 3],"float64"), )
2025-03-03 18:36:34.270790 test begin: paddle.greater_than(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([1, 2, 0],"float64"), )

[paddle error] paddle.greater_than(x=Tensor([1, 2, 1, 3],"float64"), y=Tensor([1, 2, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2, 1, 3] and the shape of Y = [1, 2, 0]. Received [3] in X is not equal to [0] in Y at i:3.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.271399 test begin: paddle.greater_than(x=Tensor([1, 2],"float64"), y=Tensor([0, 2],"float64"), )

[Pass] paddle.greater_than(x=Tensor([1, 2],"float64"), y=Tensor([0, 2],"float64"), )
2025-03-03 18:36:34.272055 test begin: paddle.greater_than(x=Tensor([1, 2],"float64"), y=Tensor([2, 0],"float64"), )

[paddle error] paddle.greater_than(x=Tensor([1, 2],"float64"), y=Tensor([2, 0],"float64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [1, 2] and the shape of Y = [2, 0]. Received [2] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.272661 test begin: paddle.greater_than(x=Tensor([1],"float64"), y=Tensor([0],"float64"), )

[Pass] paddle.greater_than(x=Tensor([1],"float64"), y=Tensor([0],"float64"), )
2025-03-03 18:36:34.273274 test begin: paddle.greater_than(x=Tensor([3, 0],"float32"), y=Tensor([3, 0],"float32"), )

[Pass] paddle.greater_than(x=Tensor([3, 0],"float32"), y=Tensor([3, 0],"float32"), )
2025-03-03 18:36:34.273815 test begin: paddle.greater_than(x=Tensor([3, 0],"float32"), y=Tensor([3, 3],"float32"), )

[paddle error] paddle.greater_than(x=Tensor([3, 0],"float32"), y=Tensor([3, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 0] and the shape of Y = [3, 3]. Received [0] in X is not equal to [3] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.274378 test begin: paddle.greater_than(x=Tensor([3, 3],"float32"), y=Tensor([0, 3],"float32"), )

[paddle error] paddle.greater_than(x=Tensor([3, 3],"float32"), y=Tensor([0, 3],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3] and the shape of Y = [0, 3]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.274967 test begin: paddle.greater_than(x=Tensor([3, 3],"float32"), y=Tensor([3, 0],"float32"), )

[paddle error] paddle.greater_than(x=Tensor([3, 3],"float32"), y=Tensor([3, 0],"float32"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3, 3] and the shape of Y = [3, 0]. Received [3] in X is not equal to [0] in Y at i:1.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.275544 test begin: paddle.greater_than(x=Tensor([3],"int64"), y=Tensor([0],"int64"), )

[paddle error] paddle.greater_than(x=Tensor([3],"int64"), y=Tensor([0],"int64"), ) 
 (InvalidArgument) Broadcast dimension mismatch. Operands could not be broadcast together with the shape of X = [3] and the shape of Y = [0]. Received [3] in X is not equal to [0] in Y at i:0.
  [Hint: Expected x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0) == true, but received x_dims_array[i] == y_dims_array[i] || (x_dims_array[i] <= 1 && x_dims_array[i] != 0) || (y_dims_array[i] <= 1 && y_dims_array[i] != 0):0 != true:1.] (at ../paddle/phi/kernels/funcs/common_shape.h:85)

2025-03-03 18:36:34.276175 test begin: paddle.histogram(input=Tensor([0, 4],"float32"), )

[Pass] paddle.histogram(input=Tensor([0, 4],"float32"), )
2025-03-03 18:36:34.277526 test begin: paddle.histogram(input=Tensor([0, 4],"float64"), )

[Pass] paddle.histogram(input=Tensor([0, 4],"float64"), )
2025-03-03 18:36:34.277977 test begin: paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, )

[Pass] paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, )
2025-03-03 18:36:34.278435 test begin: paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, max=5, )

[Pass] paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, max=5, )
2025-03-03 18:36:34.278886 test begin: paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=-2, )

[Pass] paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=-2, )
2025-03-03 18:36:34.279325 test begin: paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=-4, )

[Pass] paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=-4, )
2025-03-03 18:36:34.279764 test begin: paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=-40, )

[Pass] paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=-40, )
2025-03-03 18:36:34.280201 test begin: paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=-41, max=-5, )

[Pass] paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=-41, max=-5, )
2025-03-03 18:36:34.280662 test begin: paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=4, max=5, )

[Pass] paddle.histogram(input=Tensor([0, 4],"float64"), bins=4, min=4, max=5, )
2025-03-03 18:36:34.281155 test begin: paddle.histogram(input=Tensor([0],"float64"), )

[Pass] paddle.histogram(input=Tensor([0],"float64"), )
2025-03-03 18:36:34.281679 test begin: paddle.histogram(input=Tensor([4, 0],"float32"), )

[Pass] paddle.histogram(input=Tensor([4, 0],"float32"), )
2025-03-03 18:36:34.282222 test begin: paddle.histogram(input=Tensor([4, 0],"float64"), )

[Pass] paddle.histogram(input=Tensor([4, 0],"float64"), )
2025-03-03 18:36:34.282733 test begin: paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, )

[Pass] paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, )
2025-03-03 18:36:34.283189 test begin: paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, max=5, )

[Pass] paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, max=5, )
2025-03-03 18:36:34.283653 test begin: paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=-2, )

[Pass] paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=-2, )
2025-03-03 18:36:34.284124 test begin: paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=-4, )

[Pass] paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=-4, )
2025-03-03 18:36:34.284573 test begin: paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=-40, )

[Pass] paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=-40, )
2025-03-03 18:36:34.285096 test begin: paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=-41, max=-5, )

[Pass] paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=-41, max=-5, )
2025-03-03 18:36:34.285568 test begin: paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=4, max=5, )

[Pass] paddle.histogram(input=Tensor([4, 0],"float64"), bins=4, min=4, max=5, )
2025-03-03 18:36:34.286055 test begin: paddle.histogram_bin_edges(Tensor([0, 20],"float32"), bins=10, min=0, max=1, )

[Pass] paddle.histogram_bin_edges(Tensor([0, 20],"float32"), bins=10, min=0, max=1, )
2025-03-03 18:36:34.287193 test begin: paddle.histogram_bin_edges(Tensor([0, 20],"float32"), bins=10, min=0.2, max=0.9, )

[Pass] paddle.histogram_bin_edges(Tensor([0, 20],"float32"), bins=10, min=0.2, max=0.9, )
2025-03-03 18:36:34.287993 test begin: paddle.histogram_bin_edges(Tensor([0, 20],"float32"), bins=10, min=1, max=1, )

[Pass] paddle.histogram_bin_edges(Tensor([0, 20],"float32"), bins=10, min=1, max=1, )
2025-03-03 18:36:34.288693 test begin: paddle.histogram_bin_edges(Tensor([5, 0],"float32"), bins=10, min=0, max=1, )

[Pass] paddle.histogram_bin_edges(Tensor([5, 0],"float32"), bins=10, min=0, max=1, )
2025-03-03 18:36:34.289406 test begin: paddle.histogram_bin_edges(Tensor([5, 0],"float32"), bins=10, min=0.2, max=0.9, )

[Pass] paddle.histogram_bin_edges(Tensor([5, 0],"float32"), bins=10, min=0.2, max=0.9, )
2025-03-03 18:36:34.290264 test begin: paddle.histogram_bin_edges(Tensor([5, 0],"float32"), bins=10, min=1, max=1, )

[Pass] paddle.histogram_bin_edges(Tensor([5, 0],"float32"), bins=10, min=1, max=1, )
2025-03-03 18:36:34.291060 test begin: paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=5, weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=5, weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.291793 test begin: paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:34.292722 test begin: paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:34.293433 test begin: paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=False, )

/usr/local/lib/python3.9/dist-packages/paddle/tensor/linalg.py:5638: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach(), rather than paddle.to_tensor(sourceTensor).
  edge = paddle.to_tensor(edge)
[paddle error] paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=False, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0], X's size = 0, 'shape' is [5, 6], the capacity of 'shape' is 30.
  [Hint: Expected capacity == in_size, but received capacity:30 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-03 18:36:34.300016 test begin: paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0], X's size = 0, 'shape' is [5, 6], the capacity of 'shape' is 30.
  [Hint: Expected capacity == in_size, but received capacity:30 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-03 18:36:34.302915 test begin: paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.303596 test begin: paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.304281 test begin: paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.305158 test begin: paddle.histogramdd(Tensor([0, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=False, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.305815 test begin: paddle.histogramdd(Tensor([0, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.306443 test begin: paddle.histogramdd(Tensor([0, 2],"float32"), bins=list[2,2,], weights=None, ranges=list[0.0,1.0,0.0,1.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2],"float32"), bins=list[2,2,], weights=None, ranges=list[0.0,1.0,0.0,1.0,], density=True, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0], X's size = 0, 'shape' is [4, 4], the capacity of 'shape' is 16.
  [Hint: Expected capacity == in_size, but received capacity:16 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-03 18:36:34.309301 test begin: paddle.histogramdd(Tensor([0, 2],"float64"), bins=list[2,2,], weights=None, ranges=list[0.0,1.0,0.0,1.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([0, 2],"float64"), bins=list[2,2,], weights=None, ranges=list[0.0,1.0,0.0,1.0,], density=True, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0], X's size = 0, 'shape' is [4, 4], the capacity of 'shape' is 16.
  [Hint: Expected capacity == in_size, but received capacity:16 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-03 18:36:34.311997 test begin: paddle.histogramdd(Tensor([0, 2],"float64"), bins=list[3,3,], weights=Tensor([4],"float64"), ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([0, 2],"float64"), bins=list[3,3,], weights=Tensor([4],"float64"), ranges=None, density=False, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.312634 test begin: paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=5, weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=5, weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.313280 test begin: paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:34.313923 test begin: paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=None, ranges=None, density=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-03 18:36:34.314544 test begin: paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=False, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0], X's size = 0, 'shape' is [5, 6], the capacity of 'shape' is 30.
  [Hint: Expected capacity == in_size, but received capacity:30 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-03 18:36:34.317142 test begin: paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [0], X's size = 0, 'shape' is [5, 6], the capacity of 'shape' is 30.
  [Hint: Expected capacity == in_size, but received capacity:30 != in_size:0.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-03 18:36:34.319917 test begin: paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.320648 test begin: paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.321347 test begin: paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.322287 test begin: paddle.histogramdd(Tensor([4, 0, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=False, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.322944 test begin: paddle.histogramdd(Tensor([4, 0, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.323562 test begin: paddle.histogramdd(Tensor([4, 0],"float32"), bins=list[2,2,], weights=None, ranges=list[0.0,1.0,0.0,1.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0],"float32"), bins=list[2,2,], weights=None, ranges=list[0.0,1.0,0.0,1.0,], density=True, ) 
 (InvalidArgument) can not reshape 4, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.324192 test begin: paddle.histogramdd(Tensor([4, 0],"float64"), bins=list[2,2,], weights=None, ranges=list[0.0,1.0,0.0,1.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 0],"float64"), bins=list[2,2,], weights=None, ranges=list[0.0,1.0,0.0,1.0,], density=True, ) 
 (InvalidArgument) can not reshape 4, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.324775 test begin: paddle.histogramdd(Tensor([4, 0],"float64"), bins=list[3,3,], weights=Tensor([4],"float64"), ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 0],"float64"), bins=list[3,3,], weights=Tensor([4],"float64"), ranges=None, density=False, ) 
 (InvalidArgument) can not reshape 4, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.325461 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=5, weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=5, weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.326221 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=False, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.326961 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.327739 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=None, ranges=None, density=False, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.328275 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=None, ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=None, ranges=None, density=True, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.328794 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=False, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.329331 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=None, ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.329912 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.330640 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=list[3,4,], weights=Tensor([4, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.331537 test begin: paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 0],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 (InvalidArgument) can not reshape 4, 2, 0 to -1, 0, because the unspecified dimension 0 can be any number and is ambiguous
  [Hint: Expected unk_dim_idx == -1, but received unk_dim_idx:0 != -1:-1.] (at ../paddle/phi/infermeta/unary.cc:2217)

2025-03-03 18:36:34.333488 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=5, weights=Tensor([0, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=5, weights=Tensor([0, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.334454 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=5, weights=Tensor([4, 0],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=5, weights=Tensor([4, 0],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.335306 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([0, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([0, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.336110 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([0, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([0, 2],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.337228 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 0],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 0],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.338051 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 0],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=list[3,4,], weights=Tensor([4, 0],"float64"), ranges=list[1.0,10.0,1.0,100.0,], density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.338821 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=tuple(Tensor([0],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=tuple(Tensor([0],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 (OutOfRange) The starting index -1 of slice is out of bounds in tensor 0-th axis, it shound be in the range of [0, 0). (at ../paddle/fluid/pybind/slice_utils.h:241)

2025-03-03 18:36:34.342768 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([0],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([0],"float64"),), weights=Tensor([4, 2],"float64"), ranges=None, density=True, ) 
 (OutOfRange) The starting index -1 of slice is out of bounds in tensor 0-th axis, it shound be in the range of [0, 0). (at ../paddle/fluid/pybind/slice_utils.h:241)

2025-03-03 18:36:34.345798 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([0, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([0, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.359425 test begin: paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 0],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 2],"float64"), bins=tuple(Tensor([5],"float64"),Tensor([3],"float64"),), weights=Tensor([4, 0],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.369068 test begin: paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([0, 2],"float64"), ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([0, 2],"float64"), ranges=None, density=False, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.371505 test begin: paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([0, 2],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([0, 2],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.374006 test begin: paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 0],"float64"), ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 0],"float64"), ranges=None, density=False, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.376502 test begin: paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 0],"float64"), ranges=None, density=True, )

[paddle error] paddle.histogramdd(Tensor([4, 2, 4],"float64"), bins=list[1,2,3,4,], weights=Tensor([4, 0],"float64"), ranges=None, density=True, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.378999 test begin: paddle.histogramdd(Tensor([4, 2],"float64"), bins=list[3,3,], weights=Tensor([0],"float64"), ranges=None, density=False, )

[paddle error] paddle.histogramdd(Tensor([4, 2],"float64"), bins=list[3,3,], weights=Tensor([0],"float64"), ranges=None, density=False, ) 
 if weight tensor is provided,it should have the same shape as the input tensor excluding its innermost dimension.

2025-03-03 18:36:34.381557 test begin: paddle.hsplit(Tensor([0, 6, 3],"int64"), 2, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998194 (unix time) try "date -d @1740998194" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 91823 (TID 0x7f42f67c3700) from PID 0 ***]

2025-03-03 18:36:38.542218 test begin: paddle.hsplit(Tensor([0, 6, 3],"int64"), 3, )

W0303 18:36:41.063997 93440 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:36:41.065521 93440 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998201 (unix time) try "date -d @1740998201" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 93270 (TID 0x7fdbf16f8700) from PID 0 ***]

2025-03-03 18:36:45.671520 test begin: paddle.hsplit(Tensor([0, 6],"int64"), 2, )

W0303 18:36:48.564267 93726 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:36:48.565747 93726 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998208 (unix time) try "date -d @1740998208" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 93616 (TID 0x7f5ec9f48700) from PID 0 ***]

2025-03-03 18:36:52.955406 test begin: paddle.hsplit(Tensor([0, 6],"int64"), 3, )

W0303 18:36:55.455513 94041 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:36:55.456543 94041 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998215 (unix time) try "date -d @1740998215" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 93921 (TID 0x7f2b527c3700) from PID 0 ***]

2025-03-03 18:36:59.829123 test begin: paddle.hsplit(Tensor([0],"bool"), 3, )

W0303 18:37:02.463878 94671 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:37:02.464885 94671 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.hsplit(Tensor([0],"bool"), 3, )
2025-03-03 18:37:02.467660 test begin: paddle.hsplit(Tensor([0],"float16"), 3, )

[Pass] paddle.hsplit(Tensor([0],"float16"), 3, )
2025-03-03 18:37:02.470761 test begin: paddle.hsplit(Tensor([0],"int64"), 2, )

[Pass] paddle.hsplit(Tensor([0],"int64"), 2, )
2025-03-03 18:37:02.472955 test begin: paddle.hsplit(Tensor([4, 0, 3],"int64"), 2, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998222 (unix time) try "date -d @1740998222" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 94269 (TID 0x7f6498949700) from PID 0 ***]

2025-03-03 18:37:08.281411 test begin: paddle.hsplit(Tensor([4, 0, 3],"int64"), 3, )

W0303 18:37:11.797544 94897 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:37:11.798673 94897 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998231 (unix time) try "date -d @1740998231" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 94696 (TID 0x7f05e47c3700) from PID 0 ***]

2025-03-03 18:37:15.831743 test begin: paddle.hsplit(Tensor([4, 0],"int64"), 2, )

W0303 18:37:20.964241 102198 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:37:20.965309 102198 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998240 (unix time) try "date -d @1740998240" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 101252 (TID 0x7f1c97dc2700) from PID 0 ***]

2025-03-03 18:37:25.387757 test begin: paddle.hsplit(Tensor([4, 0],"int64"), 3, )

W0303 18:37:29.851404 102813 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:37:29.852528 102813 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998249 (unix time) try "date -d @1740998249" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 102327 (TID 0x7f28fd87e700) from PID 0 ***]

2025-03-03 18:37:34.378016 test begin: paddle.hsplit(Tensor([4, 6, 0],"int64"), 2, )

W0303 18:37:38.713050 103252 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:37:38.714071 103252 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998258 (unix time) try "date -d @1740998258" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 103056 (TID 0x7f2e38949700) from PID 0 ***]

2025-03-03 18:37:43.011872 test begin: paddle.hsplit(Tensor([4, 6, 0],"int64"), 3, )

W0303 18:37:46.950381 103567 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:37:46.951333 103567 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::concat(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::experimental::ScalarBase<paddle::Tensor> const&)
4   void phi::ConcatKernel<long, phi::GPUContext>(phi::GPUContext const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, phi::DenseTensor*)
5   void phi::funcs::ConcatFunctorWithIndexType<long, int>(phi::GPUContext const&, std::vector<phi::DenseTensor, std::allocator<phi::DenseTensor> > const&, int, phi::DenseTensor*)

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998266 (unix time) try "date -d @1740998266" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x0) received by PID 103364 (TID 0x7fa441935700) from PID 0 ***]

2025-03-03 18:37:51.747016 test begin: paddle.increment(Tensor([0],"float32"), value=2.0, )

W0303 18:37:54.696470 103942 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:37:54.697607 103942 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.increment(Tensor([0],"float32"), value=2.0, ) 
 (InvalidArgument) The number of elements in Input(X) should be 1.Now the number is 0.
  [Hint: Expected product(x.dims()) == 1UL, but received product(x.dims()):0 != 1UL:1.] (at ../paddle/phi/infermeta/unary.cc:2141)

2025-03-03 18:37:54.698631 test begin: paddle.increment(Tensor([0],"int64"), )

[paddle error] paddle.increment(Tensor([0],"int64"), ) 
 (InvalidArgument) The number of elements in Input(X) should be 1.Now the number is 0.
  [Hint: Expected product(x.dims()) == 1UL, but received product(x.dims()):0 != 1UL:1.] (at ../paddle/phi/infermeta/unary.cc:2141)

2025-03-03 18:37:54.700752 test begin: paddle.increment(x=Tensor([0],"float32"), )

[paddle error] paddle.increment(x=Tensor([0],"float32"), ) 
 (InvalidArgument) The number of elements in Input(X) should be 1.Now the number is 0.
  [Hint: Expected product(x.dims()) == 1UL, but received product(x.dims()):0 != 1UL:1.] (at ../paddle/phi/infermeta/unary.cc:2141)

2025-03-03 18:37:54.701727 test begin: paddle.increment(x=Tensor([0],"float64"), )

[paddle error] paddle.increment(x=Tensor([0],"float64"), ) 
 (InvalidArgument) The number of elements in Input(X) should be 1.Now the number is 0.
  [Hint: Expected product(x.dims()) == 1UL, but received product(x.dims()):0 != 1UL:1.] (at ../paddle/phi/infermeta/unary.cc:2141)

2025-03-03 18:37:54.705233 test begin: paddle.increment(x=Tensor([0],"float64"), value=2.0, )

[paddle error] paddle.increment(x=Tensor([0],"float64"), value=2.0, ) 
 (InvalidArgument) The number of elements in Input(X) should be 1.Now the number is 0.
  [Hint: Expected product(x.dims()) == 1UL, but received product(x.dims()):0 != 1UL:1.] (at ../paddle/phi/infermeta/unary.cc:2141)

2025-03-03 18:37:54.706109 test begin: paddle.increment(x=Tensor([0],"int64"), value=1.0, )

[paddle error] paddle.increment(x=Tensor([0],"int64"), value=1.0, ) 
 (InvalidArgument) The number of elements in Input(X) should be 1.Now the number is 0.
  [Hint: Expected product(x.dims()) == 1UL, but received product(x.dims()):0 != 1UL:1.] (at ../paddle/phi/infermeta/unary.cc:2141)

2025-03-03 18:37:54.706881 test begin: paddle.incubate.nn.functional.blha_get_max_len(Tensor([0],"int32"), Tensor([0],"int32"), Tensor([0],"float32"), )

[Pass] paddle.incubate.nn.functional.blha_get_max_len(Tensor([0],"int32"), Tensor([0],"int32"), Tensor([0],"float32"), )
2025-03-03 18:37:54.709889 test begin: paddle.incubate.nn.functional.blha_get_max_len(Tensor([0],"int32"), Tensor([10],"int32"), Tensor([10],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_blha_get_max_len(_object*, _object*, _object*)
1   blha_get_max_len_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::blha_get_max_len(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::fusion::BlhaGetMaxLenKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998275 (unix time) try "date -d @1740998275" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x194f9) received by PID 103673 (TID 0x7f9e9c949700) from PID 103673 ***]

2025-03-03 18:37:59.269001 test begin: paddle.incubate.nn.functional.blha_get_max_len(Tensor([10],"int32"), Tensor([0],"int32"), Tensor([10],"float32"), )

W0303 18:38:02.162752 104767 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:38:02.163870 104767 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_blha_get_max_len(_object*, _object*, _object*)
1   blha_get_max_len_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::blha_get_max_len(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::fusion::BlhaGetMaxLenKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998282 (unix time) try "date -d @1740998282" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19757) received by PID 104279 (TID 0x7f727ddc2700) from PID 104279 ***]

2025-03-03 18:38:14.322106 test begin: paddle.incubate.nn.functional.blha_get_max_len(Tensor([10],"int32"), Tensor([10],"int32"), Tensor([0],"float32"), )

W0303 18:38:17.113094 105282 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:38:17.114183 105282 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.incubate.nn.functional.blha_get_max_len(Tensor([10],"int32"), Tensor([10],"int32"), Tensor([0],"float32"), )
2025-03-03 18:38:17.115357 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.119532 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.122166 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.123976 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.126469 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.128434 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.130433 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.132277 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.135019 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 256],"float16"), Tensor([256],"float16"), act_method="gelu", )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 256],"float16"), Tensor([256],"float16"), act_method="gelu", ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.141922 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 640],"float16"), None, act_method="swiglu", )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 640],"float16"), None, act_method="swiglu", ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.143467 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 64],"float16"), None, act_method="swiglu", )

[paddle error] paddle.incubate.nn.functional.fused_bias_act(Tensor([0, 64],"float16"), None, act_method="swiglu", ) 
 (InvalidArgument) The size of Attr(rows) must > 0
  [Hint: Expected x.numel() / dim > 0, but received x.numel() / dim:0 <= 0:0.] (at ../paddle/phi/infermeta/multiary.cc:2297)

2025-03-03 18:38:17.144630 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([1, 0],"float16"), None, act_method="swiglu", )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998297 (unix time) try "date -d @1740998297" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f7e90d5335c) received by PID 105013 (TID 0x7f7e20949700) from PID 18446744071844475740 ***]

2025-03-03 18:38:21.400090 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([100, 0],"float16"), None, act_method="swiglu", )

W0303 18:38:24.033068 106409 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:38:24.034298 106409 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998304 (unix time) try "date -d @1740998304" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f27d37ee35c) received by PID 105359 (TID 0x7f2753d0b700) from PID 18446744072962892636 ***]

2025-03-03 18:38:28.247013 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([101, 0],"float16"), Tensor([256],"float16"), act_method="gelu", )

W0303 18:38:30.852304 107563 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:38:30.853293 107563 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998310 (unix time) try "date -d @1740998310" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fbf3f24635c) received by PID 107189 (TID 0x7fbea7744700) from PID 1059349340 ***]

2025-03-03 18:38:34.965745 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([101, 256],"float16"), Tensor([0],"float16"), act_method="gelu", )

W0303 18:38:37.738831 107870 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:38:37.739926 107870 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.incubate.nn.functional.fused_bias_act(Tensor([101, 256],"float16"), Tensor([0],"float16"), act_method="gelu", )
2025-03-03 18:38:37.742230 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"float16"), Tensor([256],"float16"), act_method="gelu", )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998317 (unix time) try "date -d @1740998317" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f56d600835c) received by PID 107658 (TID 0x7f5661dc2700) from PID 18446744073004942172 ***]

2025-03-03 18:38:41.886933 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:38:44.352072 108264 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:38:44.353030 108264 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998324 (unix time) try "date -d @1740998324" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f03059d435c) received by PID 107992 (TID 0x7f028bdc2700) from PID 94192476 ***]

2025-03-03 18:38:48.533949 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:38:51.138962 108621 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:38:51.139882 108621 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998331 (unix time) try "date -d @1740998331" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc54bffc35c) received by PID 108296 (TID 0x7fc47ff48700) from PID 1275052892 ***]

2025-03-03 18:38:55.505356 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:38:58.225865 109147 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:38:58.227195 109147 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998338 (unix time) try "date -d @1740998338" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fd0c8ff735c) received by PID 108721 (TID 0x7fd03134a700) from PID 18446744072786768732 ***]

2025-03-03 18:39:02.555099 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:39:05.025538 109668 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:39:05.026669 109668 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998345 (unix time) try "date -d @1740998345" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f778449e35c) received by PID 109461 (TID 0x7f770e949700) from PID 18446744071634019164 ***]

2025-03-03 18:39:16.068462 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:39:18.874051 110435 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:39:18.875478 110435 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998358 (unix time) try "date -d @1740998358" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f6755aa935c) received by PID 110077 (TID 0x7f66b1dc2700) from PID 1437242204 ***]

2025-03-03 18:39:23.561862 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:39:26.589615 110765 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:39:26.590633 110765 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998366 (unix time) try "date -d @1740998366" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fb441b7a35c) received by PID 110634 (TID 0x7fb38bdc2700) from PID 1102553948 ***]

2025-03-03 18:39:37.908252 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:39:40.763458 111914 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:39:40.764526 111914 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998380 (unix time) try "date -d @1740998380" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f344dc3f35c) received by PID 111628 (TID 0x7f3391dc2700) from PID 1304687452 ***]

2025-03-03 18:39:45.028400 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:39:49.081040 112229 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:39:49.082660 112229 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998389 (unix time) try "date -d @1740998389" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f4b80a9f35c) received by PID 112093 (TID 0x7f4adc949700) from PID 18446744071573205852 ***]

2025-03-03 18:39:53.473081 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:39:56.065984 112541 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:39:56.066995 112541 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998396 (unix time) try "date -d @1740998396" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b6d7) received by PID 112343 (TID 0x7f8e7f935700) from PID 112343 ***]

2025-03-03 18:40:00.804035 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:40:03.977527 113189 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:40:03.978565 113189 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998404 (unix time) try "date -d @1740998404" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b8da) received by PID 112858 (TID 0x7ff62ddc2700) from PID 112858 ***]

2025-03-03 18:40:08.713164 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:40:11.583143 113558 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:40:11.584179 113558 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998411 (unix time) try "date -d @1740998411" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bb2c) received by PID 113452 (TID 0x7fa0b5d0b700) from PID 113452 ***]

2025-03-03 18:40:15.894034 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:40:18.593964 113939 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:40:18.594976 113939 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998418 (unix time) try "date -d @1740998418" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bc4d) received by PID 113741 (TID 0x7fccb9dc2700) from PID 113741 ***]

2025-03-03 18:40:23.108741 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:40:25.670137 114272 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:40:25.671319 114272 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998425 (unix time) try "date -d @1740998425" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bd9f) received by PID 114079 (TID 0x7f2549b85700) from PID 114079 ***]

2025-03-03 18:40:31.161302 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:40:33.977138 116967 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:40:33.978996 116967 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998434 (unix time) try "date -d @1740998434" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bfb7) received by PID 114615 (TID 0x7f1140949700) from PID 114615 ***]

2025-03-03 18:40:38.353735 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:40:43.220221 117343 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:40:43.221889 117343 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998443 (unix time) try "date -d @1740998443" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c952) received by PID 117074 (TID 0x7f6ebc7c3700) from PID 117074 ***]

2025-03-03 18:40:48.208259 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:40:52.114151 119756 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:40:52.115433 119756 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998452 (unix time) try "date -d @1740998452" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1caf2) received by PID 117490 (TID 0x7f29a27c3700) from PID 117490 ***]

2025-03-03 18:41:03.836192 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 256],"float16"), Tensor([0],"float16"), act_method="gelu", )

W0303 18:41:10.078785 121236 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:41:10.080801 121236 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 256],"float16"), Tensor([0],"float16"), act_method="gelu", )
2025-03-03 18:41:10.082373 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998470 (unix time) try "date -d @1740998470" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f0d176ff35c) received by PID 120897 (TID 0x7f0c73d0b700) from PID 393212764 ***]

2025-03-03 18:41:14.630486 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:41:18.209988 124023 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:41:18.210961 124023 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998478 (unix time) try "date -d @1740998478" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f17e10d735c) received by PID 122436 (TID 0x7f171f6f8700) from PID 18446744073190339420 ***]

2025-03-03 18:41:22.780102 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:41:26.511026 124623 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:41:26.512073 124623 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998486 (unix time) try "date -d @1740998486" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f3cf306c35c) received by PID 124207 (TID 0x7f3c3d4f4700) from PID 18446744073491891036 ***]

2025-03-03 18:41:31.430406 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:41:36.347803 125773 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:41:36.348743 125773 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998496 (unix time) try "date -d @1740998496" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fee42f4b35c) received by PID 125226 (TID 0x7fed954f4700) from PID 1123332956 ***]

2025-03-03 18:41:47.821072 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:41:51.913715 127687 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:41:51.914987 127687 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998511 (unix time) try "date -d @1740998511" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc907a7a35c) received by PID 126895 (TID 0x7fc86ff48700) from PID 128426844 ***]

2025-03-03 18:41:56.411552 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:41:59.022987 128544 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:41:59.024142 128544 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998519 (unix time) try "date -d @1740998519" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f45db83e35c) received by PID 127976 (TID 0x7f4559d0b700) from PID 18446744073097438044 ***]

2025-03-03 18:42:03.861120 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:42:07.735244 129329 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:42:07.736223 129329 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998527 (unix time) try "date -d @1740998527" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7fc0c45cc35c) received by PID 128979 (TID 0x7fc048949700) from PID 18446744072708997980 ***]

2025-03-03 18:42:12.099612 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 0],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:42:15.623857 130235 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:42:15.625380 130235 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_fused_bias_act(_object*, _object*, _object*)
1   fused_bias_act_ad_func(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string, std::string, float, int, float, float)
2   paddle::experimental::fused_bias_act(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, std::string const&, std::string const&, float, int, float, float)
3   phi::FusedBiasActInferMeta(phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, phi::MetaTensor const&, std::string const&, std::string const&, float, int, float, float, phi::MetaTensor*, phi::MetaConfig)

----------------------
Error Message Summary:
----------------------
FatalError: `Erroneous arithmetic operation` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998535 (unix time) try "date -d @1740998535" if you are using GNU date ***]
  [SignalInfo: *** SIGFPE (@0x7f61d5add35c) received by PID 129660 (TID 0x7f6167dc2700) from PID 18446744072999523164 ***]

2025-03-03 18:42:20.690350 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:42:24.142633 130741 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:42:24.143872 130741 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998544 (unix time) try "date -d @1740998544" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fcf5) received by PID 130293 (TID 0x7efa65f48700) from PID 130293 ***]

2025-03-03 18:42:29.185921 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:42:33.967872 132058 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:42:33.969085 132058 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998554 (unix time) try "date -d @1740998554" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x200ec) received by PID 131308 (TID 0x7f47516f8700) from PID 131308 ***]

2025-03-03 18:42:39.074534 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:42:43.742996 133596 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:42:43.744328 133596 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998564 (unix time) try "date -d @1740998564" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2057f) received by PID 132479 (TID 0x7f07912b7700) from PID 132479 ***]

2025-03-03 18:42:48.805597 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:42:52.638379 134324 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:42:52.641634 134324 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998572 (unix time) try "date -d @1740998572" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20b4a) received by PID 133962 (TID 0x7ff381dc2700) from PID 133962 ***]

2025-03-03 18:42:57.549703 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

W0303 18:43:01.396792 135524 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:43:01.398463 135524 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998581 (unix time) try "date -d @1740998581" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20e0f) received by PID 134671 (TID 0x7efc750f7700) from PID 134671 ***]

2025-03-03 18:43:14.830180 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.831031 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.831705 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([0],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.832182 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 10],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.832485 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 10],"int32"), bias=Tensor([10],"float32"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.832837 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"float16"), bias=Tensor([512],"float16"), act_method="geglu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.833201 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"float16"), bias=Tensor([512],"float16"), act_method="gelu", )

[Skip]
2025-03-03 18:43:14.833434 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"float16"), bias=Tensor([512],"float16"), act_method="gelu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.833717 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"float16"), bias=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.833968 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"float16"), bias=Tensor([512],"float16"), act_method="swiglu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.834207 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"float32"), bias=None, act_method="gelu", )

[Skip]
2025-03-03 18:43:14.834407 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"float32"), bias=Tensor([512],"float32"), act_method="gelu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.834650 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"float32"), bias=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.834893 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.835287 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.836696 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([0, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.842564 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 10],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.842867 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 10],"int32"), bias=Tensor([10],"float32"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.843162 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"float16"), bias=Tensor([512],"float16"), act_method="geglu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.843618 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"float16"), bias=Tensor([512],"float16"), act_method="gelu", )

[Skip]
2025-03-03 18:43:14.843968 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"float16"), bias=Tensor([512],"float16"), act_method="gelu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.844209 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"float16"), bias=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.845191 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"float16"), bias=Tensor([512],"float16"), act_method="swiglu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.845450 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"float32"), bias=None, act_method="gelu", )

[Skip]
2025-03-03 18:43:14.845660 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"float32"), bias=Tensor([512],"float32"), act_method="gelu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.845863 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"float32"), bias=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.846050 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.847296 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.847675 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 0, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.847986 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="geglu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.848173 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="gelu", )

[Skip]
2025-03-03 18:43:14.848399 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="gelu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.848689 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.848912 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float16"), bias=Tensor([512],"float16"), act_method="swiglu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.849136 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float32"), bias=None, act_method="gelu", )

[Skip]
2025-03-03 18:43:14.849354 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float32"), bias=Tensor([512],"float32"), act_method="gelu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.849691 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"float32"), bias=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.849895 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.850522 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([10],"float32"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.851048 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.851381 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.851762 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 0],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.852114 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([0],"float16"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.852701 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([0],"float32"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.853676 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([0],"float32"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.854450 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([10],"float32"), dequant_scales=Tensor([0],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.854683 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"float16"), bias=Tensor([0],"float16"), act_method="geglu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.854868 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"float16"), bias=Tensor([0],"float16"), act_method="gelu", )

[Skip]
2025-03-03 18:43:14.855041 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"float16"), bias=Tensor([0],"float16"), act_method="gelu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.855277 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"float16"), bias=Tensor([0],"float16"), act_method="gelu", compute_dtype="fp16", )

[Skip]
2025-03-03 18:43:14.855443 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"float16"), bias=Tensor([0],"float16"), act_method="swiglu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.855693 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"float32"), bias=Tensor([0],"float32"), act_method="gelu", compute_dtype="default", )

[Skip]
2025-03-03 18:43:14.856029 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"float32"), bias=Tensor([0],"float32"), act_method="gelu", compute_dtype="fp32", )

[Skip]
2025-03-03 18:43:14.856386 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([0],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.856901 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([0],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.857809 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([0],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.858162 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([0],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.859333 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([0],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.859805 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([0],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.860179 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([0],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.860559 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([0],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.860955 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([0],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.861282 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([0],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.861579 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([0],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.861973 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([0],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

[Skip]
2025-03-03 18:43:14.862266 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

W0303 18:43:18.055064 137591 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:43:18.056806 137591 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.416917 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.503498 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.613067 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.617921 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.622850 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.626343 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.631139 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.635430 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([0, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.641905 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.645400 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.648929 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.652273 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 0, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.655751 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 0],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 0],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.662825 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([0, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([0, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 8, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:8 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.666918 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 0],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 0],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.672825 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([0, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([0, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 8, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:8 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.676747 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.679856 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
2025-03-03 18:43:19.684368 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
2025-03-03 18:43:19.688164 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
2025-03-03 18:43:19.691316 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
2025-03-03 18:43:19.695318 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
2025-03-03 18:43:19.698271 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([0],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
2025-03-03 18:43:19.701508 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.703429 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.705310 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.707021 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.708894 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.711774 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.714514 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.719278 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.723282 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.725600 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.728617 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.731526 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.734061 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.736757 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.739742 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.742390 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.746562 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.749835 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.753976 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), None, None, Tensor([4],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.757464 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.762406 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.766233 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([0],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([0],"float32"), Tensor([4],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.769707 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([0],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), None, None, Tensor([4],"float32"), Tensor([0],"float32"), dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.773766 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.777319 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), Tensor([4],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.780730 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.783602 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 2, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4, 4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([4],"float32"), Tensor([0],"float32"), None, None, dropout1_rate=0, dropout2_rate=0, activation="relu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=True, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:19.789534 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.795063 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.799876 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 0, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:19.809140 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 0],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 0],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.814787 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 0],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 0],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.818403 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 0],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 0],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.821699 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([0, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([0, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 508, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:508 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.848926 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([0, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([0, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 508, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:508 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:19.894566 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 0],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 0],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:20.035204 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 0],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 0],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:20.090599 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([0, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([0, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 130, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:130 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:20.116274 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([0, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([0, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 130, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:130 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:20.167529 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 0],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 0],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:20.203470 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 0],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 0],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:20.256414 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:20.382847 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
2025-03-03 18:43:20.455677 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:20.550855 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
2025-03-03 18:43:20.596969 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:20.646258 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
2025-03-03 18:43:20.691750 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:20.735919 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
2025-03-03 18:43:20.790936 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:20.862541 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
2025-03-03 18:43:20.936230 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:20.981716 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([0],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
2025-03-03 18:43:21.024960 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([0, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([0, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 508, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:508 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:21.054851 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 0],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 0],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)
  [operator < fused_feedforward > error]
2025-03-03 18:43:21.093346 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([0, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([0, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 130, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:130 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:21.143376 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 0],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 0],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:170)
  [operator < fused_feedforward > error]
2025-03-03 18:43:21.230340 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:21.293940 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:21.344245 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:21.394624 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:21.503275 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([0],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:21.564505 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([0],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([0],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
2025-03-03 18:43:21.646460 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 0, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 0, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:21.734148 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 0, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 0, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:21.799561 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 0],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 0],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:21.877847 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 0],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 0],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:21.932470 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([0, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([0, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 768, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:768 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:22.071660 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 0],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 0],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)
  [operator < fused_feedforward > error]
2025-03-03 18:43:22.250097 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([0, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([0, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 3072, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:3072 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:22.390060 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 0],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 0],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)
  [operator < fused_feedforward > error]
2025-03-03 18:43:22.516835 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([0],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([0],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:22.729293 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([0],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([0],"float16"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:22.903950 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([0],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([0],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:23.106419 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([0],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float16"), Tensor([768, 3072],"float16"), Tensor([3072, 768],"float16"), Tensor([3072],"float16"), Tensor([768],"float16"), None, None, Tensor([768],"float32"), Tensor([0],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:23.283162 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([0, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([0, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 768, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:768 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:23.352148 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 0],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 0],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:23.456889 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([0, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([0, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 3072, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:3072 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:23.563304 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 0],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 0],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:23.628259 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([0],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([0],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:23.806938 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([0],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([0],"float32"), None, None, Tensor([768],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:23.929121 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([0],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([0],"float32"), Tensor([768],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:24.057287 test begin: paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([0],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(Tensor([32, 128, 768],"float32"), Tensor([768, 3072],"float32"), Tensor([3072, 768],"float32"), Tensor([3072],"float32"), Tensor([768],"float32"), None, None, Tensor([768],"float32"), Tensor([0],"float32"), dropout1_rate=0.0, dropout2_rate=0.1, activation="gelu", ln1_epsilon=1e-05, ln2_epsilon=1e-05, pre_layer_norm=False, training=True, ring_id=-1, name=None, )
2025-03-03 18:43:24.182860 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.186698 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.188731 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.190520 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.192664 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.194955 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([0, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.197690 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.199427 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.201011 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.202590 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.204448 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.206600 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[cuda error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 0, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.208747 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.210118 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.211419 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.212635 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.214203 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.216119 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 0],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) Product from the X shape[1] to shape[n-1] must be larger than 1!
  [Hint: Expected mat_dim_x.width_ > static_cast<size_t>(1), but received mat_dim_x.width_:0 <= static_cast<size_t>(1):1.] (at ../paddle/fluid/operators/fused/fused_feedforward_op.cc:82)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.218031 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.219645 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.221179 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 2],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.222759 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.224473 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.226453 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([0, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.228664 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([2, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.232895 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.235222 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([2, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.240476 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.243102 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.245972 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 0],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.247836 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([0, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([0, 2],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.251190 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([0, 2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([0, 2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.253008 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([0, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([0, 2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 2, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:2 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.255522 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 0],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 0],"float32"), activation="gelu", dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.260198 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 0],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 0],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.262425 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 0],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 2],"float32"), linear2_weight=Tensor([2, 0],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.263999 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([0, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([0, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.265757 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([0, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([0, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.267604 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([0, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([0, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (InvalidArgument) The first matrix width should be same as second matrix height,but received first matrix width 4, second matrix height 0
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:4 != dim_b.height_:0.] (at ../paddle/phi/kernels/funcs/blas/blas_impl.h:1748)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.269458 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 0],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 0],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.272260 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 0],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 0],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.279347 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 0],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[paddle error] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 0],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)
  [operator < fused_feedforward > error]
2025-03-03 18:43:24.282284 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([0],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([0],"float32"), linear2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )
2025-03-03 18:43:24.288259 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([0],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([0],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )
2025-03-03 18:43:24.297979 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([0],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([0],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )
2025-03-03 18:43:24.301406 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([0],"float32"), dropout1_rate=0, dropout2_rate=0, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([0],"float32"), dropout1_rate=0, dropout2_rate=0, )
2025-03-03 18:43:24.307261 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([0],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([0],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )
2025-03-03 18:43:24.310289 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([0],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([0],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )
2025-03-03 18:43:24.314330 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([0],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([0],"float32"), ln1_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )
2025-03-03 18:43:24.327647 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([0],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln1_scale=Tensor([2],"float32"), ln1_bias=Tensor([0],"float32"), dropout1_rate=0, dropout2_rate=0, pre_layer_norm=True, )
2025-03-03 18:43:24.335848 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([0],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([0],"float32"), ln2_bias=Tensor([2],"float32"), dropout1_rate=0, dropout2_rate=0, )
2025-03-03 18:43:24.346236 test begin: paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([0],"float32"), dropout1_rate=0, dropout2_rate=0, )

[Pass] paddle.incubate.nn.functional.fused_feedforward(x=Tensor([1, 2, 2],"float32"), linear1_weight=Tensor([2, 4],"float32"), linear2_weight=Tensor([4, 2],"float32"), linear1_bias=Tensor([4],"float32"), linear2_bias=Tensor([2],"float32"), ln2_scale=Tensor([2],"float32"), ln2_bias=Tensor([0],"float32"), dropout1_rate=0, dropout2_rate=0, )
2025-03-03 18:43:24.349182 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 1, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 1, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.351884 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:43:24.355481 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.357216 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.360778 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
2025-03-03 18:43:24.363814 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )
2025-03-03 18:43:24.365465 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:43:24.366841 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.369246 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([0, 512],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([0, 512],"float16"), )
2025-03-03 18:43:24.370177 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([100, 512],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([100, 512],"float16"), )
2025-03-03 18:43:24.373562 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([104, 512],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([104, 512],"float16"), )
2025-03-03 18:43:24.375233 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.376268 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )
2025-03-03 18:43:24.377702 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 64],"float16"), )
2025-03-03 18:43:24.378951 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([0, 64],"float16"), )
2025-03-03 18:43:24.379894 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([1, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([1, 64],"float16"), )
2025-03-03 18:43:24.381775 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([101, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([101, 64],"float16"), )
2025-03-03 18:43:24.383684 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.385711 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.387264 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.388891 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), )

[cuda error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([0, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:43:24.390487 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([1, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([1, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([1, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([1, 0],"float16"), )
2025-03-03 18:43:24.391467 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([1, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([1, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([1, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([1, 64],"float16"), )
2025-03-03 18:43:24.392343 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([1, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([1, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([0, 64],"float16"), )
2025-03-03 18:43:24.393240 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([1, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([1, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([1, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([1, 0],"float16"), )
2025-03-03 18:43:24.394133 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([100, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([100, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([100, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([100, 0],"float16"), )
2025-03-03 18:43:24.394871 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([100, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([100, 512],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([100, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([100, 512],"float16"), )
2025-03-03 18:43:24.396521 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([100, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([0, 512],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([100, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([0, 512],"float16"), )
2025-03-03 18:43:24.398126 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([100, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([100, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([100, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([100, 0],"float16"), )
2025-03-03 18:43:24.405311 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.408090 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )
2025-03-03 18:43:24.409208 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([101, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([101, 0],"float16"), )
2025-03-03 18:43:24.410171 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([101, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([101, 64],"float16"), )
2025-03-03 18:43:24.411400 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 0],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.419523 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), Tensor([0],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), Tensor([0],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [64], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.422081 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), Tensor([64],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), Tensor([64],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, )
2025-03-03 18:43:24.424533 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([101, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([101, 64],"float16"), )
2025-03-03 18:43:24.426450 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )
2025-03-03 18:43:24.427799 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 0],"float16"), )
2025-03-03 18:43:24.429091 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([0, 64],"float16"), )
2025-03-03 18:43:24.430175 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([101, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-06, begin_norm_axis=1, bias=None, residual=Tensor([101, 0],"float16"), )
2025-03-03 18:43:24.431169 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([0],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([0],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [64], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.432493 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([0],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([0],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )
2025-03-03 18:43:24.434540 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([101, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([101, 64],"float16"), )
2025-03-03 18:43:24.440711 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )
2025-03-03 18:43:24.442643 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 0],"float16"), )
2025-03-03 18:43:24.444538 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([104, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([104, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([104, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([104, 0],"float16"), )
2025-03-03 18:43:24.445844 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([104, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([104, 512],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([104, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([104, 512],"float16"), )
2025-03-03 18:43:24.449290 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([104, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([0, 512],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([104, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([0, 512],"float16"), )
2025-03-03 18:43:24.451961 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([104, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([104, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([104, 512],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=None, residual=Tensor([104, 0],"float16"), )
2025-03-03 18:43:24.453590 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:43:24.454608 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [256]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:256.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.455667 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [256]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:256.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.457857 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [256]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:256.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.459446 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [256]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:256.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.460615 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:43:24.461964 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 0],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [256]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:256.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.463409 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:43:24.464661 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([0, 256],"float16"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([0, 256],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:43:24.466904 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 0],"float16"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 0],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:43:24.474186 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [256], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:256 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.475390 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [256], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:256 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.476650 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [256], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:256 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.478041 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [256], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:256 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:43:24.479055 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, )
2025-03-03 18:43:24.480719 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:43:24.482211 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)

 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to DGEMM  parameter number 8 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998604 (unix time) try "date -d @1740998604" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21723) received by PID 136995 (TID 0x7fafd47c3700) from PID 136995 ***]

2025-03-03 18:43:30.319673 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

W0303 18:43:33.323252 139509 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:43:33.324194 139509 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998613 (unix time) try "date -d @1740998613" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21eed) received by PID 138989 (TID 0x7fc189dc2700) from PID 138989 ***]

2025-03-03 18:43:38.832776 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )

W0303 18:43:41.708297 140296 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:43:41.709211 140296 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:43:41.710569 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998621 (unix time) try "date -d @1740998621" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x222af) received by PID 139951 (TID 0x7f7234949700) from PID 139951 ***]

2025-03-03 18:43:53.588676 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([0, 256],"float16"), residual_alpha=0.69204696, )

W0303 18:43:57.040113 141859 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:43:57.041296 141859 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([0, 256],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:43:57.042980 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([0, 256],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998637 (unix time) try "date -d @1740998637" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2286f) received by PID 141423 (TID 0x7fcf29f48700) from PID 141423 ***]

2025-03-03 18:44:01.721273 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 0],"float16"), residual_alpha=0.69204696, )

W0303 18:44:07.277446 143166 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:44:07.278659 143166 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 0],"float16"), residual_alpha=0.69204696, )
2025-03-03 18:44:07.280530 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 0],"float16"), residual_alpha=0.69204696, quant_scale=0.15, quant_round_type=1, quant_max_bound=127, quant_min_bound=-127, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998647 (unix time) try "date -d @1740998647" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22e1f) received by PID 142879 (TID 0x7fb8e9b85700) from PID 142879 ***]

2025-03-03 18:44:13.475546 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([0],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )

W0303 18:44:17.051781 144266 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:44:17.052910 144266 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([0],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:44:17.054552 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([0, 256],"float32"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([0, 256],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:44:17.057370 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 0],"float32"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), None, None, 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 0],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:44:17.059530 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([0],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [256], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:256 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.064445 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([256],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:44:17.072053 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([0],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([0],"float32"), residual=Tensor([16, 256],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:44:17.079151 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([0, 256],"float32"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([0, 256],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:44:17.084177 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 0],"float32"), residual_alpha=0.69204696, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float32"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float32"), residual=Tensor([16, 0],"float32"), residual_alpha=0.69204696, )
2025-03-03 18:44:17.088986 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 0, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 0, 64],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.094987 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 0],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 64],"float16"), )
2025-03-03 18:44:17.102888 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 0],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 0],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.110136 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 1, 0],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 1, 0],"float16"), Tensor([64],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.113354 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 1, 64],"float16"), Tensor([0],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 1, 64],"float16"), Tensor([0],"float32"), Tensor([64],"float32"), 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [64], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.119106 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 1, 64],"float16"), Tensor([64],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 1, 64],"float16"), Tensor([64],"float32"), Tensor([0],"float32"), 1e-05, begin_norm_axis=1, )
2025-03-03 18:44:17.122905 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([2, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([2, 64],"float16"), )
2025-03-03 18:44:17.129366 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )
2025-03-03 18:44:17.132429 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=None, norm_bias=None, epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 0],"float16"), )
2025-03-03 18:44:17.139517 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([0],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([0],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [64], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.158983 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([0],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([0],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), )
2025-03-03 18:44:17.163441 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([2, 1, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([2, 1, 64],"float16"), )
2025-03-03 18:44:17.167038 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 1, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 1, 64],"float16"), )
2025-03-03 18:44:17.172094 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 0, 64],"float16"), )
2025-03-03 18:44:17.175088 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 0],"float16"), )
2025-03-03 18:44:17.178067 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 0],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 0],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.180754 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([0],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([0],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [64], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.183107 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([0],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([0],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), )
2025-03-03 18:44:17.186107 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([58, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([58, 64],"float16"), )
2025-03-03 18:44:17.189552 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )
2025-03-03 18:44:17.192554 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 0],"float16"), )
2025-03-03 18:44:17.199358 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 0],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 0],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [0], received sizeof Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.205037 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([0],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([0],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), ) 
 (InvalidArgument) The normalized size of Input(X) must equal to bethe size of Weight, but receivednormalized size of Input(X) is [64], received sizeof Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:2450)

2025-03-03 18:44:17.211669 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([0],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([0],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), )
2025-03-03 18:44:17.216436 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([59, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([0],"float16"), residual=Tensor([59, 64],"float16"), )
2025-03-03 18:44:17.226105 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([0, 64],"float16"), )
2025-03-03 18:44:17.228419 test begin: paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 0],"float16"), )

[Pass] paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 0],"float16"), )
2025-03-03 18:44:17.233262 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 1024, 1024],"float16"), Tensor([1024, 1024],"float16"), Tensor([1024],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 1024, 1024],"float16"), Tensor([1024, 1024],"float16"), Tensor([1024],"float16"), name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.772698 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 1024, 1024],"float16"), Tensor([1024, 3072],"float16"), Tensor([3072],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 1024, 1024],"float16"), Tensor([1024, 3072],"float16"), Tensor([3072],"float16"), name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.842488 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 1024],"float16"), Tensor([1024, 1000],"float16"), Tensor([1000],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 1024],"float16"), Tensor([1024, 1000],"float16"), Tensor([1000],"float16"), False, None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.861337 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 115, 64],"float16"), Tensor([64, 64],"float16"), None, )

W0303 18:44:18.867251 144750 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 115, 64],"float16"), Tensor([64, 64],"float16"), None, ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 14720, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):14720 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 18:44:18.867511 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 127, 64],"float16"), Tensor([64, 64],"float16"), None, )

W0303 18:44:18.868968 144751 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 127, 64],"float16"), Tensor([64, 64],"float16"), None, ) 
 (PreconditionNotMet) Tensor's dimension is out of bound.Tensor's dimension must be equal or less than the size of its memory.But received Tensor's dimension is 16256, memory's size is 0.
  [Hint: Expected numel() * SizeOf(dtype()) <= memory_size(), but received numel() * SizeOf(dtype()):16256 > memory_size():0.] (at ../paddle/phi/core/dense_tensor_impl.cc:57)

2025-03-03 18:44:18.869121 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 132, 64],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 132, 64],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.872329 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 148, 64],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 148, 64],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.874476 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 196, 2048],"float16"), Tensor([2048, 512],"float16"), Tensor([512],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 196, 2048],"float16"), Tensor([2048, 512],"float16"), Tensor([512],"float16"), False, None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.893621 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 196, 512],"float16"), Tensor([512, 2048],"float16"), Tensor([2048],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 196, 512],"float16"), Tensor([512, 2048],"float16"), Tensor([2048],"float16"), False, None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.912409 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 40],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 40],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.914472 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 40],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 40],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.916871 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 40],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 40],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.919514 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([0, 40],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([0, 40],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.921860 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 0, 1024],"float16"), Tensor([1024, 1024],"float16"), Tensor([1024],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 0, 1024],"float16"), Tensor([1024, 1024],"float16"), Tensor([1024],"float16"), name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:18.947465 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 0, 1024],"float16"), Tensor([1024, 3072],"float16"), Tensor([3072],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 0, 1024],"float16"), Tensor([1024, 3072],"float16"), Tensor([3072],"float16"), name=None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.049598 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 0],"float16"), Tensor([1024, 1024],"float16"), Tensor([1024],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 0],"float16"), Tensor([1024, 1024],"float16"), Tensor([1024],"float16"), name=None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [1024].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:1024.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.068820 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 0],"float16"), Tensor([1024, 3072],"float16"), Tensor([3072],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 0],"float16"), Tensor([1024, 3072],"float16"), Tensor([3072],"float16"), name=None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [1024].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:1024.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.134526 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([0, 1024],"float16"), Tensor([1024],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([0, 1024],"float16"), Tensor([1024],"float16"), name=None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [1024], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:1024 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.154472 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([0, 3072],"float16"), Tensor([3072],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([0, 3072],"float16"), Tensor([3072],"float16"), name=None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [1024], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:1024 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.172658 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([1024, 0],"float16"), Tensor([1024],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([1024, 0],"float16"), Tensor([1024],"float16"), name=None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [1024] and Y's shape = [1024, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:1024 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.216738 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([1024, 0],"float16"), Tensor([3072],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([1024, 0],"float16"), Tensor([3072],"float16"), name=None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [3072] and Y's shape = [1024, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:3072 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.475681 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([1024, 1024],"float16"), Tensor([0],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([1024, 1024],"float16"), Tensor([0],"float16"), name=None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [1024, 1024]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:1024.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.516653 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([1024, 3072],"float16"), Tensor([0],"float16"), name=None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([1, 1024, 1024],"float16"), Tensor([1024, 3072],"float16"), Tensor([0],"float16"), name=None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [1024, 3072]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:3072.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.618920 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 0, 2048],"float16"), Tensor([2048, 512],"float16"), Tensor([512],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 0, 2048],"float16"), Tensor([2048, 512],"float16"), Tensor([512],"float16"), False, None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.643916 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 0, 512],"float16"), Tensor([512, 2048],"float16"), Tensor([2048],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 0, 512],"float16"), Tensor([512, 2048],"float16"), Tensor([2048],"float16"), False, None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.682173 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 0],"float16"), Tensor([1024, 1000],"float16"), Tensor([1000],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 0],"float16"), Tensor([1024, 1000],"float16"), Tensor([1000],"float16"), False, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [1024].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:1024.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.700684 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 1024],"float16"), Tensor([0, 1000],"float16"), Tensor([1000],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 1024],"float16"), Tensor([0, 1000],"float16"), Tensor([1000],"float16"), False, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [1024], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:1024 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.705675 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 1024],"float16"), Tensor([1024, 0],"float16"), Tensor([1000],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 1024],"float16"), Tensor([1024, 0],"float16"), Tensor([1000],"float16"), False, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [1000] and Y's shape = [1024, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:1000 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.715245 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 1024],"float16"), Tensor([1024, 1000],"float16"), Tensor([0],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 1024],"float16"), Tensor([1024, 1000],"float16"), Tensor([0],"float16"), False, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [1024, 1000]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:1000.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.737274 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 0],"float16"), Tensor([2048, 512],"float16"), Tensor([512],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 0],"float16"), Tensor([2048, 512],"float16"), Tensor([512],"float16"), False, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [2048].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:2048.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.777634 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 0],"float16"), Tensor([512, 2048],"float16"), Tensor([2048],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 0],"float16"), Tensor([512, 2048],"float16"), Tensor([2048],"float16"), False, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [512].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:512.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:19.798798 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 2048],"float16"), Tensor([0, 512],"float16"), Tensor([512],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 2048],"float16"), Tensor([0, 512],"float16"), Tensor([512],"float16"), False, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [2048], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:2048 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:21.460579 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 2048],"float16"), Tensor([2048, 0],"float16"), Tensor([512],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 2048],"float16"), Tensor([2048, 0],"float16"), Tensor([512],"float16"), False, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [512] and Y's shape = [2048, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:512 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:22.585676 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 2048],"float16"), Tensor([2048, 512],"float16"), Tensor([0],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 2048],"float16"), Tensor([2048, 512],"float16"), Tensor([0],"float16"), False, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [2048, 512]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:512.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:23.800343 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 512],"float16"), Tensor([0, 2048],"float16"), Tensor([2048],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 512],"float16"), Tensor([0, 2048],"float16"), Tensor([2048],"float16"), False, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [512], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:512 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.202354 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 512],"float16"), Tensor([512, 0],"float16"), Tensor([2048],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 512],"float16"), Tensor([512, 0],"float16"), Tensor([2048],"float16"), False, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [2048] and Y's shape = [512, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:2048 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.489428 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 512],"float16"), Tensor([512, 2048],"float16"), Tensor([0],"float16"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([128, 196, 512],"float16"), Tensor([512, 2048],"float16"), Tensor([0],"float16"), False, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [512, 2048]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:2048.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.783121 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 0],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 0],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [40].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:40.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.784511 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 0],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 0],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [40].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:40.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.785476 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 0],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 0],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [40].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:40.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.786323 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 0],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 0],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [40].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:40.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.787180 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([0, 40],"float32"), Tensor([50],"float32"), True, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([0, 40],"float32"), Tensor([50],"float32"), True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [50] and Y's shape = [0, 40]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:50 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.788098 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([0, 40],"float32"), Tensor([50],"float32"), True, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([0, 40],"float32"), Tensor([50],"float32"), True, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [50] and Y's shape = [0, 40]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:50 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.788955 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([0, 50],"float32"), Tensor([50],"float32"), False, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([0, 50],"float32"), Tensor([50],"float32"), False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [40], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:40 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.790782 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([0, 50],"float32"), Tensor([50],"float32"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([0, 50],"float32"), Tensor([50],"float32"), False, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [40], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:40 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.791994 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 0],"float32"), Tensor([50],"float32"), False, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 0],"float32"), Tensor([50],"float32"), False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [50] and Y's shape = [40, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:50 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.792971 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 0],"float32"), Tensor([50],"float32"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 0],"float32"), Tensor([50],"float32"), False, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [50] and Y's shape = [40, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:50 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.796216 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 50],"float32"), Tensor([0],"float32"), False, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 50],"float32"), Tensor([0],"float32"), False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [40, 50]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:50.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.797257 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 50],"float32"), Tensor([0],"float32"), False, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 50],"float32"), Tensor([0],"float32"), False, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [40, 50]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:50.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.798319 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 0],"float32"), Tensor([50],"float32"), True, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 0],"float32"), Tensor([50],"float32"), True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [40], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:40 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.799239 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 0],"float32"), Tensor([50],"float32"), True, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 0],"float32"), Tensor([50],"float32"), True, None, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [40], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:40 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.800355 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 40],"float32"), Tensor([0],"float32"), True, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 40],"float32"), Tensor([0],"float32"), True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [50, 40]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:50.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.801563 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 40],"float32"), Tensor([0],"float32"), True, None, )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 40],"float32"), Tensor([0],"float32"), True, None, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [50, 40]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:50.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.802518 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 0, 64],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 0, 64],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:24.805653 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 0, 64],"float16"), Tensor([64, 64],"float16"), None, )

[Pass] paddle.incubate.nn.functional.fused_linear(Tensor([4, 0, 64],"float16"), Tensor([64, 64],"float16"), None, )
2025-03-03 18:44:25.887496 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 115, 0],"float16"), Tensor([64, 64],"float16"), None, )

W0303 18:44:25.891893 145834 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 115, 0],"float16"), Tensor([64, 64],"float16"), None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:25.892167 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 115, 64],"float16"), Tensor([0, 64],"float16"), None, )

W0303 18:44:25.894833 145837 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 115, 64],"float16"), Tensor([0, 64],"float16"), None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:25.895001 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 115, 64],"float16"), Tensor([64, 0],"float16"), None, )

W0303 18:44:25.896694 145838 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 115, 64],"float16"), Tensor([64, 0],"float16"), None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:25.896840 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 127, 0],"float16"), Tensor([64, 64],"float16"), None, )

W0303 18:44:25.898686 145839 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 127, 0],"float16"), Tensor([64, 64],"float16"), None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:25.898829 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 127, 64],"float16"), Tensor([0, 64],"float16"), None, )

W0303 18:44:25.901171 145840 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 127, 64],"float16"), Tensor([0, 64],"float16"), None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:25.901309 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 127, 64],"float16"), Tensor([64, 0],"float16"), None, )

W0303 18:44:25.904085 145841 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 127, 64],"float16"), Tensor([64, 0],"float16"), None, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:25.904298 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 132, 0],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 132, 0],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [64].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:64.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.905660 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 132, 64],"float16"), Tensor([0, 192],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 132, 64],"float16"), Tensor([0, 192],"float16"), Tensor([192],"float16"), ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [64], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:64 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.907138 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 132, 64],"float16"), Tensor([64, 0],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 132, 64],"float16"), Tensor([64, 0],"float16"), Tensor([192],"float16"), ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [192] and Y's shape = [64, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:192 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.908604 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 132, 64],"float16"), Tensor([64, 192],"float16"), Tensor([0],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 132, 64],"float16"), Tensor([64, 192],"float16"), Tensor([0],"float16"), ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [64, 192]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:192.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.910362 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 148, 0],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 148, 0],"float16"), Tensor([64, 192],"float16"), Tensor([192],"float16"), ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [64].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:64.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.911391 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 148, 64],"float16"), Tensor([0, 192],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 148, 64],"float16"), Tensor([0, 192],"float16"), Tensor([192],"float16"), ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [64], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:64 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.913649 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 148, 64],"float16"), Tensor([64, 0],"float16"), Tensor([192],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 148, 64],"float16"), Tensor([64, 0],"float16"), Tensor([192],"float16"), ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [192] and Y's shape = [64, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:192 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.915016 test begin: paddle.incubate.nn.functional.fused_linear(Tensor([4, 148, 64],"float16"), Tensor([64, 192],"float16"), Tensor([0],"float16"), )

[paddle error] paddle.incubate.nn.functional.fused_linear(Tensor([4, 148, 64],"float16"), Tensor([64, 192],"float16"), Tensor([0],"float16"), ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [64, 192]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:192.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.916666 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([0, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "gelu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([0, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "gelu", ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.919233 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([0, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "none", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([0, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "none", ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.920730 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([0, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "relu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([0, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "relu", ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.922867 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 0],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "gelu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 0],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "gelu", ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [4].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:4.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.932869 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 0],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "none", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 0],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "none", ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [4].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:4.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.942877 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 0],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "relu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 0],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "relu", ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [4].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:4.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.952643 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([0, 128],"float64"), Tensor([128],"float64"), False, False, "gelu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([0, 128],"float64"), Tensor([128],"float64"), False, False, "gelu", ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [4], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:4 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.962375 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([0, 128],"float64"), Tensor([128],"float64"), False, False, "none", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([0, 128],"float64"), Tensor([128],"float64"), False, False, "none", ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [4], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:4 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.964700 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([0, 128],"float64"), Tensor([128],"float64"), False, False, "relu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([0, 128],"float64"), Tensor([128],"float64"), False, False, "relu", ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [4], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:4 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.966160 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 0],"float64"), Tensor([128],"float64"), False, False, "gelu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 0],"float64"), Tensor([128],"float64"), False, False, "gelu", ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [128] and Y's shape = [4, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:128 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.967230 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 0],"float64"), Tensor([128],"float64"), False, False, "none", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 0],"float64"), Tensor([128],"float64"), False, False, "none", ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [128] and Y's shape = [4, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:128 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.968206 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 0],"float64"), Tensor([128],"float64"), False, False, "relu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 0],"float64"), Tensor([128],"float64"), False, False, "relu", ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [128] and Y's shape = [4, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:128 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.969385 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([0],"float64"), False, False, "gelu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([0],"float64"), False, False, "gelu", ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [4, 128]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:128.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.970707 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([0],"float64"), False, False, "none", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([0],"float64"), False, False, "none", ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [4, 128]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:128.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.972032 test begin: paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([0],"float64"), False, False, "relu", )

[paddle error] paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([0],"float64"), False, False, "relu", ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [4, 128]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:128.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.973278 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([0, 40],"float32"), None, True, False, )

[Pass] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([0, 40],"float32"), None, True, False, )
2025-03-03 18:44:25.975166 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([0, 50],"float32"), None, True, True, )

W0303 18:44:25.976740 145929 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([0, 50],"float32"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:25.976852 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([40, 50],"float32"), None, True, True, )

W0303 18:44:25.978667 145930 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([40, 50],"float32"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:25.978827 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), True, True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [50].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:50.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.979944 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([50, 40],"float32"), None, True, False, )

W0303 18:44:25.981406 145932 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([50, 40],"float32"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:25.981569 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 30],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), True, False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [50].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:50.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.982602 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([0, 5],"float16"), None, True, False, )

[Pass] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([0, 5],"float16"), None, True, False, )
2025-03-03 18:44:25.983750 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([0, 7],"float16"), None, True, True, )

W0303 18:44:25.984707 145935 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([0, 7],"float16"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:25.984805 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([5, 7],"float16"), None, True, True, )

W0303 18:44:25.986074 145936 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([5, 7],"float16"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:25.986217 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), True, True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [7].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:7.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:25.996408 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([7, 5],"float16"), None, True, False, )

W0303 18:44:26.006976 145938 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([7, 5],"float16"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.007251 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 4],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), True, False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [7].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:7.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.008888 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([0, 40],"float32"), None, False, False, )

W0303 18:44:26.010147 145940 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([0, 40],"float32"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.010261 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([0, 50],"float32"), None, False, True, )

W0303 18:44:26.011288 145941 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([0, 50],"float32"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.011388 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, )

[Pass] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, )
2025-03-03 18:44:26.012614 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.014270 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, )

[Pass] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, )
2025-03-03 18:44:26.015433 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 50],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.017183 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([0, 5],"float16"), None, False, False, )

W0303 18:44:26.018186 145946 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([0, 5],"float16"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.018284 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([0, 7],"float16"), None, False, True, )

W0303 18:44:26.019184 145947 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([0, 7],"float16"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.019278 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([5, 7],"float16"), None, False, True, )

[Pass] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([5, 7],"float16"), None, False, True, )
2025-03-03 18:44:26.020393 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.021955 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([7, 5],"float16"), None, False, False, )

[Pass] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([7, 5],"float16"), None, False, False, )
2025-03-03 18:44:26.023003 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([0, 7],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.024426 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([40, 0],"float32"), None, False, True, )

W0303 18:44:26.025501 145953 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([40, 0],"float32"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.025631 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([40, 50],"float32"), None, False, True, )

W0303 18:44:26.026755 145954 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([40, 50],"float32"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.026879 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), False, True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [50].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:50.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.027777 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([50, 0],"float32"), None, False, False, )

W0303 18:44:26.028822 145956 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([50, 0],"float32"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.028917 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([50, 40],"float32"), None, False, False, )

W0303 18:44:26.030036 145957 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([50, 40],"float32"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.030157 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 0],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), False, False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [50].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:50.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.031070 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([0, 40],"float32"), None, False, False, )

W0303 18:44:26.032301 145959 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([0, 40],"float32"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.032429 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([0, 40],"float32"), Tensor([40],"float32"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([0, 40],"float32"), Tensor([40],"float32"), False, False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [50], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:50 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.034139 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([0, 50],"float32"), None, False, True, )

W0303 18:44:26.035650 145961 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([0, 50],"float32"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.035788 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([0, 50],"float32"), Tensor([40],"float32"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([0, 50],"float32"), Tensor([40],"float32"), False, True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [40] and Y's shape = [0, 50]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:40 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.036990 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 0],"float32"), None, False, True, )

W0303 18:44:26.038312 145963 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 0],"float32"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.038487 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 0],"float32"), Tensor([40],"float32"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 0],"float32"), Tensor([40],"float32"), False, True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [50], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:50 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.039554 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 50],"float32"), Tensor([0],"float32"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 50],"float32"), Tensor([0],"float32"), False, True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [40, 50]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:40.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.040899 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 0],"float32"), None, False, False, )

W0303 18:44:26.042325 145966 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 0],"float32"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.042466 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 0],"float32"), Tensor([40],"float32"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 0],"float32"), Tensor([40],"float32"), False, False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [40] and Y's shape = [50, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:40 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.043473 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 40],"float32"), Tensor([0],"float32"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 40],"float32"), Tensor([0],"float32"), False, False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [50, 40]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:40.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.044474 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([5, 0],"float16"), None, False, True, )

W0303 18:44:26.045610 145969 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([5, 0],"float16"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.045737 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([5, 7],"float16"), None, False, True, )

W0303 18:44:26.046985 145970 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([5, 7],"float16"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.047108 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), False, True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [7].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:7.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.047999 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([7, 0],"float16"), None, False, False, )

W0303 18:44:26.048887 145972 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([7, 0],"float16"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.048976 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([7, 5],"float16"), None, False, False, )

W0303 18:44:26.050065 145973 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([7, 5],"float16"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.050178 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 0],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), False, False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [0], Y[0] = [7].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:0 != K_from_y:7.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.051040 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([0, 5],"float16"), None, False, False, )

W0303 18:44:26.052488 145975 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([0, 5],"float16"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.052698 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([0, 5],"float16"), Tensor([5],"float16"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([0, 5],"float16"), Tensor([5],"float16"), False, False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [7], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:7 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.053570 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([0, 7],"float16"), None, False, True, )

W0303 18:44:26.054566 145977 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([0, 7],"float16"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.054668 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([0, 7],"float16"), Tensor([5],"float16"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([0, 7],"float16"), Tensor([5],"float16"), False, True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [5] and Y's shape = [0, 7]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:5 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.055535 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([5, 0],"float16"), None, False, True, )

W0303 18:44:26.056620 145979 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([5, 0],"float16"), None, False, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.056744 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([5, 0],"float16"), Tensor([5],"float16"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([5, 0],"float16"), Tensor([5],"float16"), False, True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [7], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:7 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.057722 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([5, 7],"float16"), Tensor([0],"float16"), False, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([5, 7],"float16"), Tensor([0],"float16"), False, True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [5, 7]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:5.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.058601 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([7, 0],"float16"), None, False, False, )

W0303 18:44:26.059537 145982 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([7, 0],"float16"), None, False, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.059658 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([7, 0],"float16"), Tensor([5],"float16"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([7, 0],"float16"), Tensor([5],"float16"), False, False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [5] and Y's shape = [7, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:5 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.060571 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([7, 5],"float16"), Tensor([0],"float16"), False, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([7, 5],"float16"), Tensor([0],"float16"), False, False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [7, 5]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:5.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.061411 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([40, 0],"float32"), None, True, True, )

W0303 18:44:26.062307 145985 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([40, 0],"float32"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.062409 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([40, 50],"float32"), None, True, True, )

W0303 18:44:26.063421 145986 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([40, 50],"float32"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.063527 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.065869 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([50, 0],"float32"), None, True, False, )

W0303 18:44:26.066785 145988 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([50, 0],"float32"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.066882 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([50, 40],"float32"), None, True, False, )

W0303 18:44:26.067849 145989 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([50, 40],"float32"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.067941 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 0],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.071430 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([0, 40],"float32"), None, True, False, )

W0303 18:44:26.073427 145991 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([0, 40],"float32"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.073636 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([0, 40],"float32"), Tensor([40],"float32"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([0, 40],"float32"), Tensor([40],"float32"), True, False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [50], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:50 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.074865 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([0, 50],"float32"), None, True, True, )

W0303 18:44:26.076191 145993 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([0, 50],"float32"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.076345 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([0, 50],"float32"), Tensor([40],"float32"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([0, 50],"float32"), Tensor([40],"float32"), True, True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [40] and Y's shape = [0, 50]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:40 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.077535 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 0],"float32"), None, True, True, )

W0303 18:44:26.079037 145995 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 0],"float32"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.079229 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 0],"float32"), Tensor([40],"float32"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 0],"float32"), Tensor([40],"float32"), True, True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [50], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:50 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.080536 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 50],"float32"), Tensor([0],"float32"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 50],"float32"), Tensor([0],"float32"), True, True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [40, 50]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:40.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.081752 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([50, 0],"float32"), None, True, False, )

W0303 18:44:26.083024 145998 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([50, 0],"float32"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:40)

2025-03-03 18:44:26.083179 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([50, 0],"float32"), Tensor([40],"float32"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([50, 0],"float32"), Tensor([40],"float32"), True, False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [40] and Y's shape = [50, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:40 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.084331 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([50, 40],"float32"), Tensor([0],"float32"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([50, 40],"float32"), Tensor([0],"float32"), True, False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [50, 40]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:40.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.085476 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([5, 0],"float16"), None, True, True, )

W0303 18:44:26.086555 146001 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([5, 0],"float16"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.086697 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([5, 7],"float16"), None, True, True, )

W0303 18:44:26.087875 146002 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([5, 7],"float16"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.088014 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.089970 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([7, 0],"float16"), None, True, False, )

W0303 18:44:26.091439 146004 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([7, 0],"float16"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.091561 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([7, 5],"float16"), None, True, False, )

W0303 18:44:26.092626 146005 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([7, 5],"float16"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.092730 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 0],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blaslt_impl.cu.h:510)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.094588 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([0, 5],"float16"), None, True, False, )

W0303 18:44:26.095841 146007 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([0, 5],"float16"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.095968 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([0, 5],"float16"), Tensor([5],"float16"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([0, 5],"float16"), Tensor([5],"float16"), True, False, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [7], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:7 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.096876 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([0, 7],"float16"), None, True, True, )

W0303 18:44:26.098331 146011 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([0, 7],"float16"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.098475 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([0, 7],"float16"), Tensor([5],"float16"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([0, 7],"float16"), Tensor([5],"float16"), True, True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [5] and Y's shape = [0, 7]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:5 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.099564 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([5, 0],"float16"), None, True, True, )

W0303 18:44:26.100836 146013 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([5, 0],"float16"), None, True, True, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.101019 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([5, 0],"float16"), Tensor([5],"float16"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([5, 0],"float16"), Tensor([5],"float16"), True, True, ) 
 (InvalidArgument) The last dimension of X should be equal with Y's first dimension.But received X[-1] = [7], Y[0] = [0].
  [Hint: Expected K_from_x == K_from_y, but received K_from_x:7 != K_from_y:0.] (at ../paddle/phi/infermeta/fusion.cc:1888)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.102409 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([5, 7],"float16"), Tensor([0],"float16"), True, True, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([5, 7],"float16"), Tensor([0],"float16"), True, True, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [5, 7]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:5.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.103405 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([7, 0],"float16"), None, True, False, )

W0303 18:44:26.104547 146018 backward.cc:437] While running Node (MatmulGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([7, 0],"float16"), None, True, False, ) 
 (External) CUBLAS error(7). 
  [Hint: 'CUBLAS_STATUS_INVALID_VALUE'.  An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. ] (at ../paddle/phi/kernels/funcs/blas/blas_impl.cu.h:444)

2025-03-03 18:44:26.104661 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([7, 0],"float16"), Tensor([5],"float16"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([7, 0],"float16"), Tensor([5],"float16"), True, False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [5] and Y's shape = [7, 0]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:5 != trans_y ? y_dims[0] : y_dims[1]:0.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.105543 test begin: paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([7, 5],"float16"), Tensor([0],"float16"), True, False, )

[paddle error] paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([7, 5],"float16"), Tensor([0],"float16"), True, False, ) 
 (InvalidArgument) The Input tensor bias's dimension 0 should be == Y[-1], but got bias's shape = [0] and Y's shape = [7, 5]
  [Hint: Expected bias_dims[0] == trans_y ? y_dims[0] : y_dims[1], but received bias_dims[0]:0 != trans_y ? y_dims[0] : y_dims[1]:5.] (at ../paddle/phi/infermeta/fusion.cc:1871)
  [operator < fused_gemm_epilogue > error]
2025-03-03 18:44:26.106418 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 1, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )

[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 1, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:44:26.109036 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 1, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )

[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 1, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:44:26.111643 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )

[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:44:26.113208 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )

[cuda error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([0, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:44:26.114660 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 0, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 0, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.115532 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 0],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 0],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.116306 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 1, 0],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 1, 0],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.117061 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 1, 64],"float16"), Tensor([0],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 1, 64],"float16"), Tensor([0],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [64], received size of Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.117824 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 64],"float16"), Tensor([0],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([1, 64],"float16"), Tensor([0],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [64], received size of Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.118567 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([100, 0],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([100, 0],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [512]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:512.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.119296 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([100, 512],"float16"), Tensor([0],"float16"), None, 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([100, 512],"float16"), Tensor([0],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [512], received size of Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:512 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.120836 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([101, 0],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([101, 0],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.121521 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([101, 64],"float16"), Tensor([0],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([101, 64],"float16"), Tensor([0],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [64], received size of Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.122330 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([104, 0],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([104, 0],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [512]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:512.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.123026 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([104, 512],"float16"), Tensor([0],"float16"), None, 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([104, 512],"float16"), Tensor([0],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [512], received size of Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:512 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.125969 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 0, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 0, 512],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [512]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:512.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.128696 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 0, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 0, 64],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.130233 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 1, 0],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 1, 0],"float16"), Tensor([512],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [512]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:512.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.145373 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 1, 0],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 1, 0],"float16"), Tensor([64],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [0], received size of Weight is [64]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:0 != norm_weight.dims()[0]:64.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.155633 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 1, 512],"float16"), Tensor([0],"float16"), None, 1e-05, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 1, 512],"float16"), Tensor([0],"float16"), None, 1e-05, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [512], received size of Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:512 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.174430 test begin: paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 1, 64],"float16"), Tensor([0],"float16"), None, 1e-06, begin_norm_axis=1, )

[paddle error] paddle.incubate.nn.functional.fused_rms_norm(Tensor([2, 1, 64],"float16"), Tensor([0],"float16"), None, 1e-06, begin_norm_axis=1, ) 
 (InvalidArgument) The normalized size of Input(X) must equal to be the size of Weight, but received normalized size of Input(X) is [64], received size of Weight is [0]
  [Hint: Expected normalized_dims == norm_weight.dims()[0], but received normalized_dims:64 != norm_weight.dims()[0]:0.] (at ../paddle/phi/infermeta/multiary.cc:4749)

2025-03-03 18:44:26.176064 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
2025-03-03 18:44:26.179548 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
2025-03-03 18:44:26.181814 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
2025-03-03 18:44:26.184174 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
2025-03-03 18:44:26.186839 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
2025-03-03 18:44:26.189946 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
2025-03-03 18:44:26.191567 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
2025-03-03 18:44:26.193093 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
2025-03-03 18:44:26.195283 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
2025-03-03 18:44:26.205771 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
2025-03-03 18:44:26.208871 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.211371 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.215573 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.226601 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.236934 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.239641 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.243995 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.250958 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.253417 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.255672 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([0, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.258134 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.259764 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.262816 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.265056 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.266752 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 0, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.268572 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.269755 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.270891 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.272394 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.274481 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.276273 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.277446 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.278576 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
2025-03-03 18:44:26.280150 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.282460 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Pass] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
2025-03-03 18:44:26.284278 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)

 ** On entry to GEMM_EX  parameter number 16 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 16 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to SGEMM  parameter number 13 had an illegal value
 ** On entry to SGEMM  parameter number 10 had an illegal value
 ** On entry to GEMM_EX  parameter number 16 had an illegal value
 ** On entry to GEMM_EX  parameter number 12 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 13 had an illegal value
 ** On entry to SGEMM  parameter number 10 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 16 had an illegal value
 ** On entry to GEMM_EX  parameter number 12 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to SGEMM  parameter number 10 had an illegal value
 ** On entry to SGEMM  parameter number 13 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 13 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to SGEMM  parameter number 10 had an illegal value
 ** On entry to SGEMM  parameter number 8 had an illegal value
 ** On entry to GEMM_EX  parameter number 12 had an illegal value
 ** On entry to GEMM_EX  parameter number 16 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 16 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 12 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998666 (unix time) try "date -d @1740998666" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2318a) received by PID 143754 (TID 0x7f5ecc952700) from PID 143754 ***]

2025-03-03 18:44:31.023952 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

W0303 18:44:34.062609 146717 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:44:34.063724 146717 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998674 (unix time) try "date -d @1740998674" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23c0e) received by PID 146446 (TID 0x7fcb0ddc2700) from PID 146446 ***]

2025-03-03 18:44:38.692637 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

W0303 18:44:42.730895 148001 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:44:42.732193 148001 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998682 (unix time) try "date -d @1740998682" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24018) received by PID 147480 (TID 0x7f4ae9f48700) from PID 147480 ***]

2025-03-03 18:44:49.187655 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

W0303 18:44:53.613008 149268 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:44:53.614115 149268 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998693 (unix time) try "date -d @1740998693" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2445e) received by PID 148574 (TID 0x7ff93d4f4700) from PID 148574 ***]

2025-03-03 18:44:58.392113 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

W0303 18:45:02.803145 150839 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:45:02.804186 150839 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998703 (unix time) try "date -d @1740998703" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x248dc) received by PID 149724 (TID 0x7f07f8949700) from PID 149724 ***]

2025-03-03 18:45:07.682609 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

W0303 18:45:10.827140 151640 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:45:10.828300 151640 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998711 (unix time) try "date -d @1740998711" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24f7f) received by PID 151423 (TID 0x7f4935abb700) from PID 151423 ***]

2025-03-03 18:45:15.543130 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

W0303 18:45:20.264528 152614 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:45:20.265592 152614 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, ) 
 (InvalidArgument) The num_heads of k must be equal to the num_heads of v when v is not none.But received num_heads of k is 0, num_heads of v is 2
  [Hint: Expected inputs_num_heads[1] == inputs_num_heads[2] == true, but received inputs_num_heads[1] == inputs_num_heads[2]:0 != true:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_rope_kernel.cu:233)

2025-03-03 18:45:20.268488 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[paddle error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, ) 
 (InvalidArgument) The num_heads of k must be equal to the num_heads of v when v is not none.But received num_heads of k is 0, num_heads of v is 2
  [Hint: Expected inputs_num_heads[1] == inputs_num_heads[2] == true, but received inputs_num_heads[1] == inputs_num_heads[2]:0 != true:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_rope_kernel.cu:233)

2025-03-03 18:45:20.278049 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[paddle error] paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, ) 
 (InvalidArgument) The num_heads of k must be equal to the num_heads of v when v is not none.But received num_heads of k is 0, num_heads of v is 2
  [Hint: Expected inputs_num_heads[1] == inputs_num_heads[2] == true, but received inputs_num_heads[1] == inputs_num_heads[2]:0 != true:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_rope_kernel.cu:233)

2025-03-03 18:45:20.287785 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998720 (unix time) try "date -d @1740998720" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2528b) received by PID 152203 (TID 0x7fefda949700) from PID 152203 ***]

2025-03-03 18:45:24.925145 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

W0303 18:45:27.916720 153772 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:45:27.917876 153772 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998728 (unix time) try "date -d @1740998728" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2562f) received by PID 153135 (TID 0x7f78792b7700) from PID 153135 ***]

2025-03-03 18:45:32.517306 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.518182 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.518529 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.518850 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.519174 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.519456 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.519751 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.520039 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.520313 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.520594 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.520890 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.521168 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.521668 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.521989 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.522370 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.522702 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.523052 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.523428 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.523739 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.524033 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.524292 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.524687 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.525663 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.526068 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.526501 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.526859 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.527159 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.527423 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.527723 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.527982 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.528236 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.528498 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.528772 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.529026 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.529287 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.529541 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.529821 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.530138 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([0, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.530520 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 0],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.530826 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 0, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.531123 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 0, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.531419 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.531728 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.532019 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.532366 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.532653 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.532918 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.533193 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.533516 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.533830 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.534096 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.534358 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.534688 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.534971 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.535246 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.535509 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.535780 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.536068 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.536357 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.536657 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.536935 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.537190 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.537670 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.538012 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.538393 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.538732 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.539031 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.539300 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.539557 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.539842 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.540102 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.540356 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.540640 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.540897 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.541154 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.541433 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.541715 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.541975 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.542244 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.542510 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.542793 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.543058 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.543319 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.543578 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.543864 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.544123 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.544381 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.544688 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.544952 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.545205 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.545475 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.545752 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.546015 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.546289 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.546557 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.546860 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.547163 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.547450 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.547750 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.548086 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )

[Skip]
2025-03-03 18:45:32.548351 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.548624 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.548920 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([0, 8],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.549189 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 0],"int64"), use_neox_rotary_style=True, time_major=False, )

[Skip]
2025-03-03 18:45:32.549452 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.549716 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.549975 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.550242 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.550497 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.550774 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 4, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.551033 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 4, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.551296 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.551551 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.551875 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 0, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.552136 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.552390 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.552652 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.552908 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.553165 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.553428 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 0],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.553706 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 0],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.553964 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.554217 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.554480 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.554774 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.555039 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.555303 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.555564 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.555846 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.556097 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.556362 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.556623 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.556888 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.557178 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.557470 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.557760 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.558037 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.558293 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.558547 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.558830 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.559124 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.559404 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 0, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.559867 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.560145 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.560413 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.560705 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.560982 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.561251 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.561534 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.561859 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.562115 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.562388 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.562655 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.562912 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.563178 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.563435 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.563750 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.564024 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.564281 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.564560 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.564849 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.565104 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.565369 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([0, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.565650 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.565909 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.566161 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 0, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.566424 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.566695 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.566943 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 0, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.567205 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.567455 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.567730 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.568008 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([0, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.568276 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 0],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.568541 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.568810 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.569076 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.569332 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.569589 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.569879 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.570138 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.570385 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.570669 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.570963 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.571231 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.571519 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.571802 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.572065 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.572318 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.572576 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.572861 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.573117 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.573363 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.573630 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.573904 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.574182 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.574444 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.574708 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.574959 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.575222 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.575472 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.575754 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 0, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.576026 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.576319 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.576580 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.576872 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.577137 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.577387 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.577668 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.577927 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.578180 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.578445 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.578720 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.578971 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.579234 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.579488 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.579761 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.580033 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.580293 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.580565 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.580844 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.581099 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.581349 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([0, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.581615 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.581871 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.582119 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 0, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.582377 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.582644 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.582915 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 0, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.583194 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )

[Skip]
2025-03-03 18:45:32.583476 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.583764 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 0],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.584028 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([0, 8],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.584301 test begin: paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 0],"int64"), use_neox_rotary_style=True, time_major=True, )

[Skip]
2025-03-03 18:45:32.584620 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 1, 1000],"float16"), Tensor([0, 1, 1000],"float16"), )

W0303 18:45:36.343191 154880 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:45:36.345275 154880 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
W0303 18:45:36.349594 154880 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 1, 1000],"float16"), Tensor([0, 1, 1000],"float16"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.350225 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 1, 1000],"float16"), Tensor([1, 1, 1000],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 1, 1000],"float16"), Tensor([1, 1, 1000],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():0, 1, 1000 != y.dims():1, 1, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.353310 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 1, 1000],"float32"), Tensor([0, 1, 1000],"float32"), )

W0303 18:45:36.355031 155310 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 1, 1000],"float32"), Tensor([0, 1, 1000],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.355214 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 1, 1000],"float32"), Tensor([1, 1, 1000],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 1, 1000],"float32"), Tensor([1, 1, 1000],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():0, 1, 1000 != y.dims():1, 1, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.357247 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 101],"float16"), Tensor([0, 101],"float16"), )

W0303 18:45:36.358259 155312 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 101],"float16"), Tensor([0, 101],"float16"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.358374 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 101],"float16"), Tensor([4, 101],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 101],"float16"), Tensor([4, 101],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():0, 101 != y.dims():4, 101.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.359720 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 11, 1024],"float32"), )

W0303 18:45:36.360752 155314 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 11, 1024],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.360875 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 202],"float16"), None, )

W0303 18:45:36.361846 155315 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 202],"float16"), None, ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.361960 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 202],"float32"), None, )

W0303 18:45:36.363025 155316 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 202],"float32"), None, ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.363134 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 4096, 22016],"float32"), )

W0303 18:45:36.364015 155317 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 4096, 22016],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.364127 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 4096],"float32"), )

W0303 18:45:36.364773 155318 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 4096],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.364868 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 4096],"float32"), Tensor([0, 4096],"float32"), )

W0303 18:45:36.365578 155319 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 4096],"float32"), Tensor([0, 4096],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.365695 test begin: paddle.incubate.nn.functional.swiglu(Tensor([0, 4096],"float32"), Tensor([300, 4096],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([0, 4096],"float32"), Tensor([300, 4096],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():0, 4096 != y.dims():300, 4096.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.395128 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1000],"float16"), Tensor([1, 0, 1000],"float16"), )

W0303 18:45:36.396704 155321 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1000],"float16"), Tensor([1, 0, 1000],"float16"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.396847 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1000],"float16"), Tensor([1, 1, 1000],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1000],"float16"), Tensor([1, 1, 1000],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 0, 1000 != y.dims():1, 1, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.398387 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1000],"float32"), Tensor([1, 0, 1000],"float32"), )

W0303 18:45:36.399355 155323 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1000],"float32"), Tensor([1, 0, 1000],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.399466 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1000],"float32"), Tensor([1, 1, 1000],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1000],"float32"), Tensor([1, 1, 1000],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 0, 1000 != y.dims():1, 1, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.400777 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1024],"float32"), )

W0303 18:45:36.401527 155325 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 1024],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.401638 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 22016],"float32"), )

W0303 18:45:36.402333 155326 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 0, 22016],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.402429 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 0],"float16"), Tensor([1, 1, 0],"float16"), )

W0303 18:45:36.403295 155327 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 0],"float16"), Tensor([1, 1, 0],"float16"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.403393 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 0],"float16"), Tensor([1, 1, 1000],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 0],"float16"), Tensor([1, 1, 1000],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 1, 0 != y.dims():1, 1, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.404570 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 0],"float32"), )

W0303 18:45:36.405372 155329 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 0],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.405463 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 1000],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 0],"float32"), Tensor([1, 1, 1000],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 1, 0 != y.dims():1, 1, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.406566 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float16"), Tensor([0, 1, 1000],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float16"), Tensor([0, 1, 1000],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 1, 1000 != y.dims():0, 1, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.407700 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float16"), Tensor([1, 0, 1000],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float16"), Tensor([1, 0, 1000],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 1, 1000 != y.dims():1, 0, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.408741 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float16"), Tensor([1, 1, 0],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float16"), Tensor([1, 1, 0],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 1, 1000 != y.dims():1, 1, 0.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.409822 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float32"), Tensor([0, 1, 1000],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float32"), Tensor([0, 1, 1000],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 1, 1000 != y.dims():0, 1, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.410902 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float32"), Tensor([1, 0, 1000],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float32"), Tensor([1, 0, 1000],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 1, 1000 != y.dims():1, 0, 1000.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.411950 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float32"), Tensor([1, 1, 0],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 1, 1000],"float32"), Tensor([1, 1, 0],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():1, 1, 1000 != y.dims():1, 1, 0.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.413067 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 11, 0],"float32"), )

W0303 18:45:36.413796 155337 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 11, 0],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.413896 test begin: paddle.incubate.nn.functional.swiglu(Tensor([1, 4096, 0],"float32"), )

W0303 18:45:36.414561 155338 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([1, 4096, 0],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.414660 test begin: paddle.incubate.nn.functional.swiglu(Tensor([300, 0],"float32"), )

W0303 18:45:36.415318 155339 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([300, 0],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.415395 test begin: paddle.incubate.nn.functional.swiglu(Tensor([300, 0],"float32"), Tensor([300, 0],"float32"), )

W0303 18:45:36.416059 155340 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([300, 0],"float32"), Tensor([300, 0],"float32"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.416164 test begin: paddle.incubate.nn.functional.swiglu(Tensor([300, 0],"float32"), Tensor([300, 4096],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([300, 0],"float32"), Tensor([300, 4096],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():300, 0 != y.dims():300, 4096.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.430486 test begin: paddle.incubate.nn.functional.swiglu(Tensor([300, 4096],"float32"), Tensor([0, 4096],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([300, 4096],"float32"), Tensor([0, 4096],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():300, 4096 != y.dims():0, 4096.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.444937 test begin: paddle.incubate.nn.functional.swiglu(Tensor([300, 4096],"float32"), Tensor([300, 0],"float32"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([300, 4096],"float32"), Tensor([300, 0],"float32"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():300, 4096 != y.dims():300, 0.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.473309 test begin: paddle.incubate.nn.functional.swiglu(Tensor([4, 0],"float16"), None, )

W0303 18:45:36.475137 155345 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([4, 0],"float16"), None, ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.475327 test begin: paddle.incubate.nn.functional.swiglu(Tensor([4, 0],"float16"), Tensor([4, 0],"float16"), )

W0303 18:45:36.476897 155346 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([4, 0],"float16"), Tensor([4, 0],"float16"), ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.477119 test begin: paddle.incubate.nn.functional.swiglu(Tensor([4, 0],"float16"), Tensor([4, 101],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([4, 0],"float16"), Tensor([4, 101],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():4, 0 != y.dims():4, 101.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.479169 test begin: paddle.incubate.nn.functional.swiglu(Tensor([4, 0],"float32"), None, )

W0303 18:45:36.480346 155348 backward.cc:437] While running Node (SwigluGradNode) raises an EnforceNotMet exception
[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([4, 0],"float32"), None, ) 
 (InvalidArgument) Both gradients of Input(X) and Input(Y) is None.
  [Hint: dx should not be null.] (at ../paddle/phi/kernels/gpu/swiglu_grad_kernel.cu:175)

2025-03-03 18:45:36.480500 test begin: paddle.incubate.nn.functional.swiglu(Tensor([4, 101],"float16"), Tensor([0, 101],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([4, 101],"float16"), Tensor([0, 101],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():4, 101 != y.dims():0, 101.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.483918 test begin: paddle.incubate.nn.functional.swiglu(Tensor([4, 101],"float16"), Tensor([4, 0],"float16"), )

[paddle error] paddle.incubate.nn.functional.swiglu(Tensor([4, 101],"float16"), Tensor([4, 0],"float16"), ) 
 (InvalidArgument) The shape of Input(X) should be equal of the shape of Input(Y).
  [Hint: Expected x.dims() == y.dims(), but received x.dims():4, 101 != y.dims():4, 0.] (at ../paddle/phi/infermeta/binary.cc:4394)

2025-03-03 18:45:36.487200 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([0, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([0, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The batch size of Query, Key, Value should be equal.
  [Hint: Expected ((query_batch_size == key_batch_size) && (key_batch_size == value_batch_size)) == true, but received ((query_batch_size == key_batch_size) && (key_batch_size == value_batch_size)):0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4164)

2025-03-03 18:45:36.490692 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 0, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[cuda error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 0, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:45:36.495972 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 0, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 0, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (Unimplemented) Failed to run CUTLASS Grouped FMHA kernel. (at ../paddle/phi/kernels/fusion/cutlass/memory_efficient_attention/autogen_variable/impl/cutlass_forward_f16_aligned_sm_mua_rf_32x128.cu:458)

2025-03-03 18:45:36.499032 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 0],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 0],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The head size of Query, Key should be equal.
  [Hint: Expected query_head_size == key_head_size == true, but received query_head_size == key_head_size:0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4183)

2025-03-03 18:45:36.503192 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([0, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([0, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The batch size of Query, Key, Value should be equal.
  [Hint: Expected ((query_batch_size == key_batch_size) && (key_batch_size == value_batch_size)) == true, but received ((query_batch_size == key_batch_size) && (key_batch_size == value_batch_size)):0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4164)

2025-03-03 18:45:36.507265 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 0, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 0, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The head number of Key, Value should be equal.
  [Hint: Expected (key_num_head == value_num_head) == true, but received (key_num_head == value_num_head):0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4169)

2025-03-03 18:45:36.511336 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 0, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 0, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The seq length of Key, Value should be equal.
  [Hint: Expected key_seq_length == value_seq_length == true, but received key_seq_length == value_seq_length:0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4188)

2025-03-03 18:45:36.514715 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 0],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 0],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The head size of Query, Key should be equal.
  [Hint: Expected query_head_size == key_head_size == true, but received query_head_size == key_head_size:0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4183)

2025-03-03 18:45:36.518161 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([0, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([0, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The batch size of Query, Key, Value should be equal.
  [Hint: Expected ((query_batch_size == key_batch_size) && (key_batch_size == value_batch_size)) == true, but received ((query_batch_size == key_batch_size) && (key_batch_size == value_batch_size)):0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4164)

2025-03-03 18:45:36.521230 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 0, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 0, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The head number of Key, Value should be equal.
  [Hint: Expected (key_num_head == value_num_head) == true, but received (key_num_head == value_num_head):0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4169)

2025-03-03 18:45:36.526419 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 0, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 0, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, ) 
 (InvalidArgument) The seq length of Key, Value should be equal.
  [Hint: Expected key_seq_length == value_seq_length == true, but received key_seq_length == value_seq_length:0 != true:1.] (at ../paddle/phi/infermeta/fusion.cc:4188)

2025-03-03 18:45:36.529611 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 0],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   phi::fmha_cutlassF_variable_f16_aligned_32x128_rf_sm_mua_sm80(cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, false, 32, 128, true, (cutlass::gemm::kernel::GroupScheduleMode)0, true>, phi::Params&, phi::GPUContext const&)
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998736 (unix time) try "date -d @1740998736" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25a6d) received by PID 154221 (TID 0x7f890534a700) from PID 154221 ***]

2025-03-03 18:45:41.495957 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([0, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

W0303 18:45:44.263181 155874 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:45:44.264465 155874 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998744 (unix time) try "date -d @1740998744" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x25f35) received by PID 155445 (TID 0x7fd0e7dc2700) from PID 155445 ***]

2025-03-03 18:45:49.281527 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 0],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

W0303 18:45:52.250624 157476 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:45:52.251772 157476 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998752 (unix time) try "date -d @1740998752" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x261c2) received by PID 156098 (TID 0x7fc047f48700) from PID 156098 ***]

2025-03-03 18:45:56.990054 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([0, 1],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

W0303 18:46:01.444258 158682 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:46:01.445228 158682 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998761 (unix time) try "date -d @1740998761" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x268ca) received by PID 157898 (TID 0x7f2fcbf48700) from PID 157898 ***]

2025-03-03 18:46:06.672892 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 0],"int32"), mask=Tensor([1, 1, 50, 50],"float16"), scale=0.125, )

W0303 18:46:10.740051 161079 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:46:10.741240 161079 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998771 (unix time) try "date -d @1740998771" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26dda) received by PID 159194 (TID 0x7fe98fd0b700) from PID 159194 ***]

2025-03-03 18:46:15.252260 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([0, 1, 50, 50],"float16"), scale=0.125, )

W0303 18:46:18.932442 161846 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:46:18.933467 161846 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   phi::fmha_cutlassF_variable_f16_aligned_32x128_rf_sm_mua_sm80(cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, false, 32, 128, true, (cutlass::gemm::kernel::GroupScheduleMode)0, true>, phi::Params&, phi::GPUContext const&)
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998779 (unix time) try "date -d @1740998779" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x276d0) received by PID 161488 (TID 0x7fc52fdc2700) from PID 161488 ***]

2025-03-03 18:46:23.637030 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 0, 50, 50],"float16"), scale=0.125, )

W0303 18:46:26.698025 162647 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:46:26.698951 162647 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_variable_length_memory_efficient_attention(_object*, _object*, _object*)
1   variable_length_memory_efficient_attention_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
2   paddle::experimental::variable_length_memory_efficient_attention(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, float, bool, int)
3   void phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)
4   void phi::dispatch_cutlass_forward<phi::dtype::float16, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::GPUContext const&, phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
5   void phi::dispatch_cutlass_forward_f16_sm80<phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1}>(phi::fusion::MultiHeadAttentionVariableForwardKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, float, bool, int, phi::DenseTensor*)::{lambda(auto:1, auto:2)#1})
6   phi::fmha_cutlassF_variable_f16_aligned_32x128_rf_sm_mua_sm80(cutlass::gemm::kernel::DefaultFMHAGrouped<cutlass::half_t, cutlass::arch::Sm80, true, false, 32, 128, true, (cutlass::gemm::kernel::GroupScheduleMode)0, true>, phi::Params&, phi::GPUContext const&)
7   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998786 (unix time) try "date -d @1740998786" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27a36) received by PID 162358 (TID 0x7f70eff48700) from PID 162358 ***]

2025-03-03 18:46:38.175603 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 0, 50],"float16"), scale=0.125, )

W0303 18:46:42.273505  1037 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:46:42.274507  1037 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 0, 50],"float16"), scale=0.125, ) 
 (Unimplemented) Failed to run CUTLASS Grouped FMHA kernel. (at ../paddle/phi/kernels/fusion/cutlass/memory_efficient_attention/autogen_variable/impl/cutlass_forward_f16_aligned_sm_mua_rf_32x128.cu:458)

2025-03-03 18:46:42.277395 test begin: paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 0],"float16"), scale=0.125, )

[Pass] paddle.incubate.nn.functional.variable_length_memory_efficient_attention(Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1, 31, 64],"float16"), Tensor([1, 1],"int32"), Tensor([1, 1],"int32"), mask=Tensor([1, 1, 50, 0],"float16"), scale=0.125, )
2025-03-03 18:46:42.281597 test begin: paddle.incubate.softmax_mask_fuse(Tensor([0, 1, 8, 32],"float32"), Tensor([0, 1, 8, 32],"float32"), )

[cuda error] paddle.incubate.softmax_mask_fuse(Tensor([0, 1, 8, 32],"float32"), Tensor([0, 1, 8, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.286104 test begin: paddle.incubate.softmax_mask_fuse(Tensor([0, 1, 8, 32],"float32"), Tensor([1, 1, 8, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([0, 1, 8, 32],"float32"), Tensor([1, 1, 8, 32],"float32"), ) 
 (InvalidArgument) Input x's 0th dim should be equal with input mask's 0th dim but received the 0th dimension of x and mask are not equal the 0th dim of x is 0, while the 0th dim of mask is 1.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:0 != mask_dim[idx]:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.288804 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 0, 8, 32],"float32"), Tensor([1, 0, 8, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 0, 8, 32],"float32"), Tensor([1, 0, 8, 32],"float32"), ) 
 (InvalidArgument) Input mask's second dim must be 1 received the second dimension of mask is 0
  [Hint: Expected mask_dim[1] == 1, but received mask_dim[1]:0 != 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:182)

2025-03-03 18:46:42.289938 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 0, 8, 32],"float32"), Tensor([1, 1, 8, 32],"float32"), )

[cuda error] paddle.incubate.softmax_mask_fuse(Tensor([1, 0, 8, 32],"float32"), Tensor([1, 1, 8, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.292105 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 0, 32],"float32"), Tensor([1, 1, 0, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 0, 32],"float32"), Tensor([1, 1, 0, 32],"float32"), ) 
 (InvalidArgument) Input x's second last dim must be large than 1 but received the second last dimension of x is 0
  [Hint: Expected query_seq_len > 1, but received query_seq_len:0 <= 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:168)

2025-03-03 18:46:42.292891 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 0, 32],"float32"), Tensor([1, 1, 8, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 0, 32],"float32"), Tensor([1, 1, 8, 32],"float32"), ) 
 (InvalidArgument) Input x's second last dim must be large than 1 but received the second last dimension of x is 0
  [Hint: Expected query_seq_len > 1, but received query_seq_len:0 <= 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:168)

2025-03-03 18:46:42.294383 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 0],"float32"), Tensor([1, 1, 8, 0],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 0],"float32"), Tensor([1, 1, 8, 0],"float32"), ) 
 (InvalidArgument) Input x's last dim must be between [32, 8192) received the last dimension of x is 0
  [Hint: Expected key_seq_len >= 32 && key_seq_len < 8192 == true, but received key_seq_len >= 32 && key_seq_len < 8192:0 != true:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:175)

2025-03-03 18:46:42.294970 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 0],"float32"), Tensor([1, 1, 8, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 0],"float32"), Tensor([1, 1, 8, 32],"float32"), ) 
 (InvalidArgument) Input x's last dim must be between [32, 8192) received the last dimension of x is 0
  [Hint: Expected key_seq_len >= 32 && key_seq_len < 8192 == true, but received key_seq_len >= 32 && key_seq_len < 8192:0 != true:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:175)

2025-03-03 18:46:42.296466 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 32],"float32"), Tensor([0, 1, 8, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 32],"float32"), Tensor([0, 1, 8, 32],"float32"), ) 
 (InvalidArgument) Input x's 0th dim should be equal with input mask's 0th dim but received the 0th dimension of x and mask are not equal the 0th dim of x is 1, while the 0th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:1 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.297557 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 32],"float32"), Tensor([1, 0, 8, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 32],"float32"), Tensor([1, 0, 8, 32],"float32"), ) 
 (InvalidArgument) Input mask's second dim must be 1 received the second dimension of mask is 0
  [Hint: Expected mask_dim[1] == 1, but received mask_dim[1]:0 != 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:182)

2025-03-03 18:46:42.298959 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 32],"float32"), Tensor([1, 1, 0, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 32],"float32"), Tensor([1, 1, 0, 32],"float32"), ) 
 (InvalidArgument) Input x's 2th dim should be equal with input mask's 2th dim but received the 2th dimension of x and mask are not equal the 2th dim of x is 8, while the 2th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:8 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.300254 test begin: paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 32],"float32"), Tensor([1, 1, 8, 0],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse(Tensor([1, 1, 8, 32],"float32"), Tensor([1, 1, 8, 0],"float32"), ) 
 (InvalidArgument) Input x's 3th dim should be equal with input mask's 3th dim but received the 3th dimension of x and mask are not equal the 3th dim of x is 32, while the 3th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:32 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.301287 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 1020],"float16"), mask=Tensor([0, 1, 8, 1020],"float16"), )

[cuda error] paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 1020],"float16"), mask=Tensor([0, 1, 8, 1020],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.302484 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 1020],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 1020],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), ) 
 (InvalidArgument) Input x's 0th dim should be equal with input mask's 0th dim but received the 0th dimension of x and mask are not equal the 0th dim of x is 0, while the 0th dim of mask is 2.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:0 != mask_dim[idx]:2.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.304345 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 32],"float16"), mask=Tensor([0, 1, 8, 32],"float16"), )

[cuda error] paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 32],"float16"), mask=Tensor([0, 1, 8, 32],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.306013 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 32],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([0, 8, 8, 32],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), ) 
 (InvalidArgument) Input x's 0th dim should be equal with input mask's 0th dim but received the 0th dimension of x and mask are not equal the 0th dim of x is 0, while the 0th dim of mask is 2.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:0 != mask_dim[idx]:2.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.307336 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 1020],"float16"), mask=Tensor([2, 0, 8, 1020],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 1020],"float16"), mask=Tensor([2, 0, 8, 1020],"float16"), ) 
 (InvalidArgument) Input mask's second dim must be 1 received the second dimension of mask is 0
  [Hint: Expected mask_dim[1] == 1, but received mask_dim[1]:0 != 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:182)

2025-03-03 18:46:42.308059 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 1020],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), )

[cuda error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 1020],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.310549 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 32],"float16"), mask=Tensor([2, 0, 8, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 32],"float16"), mask=Tensor([2, 0, 8, 32],"float16"), ) 
 (InvalidArgument) Input mask's second dim must be 1 received the second dimension of mask is 0
  [Hint: Expected mask_dim[1] == 1, but received mask_dim[1]:0 != 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:182)

2025-03-03 18:46:42.311307 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 32],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), )

[cuda error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 0, 8, 32],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.313054 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 0, 1020],"float16"), mask=Tensor([2, 1, 0, 1020],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 0, 1020],"float16"), mask=Tensor([2, 1, 0, 1020],"float16"), ) 
 (InvalidArgument) Input x's second last dim must be large than 1 but received the second last dimension of x is 0
  [Hint: Expected query_seq_len > 1, but received query_seq_len:0 <= 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:168)

2025-03-03 18:46:42.313636 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 0, 1020],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 0, 1020],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), ) 
 (InvalidArgument) Input x's second last dim must be large than 1 but received the second last dimension of x is 0
  [Hint: Expected query_seq_len > 1, but received query_seq_len:0 <= 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:168)

2025-03-03 18:46:42.314888 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 0, 32],"float16"), mask=Tensor([2, 1, 0, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 0, 32],"float16"), mask=Tensor([2, 1, 0, 32],"float16"), ) 
 (InvalidArgument) Input x's second last dim must be large than 1 but received the second last dimension of x is 0
  [Hint: Expected query_seq_len > 1, but received query_seq_len:0 <= 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:168)

2025-03-03 18:46:42.315548 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 0, 32],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 0, 32],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), ) 
 (InvalidArgument) Input x's second last dim must be large than 1 but received the second last dimension of x is 0
  [Hint: Expected query_seq_len > 1, but received query_seq_len:0 <= 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:168)

2025-03-03 18:46:42.316689 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 0],"float16"), mask=Tensor([2, 1, 8, 0],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 0],"float16"), mask=Tensor([2, 1, 8, 0],"float16"), ) 
 (InvalidArgument) Input x's last dim must be between [32, 8192) received the last dimension of x is 0
  [Hint: Expected key_seq_len >= 32 && key_seq_len < 8192 == true, but received key_seq_len >= 32 && key_seq_len < 8192:0 != true:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:175)

2025-03-03 18:46:42.317346 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 0],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 0],"float16"), mask=Tensor([2, 1, 8, 1020],"float16"), ) 
 (InvalidArgument) Input x's last dim must be between [32, 8192) received the last dimension of x is 0
  [Hint: Expected key_seq_len >= 32 && key_seq_len < 8192 == true, but received key_seq_len >= 32 && key_seq_len < 8192:0 != true:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:175)

2025-03-03 18:46:42.319040 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 0],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 0],"float16"), mask=Tensor([2, 1, 8, 32],"float16"), ) 
 (InvalidArgument) Input x's last dim must be between [32, 8192) received the last dimension of x is 0
  [Hint: Expected key_seq_len >= 32 && key_seq_len < 8192 == true, but received key_seq_len >= 32 && key_seq_len < 8192:0 != true:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:175)

2025-03-03 18:46:42.320196 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 1020],"float16"), mask=Tensor([0, 1, 8, 1020],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 1020],"float16"), mask=Tensor([0, 1, 8, 1020],"float16"), ) 
 (InvalidArgument) Input x's 0th dim should be equal with input mask's 0th dim but received the 0th dimension of x and mask are not equal the 0th dim of x is 2, while the 0th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:2 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.325306 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 1020],"float16"), mask=Tensor([2, 0, 8, 1020],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 1020],"float16"), mask=Tensor([2, 0, 8, 1020],"float16"), ) 
 (InvalidArgument) Input mask's second dim must be 1 received the second dimension of mask is 0
  [Hint: Expected mask_dim[1] == 1, but received mask_dim[1]:0 != 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:182)

2025-03-03 18:46:42.328274 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 1020],"float16"), mask=Tensor([2, 1, 0, 1020],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 1020],"float16"), mask=Tensor([2, 1, 0, 1020],"float16"), ) 
 (InvalidArgument) Input x's 2th dim should be equal with input mask's 2th dim but received the 2th dimension of x and mask are not equal the 2th dim of x is 8, while the 2th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:8 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.331657 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 1020],"float16"), mask=Tensor([2, 1, 8, 0],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 1020],"float16"), mask=Tensor([2, 1, 8, 0],"float16"), ) 
 (InvalidArgument) Input x's 3th dim should be equal with input mask's 3th dim but received the 3th dimension of x and mask are not equal the 3th dim of x is 1020, while the 3th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:1020 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.336269 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 32],"float16"), mask=Tensor([0, 1, 8, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 32],"float16"), mask=Tensor([0, 1, 8, 32],"float16"), ) 
 (InvalidArgument) Input x's 0th dim should be equal with input mask's 0th dim but received the 0th dimension of x and mask are not equal the 0th dim of x is 2, while the 0th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:2 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.337827 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 32],"float16"), mask=Tensor([2, 0, 8, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 32],"float16"), mask=Tensor([2, 0, 8, 32],"float16"), ) 
 (InvalidArgument) Input mask's second dim must be 1 received the second dimension of mask is 0
  [Hint: Expected mask_dim[1] == 1, but received mask_dim[1]:0 != 1:1.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:182)

2025-03-03 18:46:42.339061 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 32],"float16"), mask=Tensor([2, 1, 0, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 32],"float16"), mask=Tensor([2, 1, 0, 32],"float16"), ) 
 (InvalidArgument) Input x's 2th dim should be equal with input mask's 2th dim but received the 2th dimension of x and mask are not equal the 2th dim of x is 8, while the 2th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:8 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.340177 test begin: paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 32],"float16"), mask=Tensor([2, 1, 8, 0],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse(x=Tensor([2, 8, 8, 32],"float16"), mask=Tensor([2, 1, 8, 0],"float16"), ) 
 (InvalidArgument) Input x's 3th dim should be equal with input mask's 3th dim but received the 3th dimension of x and mask are not equal the 3th dim of x is 32, while the 3th dim of mask is 0.
  [Hint: Expected x_dim[idx] == mask_dim[idx], but received x_dim[idx]:32 != mask_dim[idx]:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_kernel.cu:201)

2025-03-03 18:46:42.341630 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([0, 16, 1024, 1024],"float16"), )

[Pass] paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([0, 16, 1024, 1024],"float16"), )
2025-03-03 18:46:42.342936 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([0, 4, 32, 32],"float16"), )

[Pass] paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([0, 4, 32, 32],"float16"), )
2025-03-03 18:46:42.343963 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 0, 1024, 1024],"float16"), )

[Pass] paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 0, 1024, 1024],"float16"), )
2025-03-03 18:46:42.345067 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 0, 32, 32],"float16"), )

[Pass] paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 0, 32, 32],"float16"), )
2025-03-03 18:46:42.346573 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 16, 0, 1024],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 16, 0, 1024],"float16"), ) 
 (InvalidArgument) Key seq len must be equal with query seq len received key len: 1024, query len: 0
  [Hint: Expected key_seq_len == query_seq_len, but received key_seq_len:1024 != query_seq_len:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_upper_triangle_kernel.cu:162)

2025-03-03 18:46:42.347057 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 16, 1024, 0],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 16, 1024, 0],"float16"), ) 
 (InvalidArgument) Key seq len must be equal with query seq len received key len: 0, query len: 1024
  [Hint: Expected key_seq_len == query_seq_len, but received key_seq_len:0 != query_seq_len:1024.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_upper_triangle_kernel.cu:162)

2025-03-03 18:46:42.347523 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 4, 0, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 4, 0, 32],"float16"), ) 
 (InvalidArgument) Key seq len must be equal with query seq len received key len: 32, query len: 0
  [Hint: Expected key_seq_len == query_seq_len, but received key_seq_len:32 != query_seq_len:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_upper_triangle_kernel.cu:162)

2025-03-03 18:46:42.347965 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 4, 32, 0],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse_upper_triangle(Tensor([1, 4, 32, 0],"float16"), ) 
 (InvalidArgument) Key seq len must be equal with query seq len received key len: 0, query len: 32
  [Hint: Expected key_seq_len == query_seq_len, but received key_seq_len:0 != query_seq_len:32.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_upper_triangle_kernel.cu:162)

2025-03-03 18:46:42.348371 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([0, 1, 32, 32],"float16"), )

[Pass] paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([0, 1, 32, 32],"float16"), )
2025-03-03 18:46:42.349349 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([0, 1, 32, 32],"float32"), )

[Pass] paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([0, 1, 32, 32],"float32"), )
2025-03-03 18:46:42.350396 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 0, 32, 32],"float16"), )

[Pass] paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 0, 32, 32],"float16"), )
2025-03-03 18:46:42.351383 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 0, 32, 32],"float32"), )

[Pass] paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 0, 32, 32],"float32"), )
2025-03-03 18:46:42.352405 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 1, 0, 32],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 1, 0, 32],"float16"), ) 
 (InvalidArgument) Key seq len must be equal with query seq len received key len: 32, query len: 0
  [Hint: Expected key_seq_len == query_seq_len, but received key_seq_len:32 != query_seq_len:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_upper_triangle_kernel.cu:162)

2025-03-03 18:46:42.352841 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 1, 0, 32],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 1, 0, 32],"float32"), ) 
 (InvalidArgument) Key seq len must be equal with query seq len received key len: 32, query len: 0
  [Hint: Expected key_seq_len == query_seq_len, but received key_seq_len:32 != query_seq_len:0.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_upper_triangle_kernel.cu:162)

2025-03-03 18:46:42.353243 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 1, 32, 0],"float16"), )

[paddle error] paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 1, 32, 0],"float16"), ) 
 (InvalidArgument) Key seq len must be equal with query seq len received key len: 0, query len: 32
  [Hint: Expected key_seq_len == query_seq_len, but received key_seq_len:0 != query_seq_len:32.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_upper_triangle_kernel.cu:162)

2025-03-03 18:46:42.353653 test begin: paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 1, 32, 0],"float32"), )

[paddle error] paddle.incubate.softmax_mask_fuse_upper_triangle(x=Tensor([1, 1, 32, 0],"float32"), ) 
 (InvalidArgument) Key seq len must be equal with query seq len received key len: 0, query len: 32
  [Hint: Expected key_seq_len == query_seq_len, but received key_seq_len:0 != query_seq_len:32.] (at ../paddle/phi/kernels/fusion/gpu/fused_softmax_mask_upper_triangle_kernel.cu:162)

2025-03-03 18:46:42.354095 test begin: paddle.index_fill(Tensor([0, 40],"float32"), Tensor([2],"int64"), 1, -1, )

[cuda error] paddle.index_fill(Tensor([0, 40],"float32"), Tensor([2],"int64"), 1, -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.357112 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 2, )

[cuda error] paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.359049 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 5, )

[cuda error] paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 5, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.361352 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 6, )

[cuda error] paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 6, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.362874 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 7, )

[cuda error] paddle.index_fill(Tensor([0],"int64"), Tensor([0],"int64"), 0, 7, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:46:42.364376 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([28],"int64"), 0, 5, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_index_put(_object*, _object*, _object*)
1   index_put_ad_func(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
2   paddle::experimental::index_put(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
3   void phi::IndexPutKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
4   void phi::LaunchIndexPutCudaKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998802 (unix time) try "date -d @1740998802" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20f) received by PID 527 (TID 0x7f1b00949700) from PID 527 ***]

2025-03-03 18:46:47.122014 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([30],"int64"), 0, 7, )

W0303 18:46:50.183569  1995 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:46:50.184715  1995 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_index_put(_object*, _object*, _object*)
1   index_put_ad_func(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
2   paddle::experimental::index_put(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
3   void phi::IndexPutKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
4   void phi::LaunchIndexPutCudaKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998810 (unix time) try "date -d @1740998810" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x606) received by PID 1542 (TID 0x7fbc60949700) from PID 1542 ***]

2025-03-03 18:47:02.284236 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([38],"int64"), 0, 6, )

W0303 18:47:06.637465  3767 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:47:06.638749  3767 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_index_put(_object*, _object*, _object*)
1   index_put_ad_func(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
2   paddle::experimental::index_put(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
3   void phi::IndexPutKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
4   void phi::LaunchIndexPutCudaKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998826 (unix time) try "date -d @1740998826" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd02) received by PID 3330 (TID 0x7f58dfd0b700) from PID 3330 ***]

2025-03-03 18:47:11.445525 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([4],"int64"), 0, 2, )

W0303 18:47:14.290218  4473 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:47:14.291213  4473 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_index_put(_object*, _object*, _object*)
1   index_put_ad_func(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
2   paddle::experimental::index_put(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
3   void phi::IndexPutKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
4   void phi::LaunchIndexPutCudaKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998834 (unix time) try "date -d @1740998834" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xf8f) received by PID 3983 (TID 0x7f6fd1abb700) from PID 3983 ***]

2025-03-03 18:47:18.988724 test begin: paddle.index_fill(Tensor([0],"int64"), Tensor([5],"int64"), 0, 2, )

W0303 18:47:21.918980  5187 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:47:21.920181  5187 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_index_put(_object*, _object*, _object*)
1   index_put_ad_func(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
2   paddle::experimental::index_put(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
3   void phi::IndexPutKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
4   void phi::LaunchIndexPutCudaKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998842 (unix time) try "date -d @1740998842" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1280) received by PID 4736 (TID 0x7fcb8bb85700) from PID 4736 ***]

2025-03-03 18:47:26.382993 test begin: paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 5, )

W0303 18:47:29.562307  6120 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:47:29.563336  6120 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 5, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:29.566876 test begin: paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 6, )

[cuda error] paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 6, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:29.571513 test begin: paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 7, )

[cuda error] paddle.index_fill(Tensor([128],"int64"), Tensor([0],"int64"), 0, 7, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:29.574695 test begin: paddle.index_fill(Tensor([20, 0],"float32"), Tensor([2],"int64"), 1, -1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_index_put(_object*, _object*, _object*)
1   index_put_ad_func(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
2   paddle::experimental::index_put(paddle::Tensor const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, paddle::Tensor const&, bool)
3   void phi::IndexPutKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
4   void phi::LaunchIndexPutCudaKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> > const&, phi::DenseTensor const&, bool, phi::DenseTensor*)
5   std::unique_ptr<phi::Allocation, std::function<void (phi::Allocation*)> >::~unique_ptr()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1740998849 (unix time) try "date -d @1740998849" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x153b) received by PID 5435 (TID 0x7f0c33f48700) from PID 5435 ***]

2025-03-03 18:47:34.666809 test begin: paddle.index_fill(Tensor([20, 40],"float32"), Tensor([0],"int64"), 1, -1, )

W0303 18:47:41.116634  7476 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0303 18:47:41.118113  7476 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.index_fill(Tensor([20, 40],"float32"), Tensor([0],"int64"), 1, -1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.122365 test begin: paddle.index_fill(Tensor([4],"int64"), Tensor([0],"int64"), 0, 2, )

[cuda error] paddle.index_fill(Tensor([4],"int64"), Tensor([0],"int64"), 0, 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.127875 test begin: paddle.index_fill(Tensor([8],"int64"), Tensor([0],"int64"), 0, 2, )

[cuda error] paddle.index_fill(Tensor([8],"int64"), Tensor([0],"int64"), 0, 2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.134666 test begin: paddle.is_empty(Tensor([0, 3],"float32"), )

[Pass] paddle.is_empty(Tensor([0, 3],"float32"), )
2025-03-03 18:47:41.135755 test begin: paddle.is_empty(Tensor([0],"int64"), )

[Pass] paddle.is_empty(Tensor([0],"int64"), )
2025-03-03 18:47:41.136619 test begin: paddle.is_empty(Tensor([2, 0],"float32"), )

[Pass] paddle.is_empty(Tensor([2, 0],"float32"), )
2025-03-03 18:47:41.137392 test begin: paddle.is_empty(x=Tensor([0, 32, 32],"float32"), )

[Pass] paddle.is_empty(x=Tensor([0, 32, 32],"float32"), )
2025-03-03 18:47:41.138492 test begin: paddle.is_empty(x=Tensor([4, 0, 32],"float32"), )

[Pass] paddle.is_empty(x=Tensor([4, 0, 32],"float32"), )
2025-03-03 18:47:41.139206 test begin: paddle.is_empty(x=Tensor([4, 32, 0],"float32"), )

[Pass] paddle.is_empty(x=Tensor([4, 32, 0],"float32"), )
2025-03-03 18:47:41.139879 test begin: paddle.isin(Tensor([0, 5, 10],"bfloat16"), Tensor([100],"bfloat16"), True, False, )

[Pass] paddle.isin(Tensor([0, 5, 10],"bfloat16"), Tensor([100],"bfloat16"), True, False, )
2025-03-03 18:47:41.148075 test begin: paddle.isin(Tensor([0, 5, 10],"bfloat16"), Tensor([100],"bfloat16"), True, True, )

[Pass] paddle.isin(Tensor([0, 5, 10],"bfloat16"), Tensor([100],"bfloat16"), True, True, )
2025-03-03 18:47:41.150567 test begin: paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, False, )

[cuda error] paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.152476 test begin: paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, True, )

[cuda error] paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.153883 test begin: paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([4, 256],"bfloat16"), False, False, )

[cuda error] paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([4, 256],"bfloat16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.155722 test begin: paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([4, 256],"bfloat16"), False, True, )

[cuda error] paddle.isin(Tensor([0, 64],"bfloat16"), Tensor([4, 256],"bfloat16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.157320 test begin: paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, False, )

[cuda error] paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.158614 test begin: paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, True, )

[cuda error] paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.159922 test begin: paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([2, 3],"bfloat16"), False, False, )

[cuda error] paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([2, 3],"bfloat16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.161437 test begin: paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([2, 3],"bfloat16"), False, True, )

[cuda error] paddle.isin(Tensor([0, 8],"bfloat16"), Tensor([2, 3],"bfloat16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.163316 test begin: paddle.isin(Tensor([2, 0, 10],"bfloat16"), Tensor([100],"bfloat16"), True, False, )

[Pass] paddle.isin(Tensor([2, 0, 10],"bfloat16"), Tensor([100],"bfloat16"), True, False, )
2025-03-03 18:47:41.165431 test begin: paddle.isin(Tensor([2, 0, 10],"bfloat16"), Tensor([100],"bfloat16"), True, True, )

[Pass] paddle.isin(Tensor([2, 0, 10],"bfloat16"), Tensor([100],"bfloat16"), True, True, )
2025-03-03 18:47:41.167435 test begin: paddle.isin(Tensor([2, 5, 0],"bfloat16"), Tensor([100],"bfloat16"), True, False, )

[Pass] paddle.isin(Tensor([2, 5, 0],"bfloat16"), Tensor([100],"bfloat16"), True, False, )
2025-03-03 18:47:41.169316 test begin: paddle.isin(Tensor([2, 5, 0],"bfloat16"), Tensor([100],"bfloat16"), True, True, )

[Pass] paddle.isin(Tensor([2, 5, 0],"bfloat16"), Tensor([100],"bfloat16"), True, True, )
2025-03-03 18:47:41.171204 test begin: paddle.isin(Tensor([2, 5, 10],"bfloat16"), Tensor([0],"bfloat16"), True, False, )

[Pass] paddle.isin(Tensor([2, 5, 10],"bfloat16"), Tensor([0],"bfloat16"), True, False, )
2025-03-03 18:47:41.172756 test begin: paddle.isin(Tensor([2, 5, 10],"bfloat16"), Tensor([0],"bfloat16"), True, True, )

[Pass] paddle.isin(Tensor([2, 5, 10],"bfloat16"), Tensor([0],"bfloat16"), True, True, )
2025-03-03 18:47:41.174374 test begin: paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 0],"bfloat16"), False, False, )

[cuda error] paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 0],"bfloat16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.175645 test begin: paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 0],"bfloat16"), False, True, )

[cuda error] paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 0],"bfloat16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.176880 test begin: paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 3],"bfloat16"), False, False, )

[cuda error] paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 3],"bfloat16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.178372 test begin: paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 3],"bfloat16"), False, True, )

[cuda error] paddle.isin(Tensor([4, 0],"bfloat16"), Tensor([2, 3],"bfloat16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.179899 test begin: paddle.isin(Tensor([4, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, False, )

[Pass] paddle.isin(Tensor([4, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, False, )
2025-03-03 18:47:41.181110 test begin: paddle.isin(Tensor([4, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, True, )

[Pass] paddle.isin(Tensor([4, 8],"bfloat16"), Tensor([0, 3],"bfloat16"), False, True, )
2025-03-03 18:47:41.182308 test begin: paddle.isin(Tensor([4, 8],"bfloat16"), Tensor([2, 0],"bfloat16"), False, False, )

[Pass] paddle.isin(Tensor([4, 8],"bfloat16"), Tensor([2, 0],"bfloat16"), False, False, )
2025-03-03 18:47:41.183540 test begin: paddle.isin(Tensor([4, 8],"bfloat16"), Tensor([2, 0],"bfloat16"), False, True, )

[Pass] paddle.isin(Tensor([4, 8],"bfloat16"), Tensor([2, 0],"bfloat16"), False, True, )
2025-03-03 18:47:41.184827 test begin: paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 0],"bfloat16"), False, False, )

[cuda error] paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 0],"bfloat16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.186053 test begin: paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 0],"bfloat16"), False, True, )

[cuda error] paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 0],"bfloat16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.187363 test begin: paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 256],"bfloat16"), False, False, )

[cuda error] paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 256],"bfloat16"), False, False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.189065 test begin: paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 256],"bfloat16"), False, True, )

[cuda error] paddle.isin(Tensor([8, 0],"bfloat16"), Tensor([4, 256],"bfloat16"), False, True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-03 18:47:41.190545 test begin: paddle.isin(Tensor([8, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, False, )

[Pass] paddle.isin(Tensor([8, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, False, )
2025-03-03 18:47:41.191892 test begin: paddle.isin(Tensor([8, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, True, )

[Pass] paddle.isin(Tensor([8, 64],"bfloat16"), Tensor([0, 256],"bfloat16"), False, True, )
2025-03-03 18:47:41.193203 test begin: paddle.isin(Tensor([8, 64],"bfloat16"), Tensor([4, 0],"bfloat16"), False, False, )

[Pass] paddle.isin(Tensor([8, 64],"bfloat16"), Tensor([4, 0],"bfloat16"), False, False, )
2025-03-03 18:47:41.194357 test begin: paddle.isin(Tensor([8, 64],"bfloat16"), Tensor([4, 0],"bfloat16"), False, True, )

[Pass] paddle.isin(Tensor([8, 64],"bfloat16"), Tensor([4, 0],"bfloat16"), False, True, )
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 79
    "paddle.nn.functional.adaptive_avg_pool2d(Tensor([128, 1024, 0, 80],"float16"), list[1,40,], )"
                                                                         ^
SyntaxError: invalid syntax
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 2, in <module>
    from .api_config import TensorConfig, APIConfig, analyse_configs, USE_CACHED_NUMPY, cached_numpy
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/api_config/__init__.py", line 1, in <module>
    from .config_analyzer import TensorConfig, APIConfig, analyse_configs, USE_CACHED_NUMPY, cached_numpy
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/api_config/config_analyzer.py", line 9, in <module>
    import torch
  File "/usr/local/lib/python3.9/dist-packages/torch/__init__.py", line 2016, in <module>
    from torch import _VF as _VF, functional as functional  # usort: skip
  File "/usr/local/lib/python3.9/dist-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/__init__.py", line 8, in <module>
    from torch.nn.modules import *  # usort: skip # noqa: F403
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/__init__.py", line 53, in <module>
    from .conv import (
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py", line 1174, in <module>
    class ConvTranspose3d(_ConvTransposeNd):
KeyboardInterrupt
